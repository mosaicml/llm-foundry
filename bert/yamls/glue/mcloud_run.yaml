# This YAML is intended to serve as a reference for running on the Mosaic Cloud
# The config from `yamls/glue/mosaic-bert-base-uncased.yaml` is copied into the `parameters` field below.
# You can copy/modify the contents of this file to run different workloads, following the other
# examples in this directory.
#
# Note that some of the fields in this template haven't been filled in yet.
# Please resolve any `null` fields before launching!
#
# When ready, use `mcli run -f yamls/glue/mcloud_run.yaml` to launch

run_name: &run_name mosaic-bert-base-uncased-glue-finetuning
cluster:       # Name of the cluster to use for this run
gpu_type: a100_80gb # Type of GPU to use (we use a100_80gb)
gpu_num: 8  # Number of GPUs to use
image: mosaicml/pytorch:1.12.1_cu116-python3.9-ubuntu20.04
integrations:
- integration_type: git_repo
  git_repo: mosaicml/examples
command: |
  cd examples/bert
  pip install -r requirements.txt
  python glue.py /mnt/config/parameters.yaml

# Starting `parameters` copied from `yamls/glue/mosaic-bert-base-uncased.yaml`
# Changes to `parameters` will be reflected in `/mnt/config/parameters.yaml`, which
# is the config that `glue.py` uses in the above command.
parameters:
  # Whether to run the various GLUE jobs serially or in parallel (use
  # parallel=True to take advantage of multiple GPUs)
  parallel: true

  # Basic run configuration, additional details will be added to this name
  # for each GLUE task, and each random seed. The base_run_name also determines
  # how runs are saved and logged in W&B
  base_run_name: *run_name
  default_seed: 19
  precision: bf16

  # Tokenizer for dataset creation
  tokenizer_name: &tokenizer_name bert-base-uncased

  # Base model config
  model:
    name: mosaic_bert
    pretrained_model_name: *tokenizer_name
    tokenizer_name: *tokenizer_name

  # Loading
  # (fill this in with the composer checkpoint from the end of pre-training a Mosaic BERT)
  starting_checkpoint_load_path: <checkpoint_from_pretraining>
  local_pretrain_checkpoint_folder: ./local-bert-checkpoints/

  # Saving
  save_finetune_checkpoint_prefix: ./local-finetune-checkpoints/ # (local)
  # save_finetune_checkpoint_prefix: s3://<bucket>/remote-finetune-checkpoints # (remote)
  save_finetune_checkpoint_folder: ${save_finetune_checkpoint_prefix}/${base_run_name}

  # Loggers
  loggers:
    wandb: # (Comment this block to disable W&B logging)
      project:      # Fill this in if using W&B
      entity:      # Fill this in if using W&B

  # Callbacks
  callbacks:
    lr_monitor: {}
    speed_monitor: {}

  # Scheduler
  scheduler:
    name: linear_decay_with_warmup
    t_warmup: 0.06dur
    alpha_f: 0.0

  # Algorithms
  algorithms:
    fused_layernorm: {}

  # Task configuration
  tasks:
    mnli:
      # Specify any extra task-specific arguments for the trainer here
      trainer_kwargs:
        # We keep one MNLI checkpoint locally so that we can start finetuning of
        # RTE, MRPC and STS-B from the MNLI checkpoint
        save_num_checkpoints_to_keep: 1
    rte:
      seeds: [19, 8364, 717, 10536, 90166]
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
    qqp:
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
    qnli:
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
    sst2:
      seeds: [19, 8364, 717]
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
    stsb:
      seeds: [19, 8364, 717, 10536, 90166]
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
    mrpc:
      seeds: [19, 8364, 717, 10536, 90166]
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
    cola:
      seeds: [19, 8364, 717, 10536]
      trainer_kwargs:
        save_num_checkpoints_to_keep: 0
