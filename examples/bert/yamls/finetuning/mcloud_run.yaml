# This YAML is built to work with the `sequence_classification.py` starter script!
#
#   Follow the instructions in that script to modify the `build_my_dataloader` function
#   and fine-tune a BERT model on your own dataset!
#
#
#   Note that some of the fields in this template haven't been filled in yet.
#   Please resolve any empty fields before launching!
#   When ready, use `mcli run -f yamls/finetuning/mcloud_run.yaml` to launch

# Mosaic Cloud will use run_name (with a unique suffix) to populate the env var $COMPOSER_RUN_NAME
run_name: finetune-mosaic-bert
cluster:       # Name of the cluster to use for this run
gpu_type: a100_80gb # Type of GPU to use (we use a100_80gb in our experiments)
gpu_num: 8  # Number of GPUs to use
image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
integrations:
- integration_type: git_repo
  git_repo: mosaicml/examples
  git_branch: v0.0.3 # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[bert]
command: |
  cd examples/examples/bert
  composer sequence_classification.py /mnt/config/parameters.yaml

parameters:
  # Run Name
  run_name: # If left blank, will be read from env var $COMPOSER_RUN_NAME

  tokenizer_name: bert-base-uncased
  max_seq_len: 128

  load_path: # (Optionally, but recommended) provide a composer checkpoint to use for the starting weights

  # Model
  model:
    name: mosaic_bert
    num_labels: 2 # <-- Make sure to update these after you modify the starter script!
    pretrained_model_name: ${tokenizer_name}
    tokenizer_name: ${tokenizer_name}

  # Dataloaders (make sure to update these after you modify the starter script!)
  train_loader:
    split: train
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    drop_last: true
    num_workers: 8

  eval_loader:
    split: validation
    tokenizer_name: ${tokenizer_name}
    max_seq_len: ${max_seq_len}
    shuffle: true
    drop_last: true
    num_workers: 8

  # Optimization
  scheduler:
    name: linear_decay_with_warmup
    t_warmup: 0.06dur # Warmup to the full LR for 6% of the training duration
    alpha_f: 0.02 # Linearly decay to 0.02x the full LR by the end of the training duration

  optimizer:
    name: decoupled_adamw
    lr: 1.0e-5
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-6

  # Training duration and evaluation frequency
  max_duration: 10ep
  eval_interval: 1ep
  global_train_batch_size: 16

  # System
  seed: 17
  device_eval_batch_size: 16
  device_train_microbatch_size: 16
  precision: amp_bf16

  # Logging
  progress_bar: false
  log_to_console: true
  console_log_interval: 10ba

  # Optionally log to W&B
  # loggers:
  #   wandb: {}

  callbacks:
    speed_monitor:
      window_size: 50
    lr_monitor: {}
