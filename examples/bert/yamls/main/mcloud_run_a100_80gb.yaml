# This YAML is intended to serve as a reference for running on the Mosaic Cloud
# The config from `yamls/main/mosaic-bert-base-uncased.yaml` is copied into the `parameters` field below.
# You can copy/modify the contents of this file to run different workloads, following the other
# examples in this directory.
#
# Note that some of the fields in this template haven't been filled in yet.
# Please resolve any `null` fields before launching!
#
# When ready, use `mcli run -f yamls/main/mcloud_run_a100_80gb.yaml` to launch

run_name: &run_name mosaic-bert-base-uncased
cluster:       # Name of the cluster to use for this run
gpu_type: a100_80gb # Type of GPU to use (we use a100_80gb in our experiments)
gpu_num: 8  # Number of GPUs to use
image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
integrations:
- integration_type: git_repo
  git_repo: mosaicml/examples
  # git_branch: # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[bert]
command: |
  cd examples/examples/bert
  composer main.py /mnt/config/parameters.yaml

# Starting `parameters` copied from `yamls/main/mosaic-bert-base-uncased.yaml`.
# Changes to `parameters` will be reflected in `/mnt/config/parameters.yaml`, which
# is the config that `main.py` uses in the above command.
parameters:
  # Supply a path to your remote data path
  data_local: &data_local /tmp/mds-cache/
  data_remote: &data_remote null

  max_seq_len: &max_seq_len 128
  tokenizer_name: &tokenizer_name bert-base-uncased
  mlm_probability: &mlm_probability 0.3 # Mosaic BERT should use 30% masking for optimal performance

  # Run Name
  run_name: *run_name

  # Model
  model:
    name: mosaic_bert
    pretrained_model_name: *tokenizer_name
    tokenizer_name: *tokenizer_name

  # Dataloaders
  train_loader:
    name: text
    dataset:
      local: *data_local
      remote: *data_remote
      split: train
      tokenizer_name: *tokenizer_name
      max_seq_len: *max_seq_len
      group_method: truncate
      shuffle: true
      mlm_probability: *mlm_probability
    drop_last: true
    num_workers: 8

  eval_loader:
    name: text
    dataset:
      local: *data_local
      remote: *data_remote
      split: val
      tokenizer_name: *tokenizer_name
      max_seq_len: *max_seq_len
      group_method: truncate
      shuffle: false
      mlm_probability: 0.15 # We always evaluate at 15% masking for consistent comparison
    drop_last: false
    num_workers: 8

  # Optimization
  scheduler:
    name: linear_decay_with_warmup
    t_warmup: 0.06dur # Warmup to the full LR for 6% of the training duration
    alpha_f: 0.02 # Linearly decay to 0.02x the full LR by the end of the training duration

  optimizer:
    name: decoupled_adamw
    lr: 5.0e-4 # Peak learning rate
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5 # Amount of weight decay regularization

  algorithms:
    fused_layernorm: {}

  max_duration: 286720000sp # Subsample the training data for ~275M samples
  eval_interval: 2000ba
  global_train_batch_size: 4096

  # System
  seed: 17
  device_eval_batch_size: 512
  device_train_microbatch_size: 512
  precision: bf16

  # Logging
  progress_bar: false
  log_to_console: true
  console_log_interval: 1ba

  callbacks:
    speed_monitor:
      window_size: 500
    lr_monitor: {}

#   (Optional) W&B logging
#   loggers:
#     wandb:
#       project:
#       entity:

#   (Optional) Checkpoint to local filesystem or remote object store
#   save_interval: 3500ba
#   save_num_checkpoints_to_keep: 1  # Important, this cleans up checkpoints saved to DISK
#  save_folder:      # e.g. './{run_name}/ckpt' (local) or 's3://mybucket/mydir/{run_name}/ckpt' (remote)

#   (Optional) Load from local filesystem or remote object store to
#  start from an existing model checkpoint;
#   e.g. './ckpt/latest-rank{rank}.pt' (local), or
#   's3://mybucket/mydir/ckpt/latest-rank{rank}.pt' (remote)
#   load_path: null
