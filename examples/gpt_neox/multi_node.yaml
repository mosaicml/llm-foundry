run_name: neox-multi-node
image: shivanshupurohit/gpt-neox:112 # Docker image provided by EleutherAI
gpu_num: 16
cluster: # ADD YOUR CLUSTER HERE

integrations:
- integration_type: git_repo
  git_repo: EleutherAI/gpt-neox
  git_commit: 72c80715c366cc4ad623050d6bcb984fe6638814 # main as of 02-27-2023
  path: /workspace/gpt-neox
- integration_type: git_repo
  git_repo: EleutherAI/DeeperSpeed
  git_commit: 7069d10d2c9abac50576c84cb7e45910fafa218c # main as of 02-27-2023
  path: /workspace/DeeperSpeed

command: |
  # Install the requirements for GPT-NeoX
  cd /workspace/gpt-neox
  pip install -r requirements/requirements.txt

  # install EleutherAI's fork of deepspeed
  cd /workspace/DeeperSpeed
  pip install .

  # create a fake hostfile so that GPT-NeoX and DeepSpeed understand the cluster shape
  # Note: this assumes that all nodes have the same number of devices
  python -c '
  import os; \
  import torch; \
  filehandle = open("/tmp/deepspeed_mvapich_hostfile", "w"); \
  world_size = os.environ["WORLD_SIZE"]; \
  device_count = torch.cuda.device_count(); \
  num_nodes = int(world_size) // device_count; \
  _ = [filehandle.write(f"node-{node} slots={device_count}\\n") for node in range(num_nodes)]; \
  '

  # create a GPT-NeoX config file for data paths, eval split, wandb setup, and launcher
  cd /workspace/gpt-neox/configs
  python -c '
  import json; \
  import os; \
  filehandle = open("extra-configs.yml", "w"); \
  values = { \
    "data-path": "data/enwik8/enwik8_text_document", \
    "use_shared_fs": False, \
    "vocab-file": "data/gpt2-vocab.json", \
    "merge-file": "data/gpt2-merges.txt", \
    "eval-interval": 100, \
    "eval-iters": 100, \
    "split": "949,50,1", \
    "use_wandb": True, \
    "wandb_project": <!!! your wandb project name here !!!>, \
    "wandb_team": <!!! your wandb team name here !!!>, \
    "wandb_group": os.environ["RUN_NAME"], \
    "launcher": "mosaicml" \
  }; \
  json.dump(values, filehandle); \
  '

  cd /workspace/gpt-neox

  # download and prepare data
  # see https://github.com/EleutherAI/gpt-neox/blob/72c80715c366cc4ad623050d6bcb984fe6638814/README.md?plain=1#L122)
  # for more details on the command
  python prepare_data.py enwik8 -d ./data

  # run training
  # see https://github.com/EleutherAI/gpt-neox/blob/72c80715c366cc4ad623050d6bcb984fe6638814/README.md?plain=1#L216)
  # for more details on the command
  # see https://github.com/EleutherAI/gpt-neox/blob/72c80715c366cc4ad623050d6bcb984fe6638814/README.md?plain=1#L112
  # for more details on configuration
  ./deepy.py train.py configs/125M-json.yml configs/extra-configs.yml --hostfile /tmp/deepspeed_mvapich_hostfile
