run_name: nemo-megatron-gpt-124m-gpu-16
cluster: r0z0 # Update with your cluster here!
gpu_num: 16

# For the latest NeMo container version, see https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo
image: nvcr.io/nvidia/nemo:22.11

env_variables:
# Configure Python to not buffer stdout and stderr, so output shows up in console immediately
- key: PYTHONUNBUFFERED
  value: '1'

integrations:
- integration_type: apt_packages
  # Install parallel to launch multiple processes per node with rank per process
  packages:
  - parallel

command: |
  # getting the vocab, merge files for the tokenizer
  wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
  wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt

  # Make sure to prepare and download the training data, as defined in NeMo documentation:
  # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/gpt/gpt_training.html

  # Make sure to update the training dataset path below
  seq 0 7 | parallel -u \ 'CUDA_VISIBLE_DEVICES={} RANK=$(( $NODE_RANK * 8 + {} )) \
  python3 examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  --config-path=/workspace/nemo/examples/nlp/language_modeling/conf/ \
  --config-name=megatron_gpt_config.yaml \
  model.data.data_prefix=[1.0,/your_dataset_path_here/] \
  model.tokenizer.vocab_file=gpt2-vocab.json \
  model.tokenizer.merge_file=gpt2-merges.txt \
  model.optim.name="fused_adam" \
  trainer.devices=1'
