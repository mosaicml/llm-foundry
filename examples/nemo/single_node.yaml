run_name: nemo-megatron-gpt-124m-gpu-8
cluster: r0z0 # Update with your cluster here!
gpu_num: 8

# For the latest NeMo container version, see https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo
image: nvcr.io/nvidia/nemo:22.11

env_variables:
# Configure Python to not buffer stdout and stderr, so output shows up in console immediately
- key: PYTHONUNBUFFERED
  value: '1'

command: |
  # Getting the tokenizer vocab and merge files
  wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
  wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt

  # Make sure to prepare and download the training data, as defined in NeMo documentation:
  # https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/gpt/gpt_training.html

  # Make sure to update the training dataset path below
  python3 examples/nlp/language_modeling/megatron_gpt_pretraining.py \
  --config-path=/workspace/nemo/examples/nlp/language_modeling/conf/ \
  --config-name=megatron_gpt_config.yaml \
  model.data.data_prefix=[1.0,/your_dataset_path_here/] \
  model.tokenizer.vocab_file=gpt2-vocab.json \
  model.tokenizer.merge_file=gpt2-merges.txt \
  model.optim.name="fused_adam" \
  trainer.devices=8
