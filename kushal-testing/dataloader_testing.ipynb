{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workdisk/kushal/llm-foundry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Union, List, Optional\n",
    "import os\n",
    "\n",
    "from llmfoundry.data import StreamingTextDataset\n",
    "from llmfoundry.tokenizers import ChronosTokenizerWrapper\n",
    "from chronos.chronos import ChronosConfig, ChronosPipeline\n",
    "\n",
    "from streaming import Stream\n",
    "from streaming.base import MDSWriter, StreamingDataset\n",
    "from transformers import PreTrainedTokenizerBase, PreTrainedTokenizer, AutoConfig\n",
    "\n",
    "from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Union, Tuple, cast\n",
    "\n",
    "%cd /mnt/workdisk/kushal/llm-foundry/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./kushal-testing/hospital_train.csv')\n",
    "test = pd.read_csv('./kushal-testing/hospital_test.csv')\n",
    "context_len, horizon_len = train.shape[1], test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000e+20, -1.4996e+01, -1.4989e+01,  ...,  1.4989e+01,\n",
       "          1.4996e+01,  1.0000e+20]),\n",
       " torch.Tensor,\n",
       " torch.Size([4094]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('amazon/chronos-t5-small')\n",
    "assert hasattr(config, \"chronos_config\"), \"Not a Chronos config file\"\n",
    "chronos_config = ChronosConfig(**config.chronos_config)  # dictionary of content in \"chronos_config\" of amazon/chronos-t5-{} config.json\n",
    "chronos_tokenizer = chronos_config.create_tokenizer()\n",
    "chronos_tokenizer.boundaries, type(chronos_tokenizer.boundaries), chronos_tokenizer.boundaries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(-1.0000e+20): tensor(-1.0000e+20),\n",
       " tensor(-14.9963): tensor(-14.9963),\n",
       " tensor(-14.9890): tensor(-14.9890),\n",
       " tensor(-14.9817): tensor(-14.9817),\n",
       " tensor(-14.9743): tensor(-14.9743),\n",
       " tensor(-14.9670): tensor(-14.9670),\n",
       " tensor(-14.9597): tensor(-14.9597),\n",
       " tensor(-14.9523): tensor(-14.9523),\n",
       " tensor(-14.9450): tensor(-14.9450),\n",
       " tensor(-14.9377): tensor(-14.9377),\n",
       " tensor(-14.9304): tensor(-14.9304),\n",
       " tensor(-14.9230): tensor(-14.9230),\n",
       " tensor(-14.9157): tensor(-14.9157),\n",
       " tensor(-14.9084): tensor(-14.9084),\n",
       " tensor(-14.9010): tensor(-14.9010),\n",
       " tensor(-14.8937): tensor(-14.8937),\n",
       " tensor(-14.8864): tensor(-14.8864),\n",
       " tensor(-14.8790): tensor(-14.8790),\n",
       " tensor(-14.8717): tensor(-14.8717),\n",
       " tensor(-14.8644): tensor(-14.8644),\n",
       " tensor(-14.8570): tensor(-14.8570),\n",
       " tensor(-14.8497): tensor(-14.8497),\n",
       " tensor(-14.8424): tensor(-14.8424),\n",
       " tensor(-14.8350): tensor(-14.8350),\n",
       " tensor(-14.8277): tensor(-14.8277),\n",
       " tensor(-14.8204): tensor(-14.8204),\n",
       " tensor(-14.8130): tensor(-14.8130),\n",
       " tensor(-14.8057): tensor(-14.8057),\n",
       " tensor(-14.7984): tensor(-14.7984),\n",
       " tensor(-14.7911): tensor(-14.7911),\n",
       " tensor(-14.7837): tensor(-14.7837),\n",
       " tensor(-14.7764): tensor(-14.7764),\n",
       " tensor(-14.7691): tensor(-14.7691),\n",
       " tensor(-14.7617): tensor(-14.7617),\n",
       " tensor(-14.7544): tensor(-14.7544),\n",
       " tensor(-14.7471): tensor(-14.7471),\n",
       " tensor(-14.7397): tensor(-14.7397),\n",
       " tensor(-14.7324): tensor(-14.7324),\n",
       " tensor(-14.7251): tensor(-14.7251),\n",
       " tensor(-14.7177): tensor(-14.7177),\n",
       " tensor(-14.7104): tensor(-14.7104),\n",
       " tensor(-14.7031): tensor(-14.7031),\n",
       " tensor(-14.6957): tensor(-14.6957),\n",
       " tensor(-14.6884): tensor(-14.6884),\n",
       " tensor(-14.6811): tensor(-14.6811),\n",
       " tensor(-14.6738): tensor(-14.6738),\n",
       " tensor(-14.6664): tensor(-14.6664),\n",
       " tensor(-14.6591): tensor(-14.6591),\n",
       " tensor(-14.6518): tensor(-14.6518),\n",
       " tensor(-14.6444): tensor(-14.6444),\n",
       " tensor(-14.6371): tensor(-14.6371),\n",
       " tensor(-14.6298): tensor(-14.6298),\n",
       " tensor(-14.6224): tensor(-14.6224),\n",
       " tensor(-14.6151): tensor(-14.6151),\n",
       " tensor(-14.6078): tensor(-14.6078),\n",
       " tensor(-14.6004): tensor(-14.6004),\n",
       " tensor(-14.5931): tensor(-14.5931),\n",
       " tensor(-14.5858): tensor(-14.5858),\n",
       " tensor(-14.5784): tensor(-14.5784),\n",
       " tensor(-14.5711): tensor(-14.5711),\n",
       " tensor(-14.5638): tensor(-14.5638),\n",
       " tensor(-14.5565): tensor(-14.5565),\n",
       " tensor(-14.5491): tensor(-14.5491),\n",
       " tensor(-14.5418): tensor(-14.5418),\n",
       " tensor(-14.5345): tensor(-14.5345),\n",
       " tensor(-14.5271): tensor(-14.5271),\n",
       " tensor(-14.5198): tensor(-14.5198),\n",
       " tensor(-14.5125): tensor(-14.5125),\n",
       " tensor(-14.5051): tensor(-14.5051),\n",
       " tensor(-14.4978): tensor(-14.4978),\n",
       " tensor(-14.4905): tensor(-14.4905),\n",
       " tensor(-14.4831): tensor(-14.4831),\n",
       " tensor(-14.4758): tensor(-14.4758),\n",
       " tensor(-14.4685): tensor(-14.4685),\n",
       " tensor(-14.4611): tensor(-14.4611),\n",
       " tensor(-14.4538): tensor(-14.4538),\n",
       " tensor(-14.4465): tensor(-14.4465),\n",
       " tensor(-14.4391): tensor(-14.4391),\n",
       " tensor(-14.4318): tensor(-14.4318),\n",
       " tensor(-14.4245): tensor(-14.4245),\n",
       " tensor(-14.4172): tensor(-14.4172),\n",
       " tensor(-14.4098): tensor(-14.4098),\n",
       " tensor(-14.4025): tensor(-14.4025),\n",
       " tensor(-14.3952): tensor(-14.3952),\n",
       " tensor(-14.3878): tensor(-14.3878),\n",
       " tensor(-14.3805): tensor(-14.3805),\n",
       " tensor(-14.3732): tensor(-14.3732),\n",
       " tensor(-14.3658): tensor(-14.3658),\n",
       " tensor(-14.3585): tensor(-14.3585),\n",
       " tensor(-14.3512): tensor(-14.3512),\n",
       " tensor(-14.3438): tensor(-14.3438),\n",
       " tensor(-14.3365): tensor(-14.3365),\n",
       " tensor(-14.3292): tensor(-14.3292),\n",
       " tensor(-14.3218): tensor(-14.3218),\n",
       " tensor(-14.3145): tensor(-14.3145),\n",
       " tensor(-14.3072): tensor(-14.3072),\n",
       " tensor(-14.2999): tensor(-14.2999),\n",
       " tensor(-14.2925): tensor(-14.2925),\n",
       " tensor(-14.2852): tensor(-14.2852),\n",
       " tensor(-14.2779): tensor(-14.2779),\n",
       " tensor(-14.2705): tensor(-14.2705),\n",
       " tensor(-14.2632): tensor(-14.2632),\n",
       " tensor(-14.2559): tensor(-14.2559),\n",
       " tensor(-14.2485): tensor(-14.2485),\n",
       " tensor(-14.2412): tensor(-14.2412),\n",
       " tensor(-14.2339): tensor(-14.2339),\n",
       " tensor(-14.2265): tensor(-14.2265),\n",
       " tensor(-14.2192): tensor(-14.2192),\n",
       " tensor(-14.2119): tensor(-14.2119),\n",
       " tensor(-14.2045): tensor(-14.2045),\n",
       " tensor(-14.1972): tensor(-14.1972),\n",
       " tensor(-14.1899): tensor(-14.1899),\n",
       " tensor(-14.1826): tensor(-14.1826),\n",
       " tensor(-14.1752): tensor(-14.1752),\n",
       " tensor(-14.1679): tensor(-14.1679),\n",
       " tensor(-14.1606): tensor(-14.1606),\n",
       " tensor(-14.1532): tensor(-14.1532),\n",
       " tensor(-14.1459): tensor(-14.1459),\n",
       " tensor(-14.1386): tensor(-14.1386),\n",
       " tensor(-14.1312): tensor(-14.1312),\n",
       " tensor(-14.1239): tensor(-14.1239),\n",
       " tensor(-14.1166): tensor(-14.1166),\n",
       " tensor(-14.1092): tensor(-14.1092),\n",
       " tensor(-14.1019): tensor(-14.1019),\n",
       " tensor(-14.0946): tensor(-14.0946),\n",
       " tensor(-14.0872): tensor(-14.0872),\n",
       " tensor(-14.0799): tensor(-14.0799),\n",
       " tensor(-14.0726): tensor(-14.0726),\n",
       " tensor(-14.0652): tensor(-14.0652),\n",
       " tensor(-14.0579): tensor(-14.0579),\n",
       " tensor(-14.0506): tensor(-14.0506),\n",
       " tensor(-14.0433): tensor(-14.0433),\n",
       " tensor(-14.0359): tensor(-14.0359),\n",
       " tensor(-14.0286): tensor(-14.0286),\n",
       " tensor(-14.0213): tensor(-14.0213),\n",
       " tensor(-14.0139): tensor(-14.0139),\n",
       " tensor(-14.0066): tensor(-14.0066),\n",
       " tensor(-13.9993): tensor(-13.9993),\n",
       " tensor(-13.9919): tensor(-13.9919),\n",
       " tensor(-13.9846): tensor(-13.9846),\n",
       " tensor(-13.9773): tensor(-13.9773),\n",
       " tensor(-13.9699): tensor(-13.9699),\n",
       " tensor(-13.9626): tensor(-13.9626),\n",
       " tensor(-13.9553): tensor(-13.9553),\n",
       " tensor(-13.9479): tensor(-13.9479),\n",
       " tensor(-13.9406): tensor(-13.9406),\n",
       " tensor(-13.9333): tensor(-13.9333),\n",
       " tensor(-13.9260): tensor(-13.9260),\n",
       " tensor(-13.9186): tensor(-13.9186),\n",
       " tensor(-13.9113): tensor(-13.9113),\n",
       " tensor(-13.9040): tensor(-13.9040),\n",
       " tensor(-13.8966): tensor(-13.8966),\n",
       " tensor(-13.8893): tensor(-13.8893),\n",
       " tensor(-13.8820): tensor(-13.8820),\n",
       " tensor(-13.8746): tensor(-13.8746),\n",
       " tensor(-13.8673): tensor(-13.8673),\n",
       " tensor(-13.8600): tensor(-13.8600),\n",
       " tensor(-13.8526): tensor(-13.8526),\n",
       " tensor(-13.8453): tensor(-13.8453),\n",
       " tensor(-13.8380): tensor(-13.8380),\n",
       " tensor(-13.8306): tensor(-13.8306),\n",
       " tensor(-13.8233): tensor(-13.8233),\n",
       " tensor(-13.8160): tensor(-13.8160),\n",
       " tensor(-13.8087): tensor(-13.8087),\n",
       " tensor(-13.8013): tensor(-13.8013),\n",
       " tensor(-13.7940): tensor(-13.7940),\n",
       " tensor(-13.7867): tensor(-13.7867),\n",
       " tensor(-13.7793): tensor(-13.7793),\n",
       " tensor(-13.7720): tensor(-13.7720),\n",
       " tensor(-13.7647): tensor(-13.7647),\n",
       " tensor(-13.7573): tensor(-13.7573),\n",
       " tensor(-13.7500): tensor(-13.7500),\n",
       " tensor(-13.7427): tensor(-13.7427),\n",
       " tensor(-13.7353): tensor(-13.7353),\n",
       " tensor(-13.7280): tensor(-13.7280),\n",
       " tensor(-13.7207): tensor(-13.7207),\n",
       " tensor(-13.7133): tensor(-13.7133),\n",
       " tensor(-13.7060): tensor(-13.7060),\n",
       " tensor(-13.6987): tensor(-13.6987),\n",
       " tensor(-13.6913): tensor(-13.6913),\n",
       " tensor(-13.6840): tensor(-13.6840),\n",
       " tensor(-13.6767): tensor(-13.6767),\n",
       " tensor(-13.6694): tensor(-13.6694),\n",
       " tensor(-13.6620): tensor(-13.6620),\n",
       " tensor(-13.6547): tensor(-13.6547),\n",
       " tensor(-13.6474): tensor(-13.6474),\n",
       " tensor(-13.6400): tensor(-13.6400),\n",
       " tensor(-13.6327): tensor(-13.6327),\n",
       " tensor(-13.6254): tensor(-13.6254),\n",
       " tensor(-13.6180): tensor(-13.6180),\n",
       " tensor(-13.6107): tensor(-13.6107),\n",
       " tensor(-13.6034): tensor(-13.6034),\n",
       " tensor(-13.5960): tensor(-13.5960),\n",
       " tensor(-13.5887): tensor(-13.5887),\n",
       " tensor(-13.5814): tensor(-13.5814),\n",
       " tensor(-13.5740): tensor(-13.5740),\n",
       " tensor(-13.5667): tensor(-13.5667),\n",
       " tensor(-13.5594): tensor(-13.5594),\n",
       " tensor(-13.5521): tensor(-13.5521),\n",
       " tensor(-13.5447): tensor(-13.5447),\n",
       " tensor(-13.5374): tensor(-13.5374),\n",
       " tensor(-13.5301): tensor(-13.5301),\n",
       " tensor(-13.5227): tensor(-13.5227),\n",
       " tensor(-13.5154): tensor(-13.5154),\n",
       " tensor(-13.5081): tensor(-13.5081),\n",
       " tensor(-13.5007): tensor(-13.5007),\n",
       " tensor(-13.4934): tensor(-13.4934),\n",
       " tensor(-13.4861): tensor(-13.4861),\n",
       " tensor(-13.4787): tensor(-13.4787),\n",
       " tensor(-13.4714): tensor(-13.4714),\n",
       " tensor(-13.4641): tensor(-13.4641),\n",
       " tensor(-13.4567): tensor(-13.4567),\n",
       " tensor(-13.4494): tensor(-13.4494),\n",
       " tensor(-13.4421): tensor(-13.4421),\n",
       " tensor(-13.4348): tensor(-13.4348),\n",
       " tensor(-13.4274): tensor(-13.4274),\n",
       " tensor(-13.4201): tensor(-13.4201),\n",
       " tensor(-13.4128): tensor(-13.4128),\n",
       " tensor(-13.4054): tensor(-13.4054),\n",
       " tensor(-13.3981): tensor(-13.3981),\n",
       " tensor(-13.3908): tensor(-13.3908),\n",
       " tensor(-13.3834): tensor(-13.3834),\n",
       " tensor(-13.3761): tensor(-13.3761),\n",
       " tensor(-13.3688): tensor(-13.3688),\n",
       " tensor(-13.3614): tensor(-13.3614),\n",
       " tensor(-13.3541): tensor(-13.3541),\n",
       " tensor(-13.3468): tensor(-13.3468),\n",
       " tensor(-13.3394): tensor(-13.3394),\n",
       " tensor(-13.3321): tensor(-13.3321),\n",
       " tensor(-13.3248): tensor(-13.3248),\n",
       " tensor(-13.3174): tensor(-13.3174),\n",
       " tensor(-13.3101): tensor(-13.3101),\n",
       " tensor(-13.3028): tensor(-13.3028),\n",
       " tensor(-13.2955): tensor(-13.2955),\n",
       " tensor(-13.2881): tensor(-13.2881),\n",
       " tensor(-13.2808): tensor(-13.2808),\n",
       " tensor(-13.2735): tensor(-13.2735),\n",
       " tensor(-13.2661): tensor(-13.2661),\n",
       " tensor(-13.2588): tensor(-13.2588),\n",
       " tensor(-13.2515): tensor(-13.2515),\n",
       " tensor(-13.2441): tensor(-13.2441),\n",
       " tensor(-13.2368): tensor(-13.2368),\n",
       " tensor(-13.2295): tensor(-13.2295),\n",
       " tensor(-13.2221): tensor(-13.2221),\n",
       " tensor(-13.2148): tensor(-13.2148),\n",
       " tensor(-13.2075): tensor(-13.2075),\n",
       " tensor(-13.2001): tensor(-13.2001),\n",
       " tensor(-13.1928): tensor(-13.1928),\n",
       " tensor(-13.1855): tensor(-13.1855),\n",
       " tensor(-13.1782): tensor(-13.1782),\n",
       " tensor(-13.1708): tensor(-13.1708),\n",
       " tensor(-13.1635): tensor(-13.1635),\n",
       " tensor(-13.1562): tensor(-13.1562),\n",
       " tensor(-13.1488): tensor(-13.1488),\n",
       " tensor(-13.1415): tensor(-13.1415),\n",
       " tensor(-13.1342): tensor(-13.1342),\n",
       " tensor(-13.1268): tensor(-13.1268),\n",
       " tensor(-13.1195): tensor(-13.1195),\n",
       " tensor(-13.1122): tensor(-13.1122),\n",
       " tensor(-13.1048): tensor(-13.1048),\n",
       " tensor(-13.0975): tensor(-13.0975),\n",
       " tensor(-13.0902): tensor(-13.0902),\n",
       " tensor(-13.0828): tensor(-13.0828),\n",
       " tensor(-13.0755): tensor(-13.0755),\n",
       " tensor(-13.0682): tensor(-13.0682),\n",
       " tensor(-13.0609): tensor(-13.0609),\n",
       " tensor(-13.0535): tensor(-13.0535),\n",
       " tensor(-13.0462): tensor(-13.0462),\n",
       " tensor(-13.0389): tensor(-13.0389),\n",
       " tensor(-13.0315): tensor(-13.0315),\n",
       " tensor(-13.0242): tensor(-13.0242),\n",
       " tensor(-13.0169): tensor(-13.0169),\n",
       " tensor(-13.0095): tensor(-13.0095),\n",
       " tensor(-13.0022): tensor(-13.0022),\n",
       " tensor(-12.9949): tensor(-12.9949),\n",
       " tensor(-12.9875): tensor(-12.9875),\n",
       " tensor(-12.9802): tensor(-12.9802),\n",
       " tensor(-12.9729): tensor(-12.9729),\n",
       " tensor(-12.9655): tensor(-12.9655),\n",
       " tensor(-12.9582): tensor(-12.9582),\n",
       " tensor(-12.9509): tensor(-12.9509),\n",
       " tensor(-12.9435): tensor(-12.9435),\n",
       " tensor(-12.9362): tensor(-12.9362),\n",
       " tensor(-12.9289): tensor(-12.9289),\n",
       " tensor(-12.9216): tensor(-12.9216),\n",
       " tensor(-12.9142): tensor(-12.9142),\n",
       " tensor(-12.9069): tensor(-12.9069),\n",
       " tensor(-12.8996): tensor(-12.8996),\n",
       " tensor(-12.8922): tensor(-12.8922),\n",
       " tensor(-12.8849): tensor(-12.8849),\n",
       " tensor(-12.8776): tensor(-12.8776),\n",
       " tensor(-12.8702): tensor(-12.8702),\n",
       " tensor(-12.8629): tensor(-12.8629),\n",
       " tensor(-12.8556): tensor(-12.8556),\n",
       " tensor(-12.8482): tensor(-12.8482),\n",
       " tensor(-12.8409): tensor(-12.8409),\n",
       " tensor(-12.8336): tensor(-12.8336),\n",
       " tensor(-12.8262): tensor(-12.8262),\n",
       " tensor(-12.8189): tensor(-12.8189),\n",
       " tensor(-12.8116): tensor(-12.8116),\n",
       " tensor(-12.8043): tensor(-12.8043),\n",
       " tensor(-12.7969): tensor(-12.7969),\n",
       " tensor(-12.7896): tensor(-12.7896),\n",
       " tensor(-12.7823): tensor(-12.7823),\n",
       " tensor(-12.7749): tensor(-12.7749),\n",
       " tensor(-12.7676): tensor(-12.7676),\n",
       " tensor(-12.7603): tensor(-12.7603),\n",
       " tensor(-12.7529): tensor(-12.7529),\n",
       " tensor(-12.7456): tensor(-12.7456),\n",
       " tensor(-12.7383): tensor(-12.7383),\n",
       " tensor(-12.7309): tensor(-12.7309),\n",
       " tensor(-12.7236): tensor(-12.7236),\n",
       " tensor(-12.7163): tensor(-12.7163),\n",
       " tensor(-12.7089): tensor(-12.7089),\n",
       " tensor(-12.7016): tensor(-12.7016),\n",
       " tensor(-12.6943): tensor(-12.6943),\n",
       " tensor(-12.6870): tensor(-12.6870),\n",
       " tensor(-12.6796): tensor(-12.6796),\n",
       " tensor(-12.6723): tensor(-12.6723),\n",
       " tensor(-12.6650): tensor(-12.6650),\n",
       " tensor(-12.6576): tensor(-12.6576),\n",
       " tensor(-12.6503): tensor(-12.6503),\n",
       " tensor(-12.6430): tensor(-12.6430),\n",
       " tensor(-12.6356): tensor(-12.6356),\n",
       " tensor(-12.6283): tensor(-12.6283),\n",
       " tensor(-12.6210): tensor(-12.6210),\n",
       " tensor(-12.6136): tensor(-12.6136),\n",
       " tensor(-12.6063): tensor(-12.6063),\n",
       " tensor(-12.5990): tensor(-12.5990),\n",
       " tensor(-12.5916): tensor(-12.5916),\n",
       " tensor(-12.5843): tensor(-12.5843),\n",
       " tensor(-12.5770): tensor(-12.5770),\n",
       " tensor(-12.5696): tensor(-12.5696),\n",
       " tensor(-12.5623): tensor(-12.5623),\n",
       " tensor(-12.5550): tensor(-12.5550),\n",
       " tensor(-12.5477): tensor(-12.5477),\n",
       " tensor(-12.5403): tensor(-12.5403),\n",
       " tensor(-12.5330): tensor(-12.5330),\n",
       " tensor(-12.5257): tensor(-12.5257),\n",
       " tensor(-12.5183): tensor(-12.5183),\n",
       " tensor(-12.5110): tensor(-12.5110),\n",
       " tensor(-12.5037): tensor(-12.5037),\n",
       " tensor(-12.4963): tensor(-12.4963),\n",
       " tensor(-12.4890): tensor(-12.4890),\n",
       " tensor(-12.4817): tensor(-12.4817),\n",
       " tensor(-12.4743): tensor(-12.4743),\n",
       " tensor(-12.4670): tensor(-12.4670),\n",
       " tensor(-12.4597): tensor(-12.4597),\n",
       " tensor(-12.4523): tensor(-12.4523),\n",
       " tensor(-12.4450): tensor(-12.4450),\n",
       " tensor(-12.4377): tensor(-12.4377),\n",
       " tensor(-12.4304): tensor(-12.4304),\n",
       " tensor(-12.4230): tensor(-12.4230),\n",
       " tensor(-12.4157): tensor(-12.4157),\n",
       " tensor(-12.4084): tensor(-12.4084),\n",
       " tensor(-12.4010): tensor(-12.4010),\n",
       " tensor(-12.3937): tensor(-12.3937),\n",
       " tensor(-12.3864): tensor(-12.3864),\n",
       " tensor(-12.3790): tensor(-12.3790),\n",
       " tensor(-12.3717): tensor(-12.3717),\n",
       " tensor(-12.3644): tensor(-12.3644),\n",
       " tensor(-12.3570): tensor(-12.3570),\n",
       " tensor(-12.3497): tensor(-12.3497),\n",
       " tensor(-12.3424): tensor(-12.3424),\n",
       " tensor(-12.3350): tensor(-12.3350),\n",
       " tensor(-12.3277): tensor(-12.3277),\n",
       " tensor(-12.3204): tensor(-12.3204),\n",
       " tensor(-12.3130): tensor(-12.3130),\n",
       " tensor(-12.3057): tensor(-12.3057),\n",
       " tensor(-12.2984): tensor(-12.2984),\n",
       " tensor(-12.2911): tensor(-12.2911),\n",
       " tensor(-12.2837): tensor(-12.2837),\n",
       " tensor(-12.2764): tensor(-12.2764),\n",
       " tensor(-12.2691): tensor(-12.2691),\n",
       " tensor(-12.2617): tensor(-12.2617),\n",
       " tensor(-12.2544): tensor(-12.2544),\n",
       " tensor(-12.2471): tensor(-12.2471),\n",
       " tensor(-12.2397): tensor(-12.2397),\n",
       " tensor(-12.2324): tensor(-12.2324),\n",
       " tensor(-12.2251): tensor(-12.2251),\n",
       " tensor(-12.2177): tensor(-12.2177),\n",
       " tensor(-12.2104): tensor(-12.2104),\n",
       " tensor(-12.2031): tensor(-12.2031),\n",
       " tensor(-12.1957): tensor(-12.1957),\n",
       " tensor(-12.1884): tensor(-12.1884),\n",
       " tensor(-12.1811): tensor(-12.1811),\n",
       " tensor(-12.1738): tensor(-12.1738),\n",
       " tensor(-12.1664): tensor(-12.1664),\n",
       " tensor(-12.1591): tensor(-12.1591),\n",
       " tensor(-12.1518): tensor(-12.1518),\n",
       " tensor(-12.1444): tensor(-12.1444),\n",
       " tensor(-12.1371): tensor(-12.1371),\n",
       " tensor(-12.1298): tensor(-12.1298),\n",
       " tensor(-12.1224): tensor(-12.1224),\n",
       " tensor(-12.1151): tensor(-12.1151),\n",
       " tensor(-12.1078): tensor(-12.1078),\n",
       " tensor(-12.1004): tensor(-12.1004),\n",
       " tensor(-12.0931): tensor(-12.0931),\n",
       " tensor(-12.0858): tensor(-12.0858),\n",
       " tensor(-12.0784): tensor(-12.0784),\n",
       " tensor(-12.0711): tensor(-12.0711),\n",
       " tensor(-12.0638): tensor(-12.0638),\n",
       " tensor(-12.0565): tensor(-12.0565),\n",
       " tensor(-12.0491): tensor(-12.0491),\n",
       " tensor(-12.0418): tensor(-12.0418),\n",
       " tensor(-12.0345): tensor(-12.0345),\n",
       " tensor(-12.0271): tensor(-12.0271),\n",
       " tensor(-12.0198): tensor(-12.0198),\n",
       " tensor(-12.0125): tensor(-12.0125),\n",
       " tensor(-12.0051): tensor(-12.0051),\n",
       " tensor(-11.9978): tensor(-11.9978),\n",
       " tensor(-11.9905): tensor(-11.9905),\n",
       " tensor(-11.9831): tensor(-11.9831),\n",
       " tensor(-11.9758): tensor(-11.9758),\n",
       " tensor(-11.9685): tensor(-11.9685),\n",
       " tensor(-11.9611): tensor(-11.9611),\n",
       " tensor(-11.9538): tensor(-11.9538),\n",
       " tensor(-11.9465): tensor(-11.9465),\n",
       " tensor(-11.9391): tensor(-11.9391),\n",
       " tensor(-11.9318): tensor(-11.9318),\n",
       " tensor(-11.9245): tensor(-11.9245),\n",
       " tensor(-11.9172): tensor(-11.9172),\n",
       " tensor(-11.9098): tensor(-11.9098),\n",
       " tensor(-11.9025): tensor(-11.9025),\n",
       " tensor(-11.8952): tensor(-11.8952),\n",
       " tensor(-11.8878): tensor(-11.8878),\n",
       " tensor(-11.8805): tensor(-11.8805),\n",
       " tensor(-11.8732): tensor(-11.8732),\n",
       " tensor(-11.8658): tensor(-11.8658),\n",
       " tensor(-11.8585): tensor(-11.8585),\n",
       " tensor(-11.8512): tensor(-11.8512),\n",
       " tensor(-11.8438): tensor(-11.8438),\n",
       " tensor(-11.8365): tensor(-11.8365),\n",
       " tensor(-11.8292): tensor(-11.8292),\n",
       " tensor(-11.8218): tensor(-11.8218),\n",
       " tensor(-11.8145): tensor(-11.8145),\n",
       " tensor(-11.8072): tensor(-11.8072),\n",
       " tensor(-11.7999): tensor(-11.7999),\n",
       " tensor(-11.7925): tensor(-11.7925),\n",
       " tensor(-11.7852): tensor(-11.7852),\n",
       " tensor(-11.7779): tensor(-11.7779),\n",
       " tensor(-11.7705): tensor(-11.7705),\n",
       " tensor(-11.7632): tensor(-11.7632),\n",
       " tensor(-11.7559): tensor(-11.7559),\n",
       " tensor(-11.7485): tensor(-11.7485),\n",
       " tensor(-11.7412): tensor(-11.7412),\n",
       " tensor(-11.7339): tensor(-11.7339),\n",
       " tensor(-11.7265): tensor(-11.7265),\n",
       " tensor(-11.7192): tensor(-11.7192),\n",
       " tensor(-11.7119): tensor(-11.7119),\n",
       " tensor(-11.7045): tensor(-11.7045),\n",
       " tensor(-11.6972): tensor(-11.6972),\n",
       " tensor(-11.6899): tensor(-11.6899),\n",
       " tensor(-11.6826): tensor(-11.6826),\n",
       " tensor(-11.6752): tensor(-11.6752),\n",
       " tensor(-11.6679): tensor(-11.6679),\n",
       " tensor(-11.6606): tensor(-11.6606),\n",
       " tensor(-11.6532): tensor(-11.6532),\n",
       " tensor(-11.6459): tensor(-11.6459),\n",
       " tensor(-11.6386): tensor(-11.6386),\n",
       " tensor(-11.6312): tensor(-11.6312),\n",
       " tensor(-11.6239): tensor(-11.6239),\n",
       " tensor(-11.6166): tensor(-11.6166),\n",
       " tensor(-11.6092): tensor(-11.6092),\n",
       " tensor(-11.6019): tensor(-11.6019),\n",
       " tensor(-11.5946): tensor(-11.5946),\n",
       " tensor(-11.5872): tensor(-11.5872),\n",
       " tensor(-11.5799): tensor(-11.5799),\n",
       " tensor(-11.5726): tensor(-11.5726),\n",
       " tensor(-11.5652): tensor(-11.5652),\n",
       " tensor(-11.5579): tensor(-11.5579),\n",
       " tensor(-11.5506): tensor(-11.5506),\n",
       " tensor(-11.5433): tensor(-11.5433),\n",
       " tensor(-11.5359): tensor(-11.5359),\n",
       " tensor(-11.5286): tensor(-11.5286),\n",
       " tensor(-11.5213): tensor(-11.5213),\n",
       " tensor(-11.5139): tensor(-11.5139),\n",
       " tensor(-11.5066): tensor(-11.5066),\n",
       " tensor(-11.4993): tensor(-11.4993),\n",
       " tensor(-11.4919): tensor(-11.4919),\n",
       " tensor(-11.4846): tensor(-11.4846),\n",
       " tensor(-11.4773): tensor(-11.4773),\n",
       " tensor(-11.4699): tensor(-11.4699),\n",
       " tensor(-11.4626): tensor(-11.4626),\n",
       " tensor(-11.4553): tensor(-11.4553),\n",
       " tensor(-11.4479): tensor(-11.4479),\n",
       " tensor(-11.4406): tensor(-11.4406),\n",
       " tensor(-11.4333): tensor(-11.4333),\n",
       " tensor(-11.4260): tensor(-11.4260),\n",
       " tensor(-11.4186): tensor(-11.4186),\n",
       " tensor(-11.4113): tensor(-11.4113),\n",
       " tensor(-11.4040): tensor(-11.4040),\n",
       " tensor(-11.3966): tensor(-11.3966),\n",
       " tensor(-11.3893): tensor(-11.3893),\n",
       " tensor(-11.3820): tensor(-11.3820),\n",
       " tensor(-11.3746): tensor(-11.3746),\n",
       " tensor(-11.3673): tensor(-11.3673),\n",
       " tensor(-11.3600): tensor(-11.3600),\n",
       " tensor(-11.3526): tensor(-11.3526),\n",
       " tensor(-11.3453): tensor(-11.3453),\n",
       " tensor(-11.3380): tensor(-11.3380),\n",
       " tensor(-11.3306): tensor(-11.3306),\n",
       " tensor(-11.3233): tensor(-11.3233),\n",
       " tensor(-11.3160): tensor(-11.3160),\n",
       " tensor(-11.3087): tensor(-11.3087),\n",
       " tensor(-11.3013): tensor(-11.3013),\n",
       " tensor(-11.2940): tensor(-11.2940),\n",
       " tensor(-11.2867): tensor(-11.2867),\n",
       " tensor(-11.2793): tensor(-11.2793),\n",
       " tensor(-11.2720): tensor(-11.2720),\n",
       " tensor(-11.2647): tensor(-11.2647),\n",
       " tensor(-11.2573): tensor(-11.2573),\n",
       " tensor(-11.2500): tensor(-11.2500),\n",
       " tensor(-11.2427): tensor(-11.2427),\n",
       " tensor(-11.2353): tensor(-11.2353),\n",
       " tensor(-11.2280): tensor(-11.2280),\n",
       " tensor(-11.2207): tensor(-11.2207),\n",
       " tensor(-11.2133): tensor(-11.2133),\n",
       " tensor(-11.2060): tensor(-11.2060),\n",
       " tensor(-11.1987): tensor(-11.1987),\n",
       " tensor(-11.1913): tensor(-11.1913),\n",
       " tensor(-11.1840): tensor(-11.1840),\n",
       " tensor(-11.1767): tensor(-11.1767),\n",
       " tensor(-11.1694): tensor(-11.1694),\n",
       " tensor(-11.1620): tensor(-11.1620),\n",
       " tensor(-11.1547): tensor(-11.1547),\n",
       " tensor(-11.1474): tensor(-11.1474),\n",
       " tensor(-11.1400): tensor(-11.1400),\n",
       " tensor(-11.1327): tensor(-11.1327),\n",
       " tensor(-11.1254): tensor(-11.1254),\n",
       " tensor(-11.1180): tensor(-11.1180),\n",
       " tensor(-11.1107): tensor(-11.1107),\n",
       " tensor(-11.1034): tensor(-11.1034),\n",
       " tensor(-11.0960): tensor(-11.0960),\n",
       " tensor(-11.0887): tensor(-11.0887),\n",
       " tensor(-11.0814): tensor(-11.0814),\n",
       " tensor(-11.0740): tensor(-11.0740),\n",
       " tensor(-11.0667): tensor(-11.0667),\n",
       " tensor(-11.0594): tensor(-11.0594),\n",
       " tensor(-11.0521): tensor(-11.0521),\n",
       " tensor(-11.0447): tensor(-11.0447),\n",
       " tensor(-11.0374): tensor(-11.0374),\n",
       " tensor(-11.0301): tensor(-11.0301),\n",
       " tensor(-11.0227): tensor(-11.0227),\n",
       " tensor(-11.0154): tensor(-11.0154),\n",
       " tensor(-11.0081): tensor(-11.0081),\n",
       " tensor(-11.0007): tensor(-11.0007),\n",
       " tensor(-10.9934): tensor(-10.9934),\n",
       " tensor(-10.9861): tensor(-10.9861),\n",
       " tensor(-10.9787): tensor(-10.9787),\n",
       " tensor(-10.9714): tensor(-10.9714),\n",
       " tensor(-10.9641): tensor(-10.9641),\n",
       " tensor(-10.9567): tensor(-10.9567),\n",
       " tensor(-10.9494): tensor(-10.9494),\n",
       " tensor(-10.9421): tensor(-10.9421),\n",
       " tensor(-10.9348): tensor(-10.9348),\n",
       " tensor(-10.9274): tensor(-10.9274),\n",
       " tensor(-10.9201): tensor(-10.9201),\n",
       " tensor(-10.9128): tensor(-10.9128),\n",
       " tensor(-10.9054): tensor(-10.9054),\n",
       " tensor(-10.8981): tensor(-10.8981),\n",
       " tensor(-10.8908): tensor(-10.8908),\n",
       " tensor(-10.8834): tensor(-10.8834),\n",
       " tensor(-10.8761): tensor(-10.8761),\n",
       " tensor(-10.8688): tensor(-10.8688),\n",
       " tensor(-10.8614): tensor(-10.8614),\n",
       " tensor(-10.8541): tensor(-10.8541),\n",
       " tensor(-10.8468): tensor(-10.8468),\n",
       " tensor(-10.8394): tensor(-10.8394),\n",
       " tensor(-10.8321): tensor(-10.8321),\n",
       " tensor(-10.8248): tensor(-10.8248),\n",
       " tensor(-10.8174): tensor(-10.8174),\n",
       " tensor(-10.8101): tensor(-10.8101),\n",
       " tensor(-10.8028): tensor(-10.8028),\n",
       " tensor(-10.7955): tensor(-10.7955),\n",
       " tensor(-10.7881): tensor(-10.7881),\n",
       " tensor(-10.7808): tensor(-10.7808),\n",
       " tensor(-10.7735): tensor(-10.7735),\n",
       " tensor(-10.7661): tensor(-10.7661),\n",
       " tensor(-10.7588): tensor(-10.7588),\n",
       " tensor(-10.7515): tensor(-10.7515),\n",
       " tensor(-10.7441): tensor(-10.7441),\n",
       " tensor(-10.7368): tensor(-10.7368),\n",
       " tensor(-10.7295): tensor(-10.7295),\n",
       " tensor(-10.7221): tensor(-10.7221),\n",
       " tensor(-10.7148): tensor(-10.7148),\n",
       " tensor(-10.7075): tensor(-10.7075),\n",
       " tensor(-10.7001): tensor(-10.7001),\n",
       " tensor(-10.6928): tensor(-10.6928),\n",
       " tensor(-10.6855): tensor(-10.6855),\n",
       " tensor(-10.6782): tensor(-10.6782),\n",
       " tensor(-10.6708): tensor(-10.6708),\n",
       " tensor(-10.6635): tensor(-10.6635),\n",
       " tensor(-10.6562): tensor(-10.6562),\n",
       " tensor(-10.6488): tensor(-10.6488),\n",
       " tensor(-10.6415): tensor(-10.6415),\n",
       " tensor(-10.6342): tensor(-10.6342),\n",
       " tensor(-10.6268): tensor(-10.6268),\n",
       " tensor(-10.6195): tensor(-10.6195),\n",
       " tensor(-10.6122): tensor(-10.6122),\n",
       " tensor(-10.6048): tensor(-10.6048),\n",
       " tensor(-10.5975): tensor(-10.5975),\n",
       " tensor(-10.5902): tensor(-10.5902),\n",
       " tensor(-10.5828): tensor(-10.5828),\n",
       " tensor(-10.5755): tensor(-10.5755),\n",
       " tensor(-10.5682): tensor(-10.5682),\n",
       " tensor(-10.5609): tensor(-10.5609),\n",
       " tensor(-10.5535): tensor(-10.5535),\n",
       " tensor(-10.5462): tensor(-10.5462),\n",
       " tensor(-10.5389): tensor(-10.5389),\n",
       " tensor(-10.5315): tensor(-10.5315),\n",
       " tensor(-10.5242): tensor(-10.5242),\n",
       " tensor(-10.5169): tensor(-10.5169),\n",
       " tensor(-10.5095): tensor(-10.5095),\n",
       " tensor(-10.5022): tensor(-10.5022),\n",
       " tensor(-10.4949): tensor(-10.4949),\n",
       " tensor(-10.4875): tensor(-10.4875),\n",
       " tensor(-10.4802): tensor(-10.4802),\n",
       " tensor(-10.4729): tensor(-10.4729),\n",
       " tensor(-10.4655): tensor(-10.4655),\n",
       " tensor(-10.4582): tensor(-10.4582),\n",
       " tensor(-10.4509): tensor(-10.4509),\n",
       " tensor(-10.4435): tensor(-10.4435),\n",
       " tensor(-10.4362): tensor(-10.4362),\n",
       " tensor(-10.4289): tensor(-10.4289),\n",
       " tensor(-10.4216): tensor(-10.4216),\n",
       " tensor(-10.4142): tensor(-10.4142),\n",
       " tensor(-10.4069): tensor(-10.4069),\n",
       " tensor(-10.3996): tensor(-10.3996),\n",
       " tensor(-10.3922): tensor(-10.3922),\n",
       " tensor(-10.3849): tensor(-10.3849),\n",
       " tensor(-10.3776): tensor(-10.3776),\n",
       " tensor(-10.3702): tensor(-10.3702),\n",
       " tensor(-10.3629): tensor(-10.3629),\n",
       " tensor(-10.3556): tensor(-10.3556),\n",
       " tensor(-10.3482): tensor(-10.3482),\n",
       " tensor(-10.3409): tensor(-10.3409),\n",
       " tensor(-10.3336): tensor(-10.3336),\n",
       " tensor(-10.3262): tensor(-10.3262),\n",
       " tensor(-10.3189): tensor(-10.3189),\n",
       " tensor(-10.3116): tensor(-10.3116),\n",
       " tensor(-10.3043): tensor(-10.3043),\n",
       " tensor(-10.2969): tensor(-10.2969),\n",
       " tensor(-10.2896): tensor(-10.2896),\n",
       " tensor(-10.2823): tensor(-10.2823),\n",
       " tensor(-10.2749): tensor(-10.2749),\n",
       " tensor(-10.2676): tensor(-10.2676),\n",
       " tensor(-10.2603): tensor(-10.2603),\n",
       " tensor(-10.2529): tensor(-10.2529),\n",
       " tensor(-10.2456): tensor(-10.2456),\n",
       " tensor(-10.2383): tensor(-10.2383),\n",
       " tensor(-10.2309): tensor(-10.2309),\n",
       " tensor(-10.2236): tensor(-10.2236),\n",
       " tensor(-10.2163): tensor(-10.2163),\n",
       " tensor(-10.2089): tensor(-10.2089),\n",
       " tensor(-10.2016): tensor(-10.2016),\n",
       " tensor(-10.1943): tensor(-10.1943),\n",
       " tensor(-10.1870): tensor(-10.1870),\n",
       " tensor(-10.1796): tensor(-10.1796),\n",
       " tensor(-10.1723): tensor(-10.1723),\n",
       " tensor(-10.1650): tensor(-10.1650),\n",
       " tensor(-10.1576): tensor(-10.1576),\n",
       " tensor(-10.1503): tensor(-10.1503),\n",
       " tensor(-10.1430): tensor(-10.1430),\n",
       " tensor(-10.1356): tensor(-10.1356),\n",
       " tensor(-10.1283): tensor(-10.1283),\n",
       " tensor(-10.1210): tensor(-10.1210),\n",
       " tensor(-10.1136): tensor(-10.1136),\n",
       " tensor(-10.1063): tensor(-10.1063),\n",
       " tensor(-10.0990): tensor(-10.0990),\n",
       " tensor(-10.0916): tensor(-10.0916),\n",
       " tensor(-10.0843): tensor(-10.0843),\n",
       " tensor(-10.0770): tensor(-10.0770),\n",
       " tensor(-10.0696): tensor(-10.0696),\n",
       " tensor(-10.0623): tensor(-10.0623),\n",
       " tensor(-10.0550): tensor(-10.0550),\n",
       " tensor(-10.0477): tensor(-10.0477),\n",
       " tensor(-10.0403): tensor(-10.0403),\n",
       " tensor(-10.0330): tensor(-10.0330),\n",
       " tensor(-10.0257): tensor(-10.0257),\n",
       " tensor(-10.0183): tensor(-10.0183),\n",
       " tensor(-10.0110): tensor(-10.0110),\n",
       " tensor(-10.0037): tensor(-10.0037),\n",
       " tensor(-9.9963): tensor(-9.9963),\n",
       " tensor(-9.9890): tensor(-9.9890),\n",
       " tensor(-9.9817): tensor(-9.9817),\n",
       " tensor(-9.9743): tensor(-9.9743),\n",
       " tensor(-9.9670): tensor(-9.9670),\n",
       " tensor(-9.9597): tensor(-9.9597),\n",
       " tensor(-9.9523): tensor(-9.9523),\n",
       " tensor(-9.9450): tensor(-9.9450),\n",
       " tensor(-9.9377): tensor(-9.9377),\n",
       " tensor(-9.9304): tensor(-9.9304),\n",
       " tensor(-9.9230): tensor(-9.9230),\n",
       " tensor(-9.9157): tensor(-9.9157),\n",
       " tensor(-9.9084): tensor(-9.9084),\n",
       " tensor(-9.9010): tensor(-9.9010),\n",
       " tensor(-9.8937): tensor(-9.8937),\n",
       " tensor(-9.8864): tensor(-9.8864),\n",
       " tensor(-9.8790): tensor(-9.8790),\n",
       " tensor(-9.8717): tensor(-9.8717),\n",
       " tensor(-9.8644): tensor(-9.8644),\n",
       " tensor(-9.8570): tensor(-9.8570),\n",
       " tensor(-9.8497): tensor(-9.8497),\n",
       " tensor(-9.8424): tensor(-9.8424),\n",
       " tensor(-9.8350): tensor(-9.8350),\n",
       " tensor(-9.8277): tensor(-9.8277),\n",
       " tensor(-9.8204): tensor(-9.8204),\n",
       " tensor(-9.8130): tensor(-9.8130),\n",
       " tensor(-9.8057): tensor(-9.8057),\n",
       " tensor(-9.7984): tensor(-9.7984),\n",
       " tensor(-9.7911): tensor(-9.7911),\n",
       " tensor(-9.7837): tensor(-9.7837),\n",
       " tensor(-9.7764): tensor(-9.7764),\n",
       " tensor(-9.7691): tensor(-9.7691),\n",
       " tensor(-9.7617): tensor(-9.7617),\n",
       " tensor(-9.7544): tensor(-9.7544),\n",
       " tensor(-9.7471): tensor(-9.7471),\n",
       " tensor(-9.7397): tensor(-9.7397),\n",
       " tensor(-9.7324): tensor(-9.7324),\n",
       " tensor(-9.7251): tensor(-9.7251),\n",
       " tensor(-9.7177): tensor(-9.7177),\n",
       " tensor(-9.7104): tensor(-9.7104),\n",
       " tensor(-9.7031): tensor(-9.7031),\n",
       " tensor(-9.6957): tensor(-9.6957),\n",
       " tensor(-9.6884): tensor(-9.6884),\n",
       " tensor(-9.6811): tensor(-9.6811),\n",
       " tensor(-9.6738): tensor(-9.6738),\n",
       " tensor(-9.6664): tensor(-9.6664),\n",
       " tensor(-9.6591): tensor(-9.6591),\n",
       " tensor(-9.6518): tensor(-9.6518),\n",
       " tensor(-9.6444): tensor(-9.6444),\n",
       " tensor(-9.6371): tensor(-9.6371),\n",
       " tensor(-9.6298): tensor(-9.6298),\n",
       " tensor(-9.6224): tensor(-9.6224),\n",
       " tensor(-9.6151): tensor(-9.6151),\n",
       " tensor(-9.6078): tensor(-9.6078),\n",
       " tensor(-9.6004): tensor(-9.6004),\n",
       " tensor(-9.5931): tensor(-9.5931),\n",
       " tensor(-9.5858): tensor(-9.5858),\n",
       " tensor(-9.5784): tensor(-9.5784),\n",
       " tensor(-9.5711): tensor(-9.5711),\n",
       " tensor(-9.5638): tensor(-9.5638),\n",
       " tensor(-9.5565): tensor(-9.5565),\n",
       " tensor(-9.5491): tensor(-9.5491),\n",
       " tensor(-9.5418): tensor(-9.5418),\n",
       " tensor(-9.5345): tensor(-9.5345),\n",
       " tensor(-9.5271): tensor(-9.5271),\n",
       " tensor(-9.5198): tensor(-9.5198),\n",
       " tensor(-9.5125): tensor(-9.5125),\n",
       " tensor(-9.5051): tensor(-9.5051),\n",
       " tensor(-9.4978): tensor(-9.4978),\n",
       " tensor(-9.4905): tensor(-9.4905),\n",
       " tensor(-9.4831): tensor(-9.4831),\n",
       " tensor(-9.4758): tensor(-9.4758),\n",
       " tensor(-9.4685): tensor(-9.4685),\n",
       " tensor(-9.4611): tensor(-9.4611),\n",
       " tensor(-9.4538): tensor(-9.4538),\n",
       " tensor(-9.4465): tensor(-9.4465),\n",
       " tensor(-9.4391): tensor(-9.4391),\n",
       " tensor(-9.4318): tensor(-9.4318),\n",
       " tensor(-9.4245): tensor(-9.4245),\n",
       " tensor(-9.4172): tensor(-9.4172),\n",
       " tensor(-9.4098): tensor(-9.4098),\n",
       " tensor(-9.4025): tensor(-9.4025),\n",
       " tensor(-9.3952): tensor(-9.3952),\n",
       " tensor(-9.3878): tensor(-9.3878),\n",
       " tensor(-9.3805): tensor(-9.3805),\n",
       " tensor(-9.3732): tensor(-9.3732),\n",
       " tensor(-9.3658): tensor(-9.3658),\n",
       " tensor(-9.3585): tensor(-9.3585),\n",
       " tensor(-9.3512): tensor(-9.3512),\n",
       " tensor(-9.3438): tensor(-9.3438),\n",
       " tensor(-9.3365): tensor(-9.3365),\n",
       " tensor(-9.3292): tensor(-9.3292),\n",
       " tensor(-9.3218): tensor(-9.3218),\n",
       " tensor(-9.3145): tensor(-9.3145),\n",
       " tensor(-9.3072): tensor(-9.3072),\n",
       " tensor(-9.2999): tensor(-9.2999),\n",
       " tensor(-9.2925): tensor(-9.2925),\n",
       " tensor(-9.2852): tensor(-9.2852),\n",
       " tensor(-9.2779): tensor(-9.2779),\n",
       " tensor(-9.2705): tensor(-9.2705),\n",
       " tensor(-9.2632): tensor(-9.2632),\n",
       " tensor(-9.2559): tensor(-9.2559),\n",
       " tensor(-9.2485): tensor(-9.2485),\n",
       " tensor(-9.2412): tensor(-9.2412),\n",
       " tensor(-9.2339): tensor(-9.2339),\n",
       " tensor(-9.2265): tensor(-9.2265),\n",
       " tensor(-9.2192): tensor(-9.2192),\n",
       " tensor(-9.2119): tensor(-9.2119),\n",
       " tensor(-9.2045): tensor(-9.2045),\n",
       " tensor(-9.1972): tensor(-9.1972),\n",
       " tensor(-9.1899): tensor(-9.1899),\n",
       " tensor(-9.1826): tensor(-9.1826),\n",
       " tensor(-9.1752): tensor(-9.1752),\n",
       " tensor(-9.1679): tensor(-9.1679),\n",
       " tensor(-9.1606): tensor(-9.1606),\n",
       " tensor(-9.1532): tensor(-9.1532),\n",
       " tensor(-9.1459): tensor(-9.1459),\n",
       " tensor(-9.1386): tensor(-9.1386),\n",
       " tensor(-9.1312): tensor(-9.1312),\n",
       " tensor(-9.1239): tensor(-9.1239),\n",
       " tensor(-9.1166): tensor(-9.1166),\n",
       " tensor(-9.1092): tensor(-9.1092),\n",
       " tensor(-9.1019): tensor(-9.1019),\n",
       " tensor(-9.0946): tensor(-9.0946),\n",
       " tensor(-9.0872): tensor(-9.0872),\n",
       " tensor(-9.0799): tensor(-9.0799),\n",
       " tensor(-9.0726): tensor(-9.0726),\n",
       " tensor(-9.0652): tensor(-9.0652),\n",
       " tensor(-9.0579): tensor(-9.0579),\n",
       " tensor(-9.0506): tensor(-9.0506),\n",
       " tensor(-9.0433): tensor(-9.0433),\n",
       " tensor(-9.0359): tensor(-9.0359),\n",
       " tensor(-9.0286): tensor(-9.0286),\n",
       " tensor(-9.0213): tensor(-9.0213),\n",
       " tensor(-9.0139): tensor(-9.0139),\n",
       " tensor(-9.0066): tensor(-9.0066),\n",
       " tensor(-8.9993): tensor(-8.9993),\n",
       " tensor(-8.9919): tensor(-8.9919),\n",
       " tensor(-8.9846): tensor(-8.9846),\n",
       " tensor(-8.9773): tensor(-8.9773),\n",
       " tensor(-8.9699): tensor(-8.9699),\n",
       " tensor(-8.9626): tensor(-8.9626),\n",
       " tensor(-8.9553): tensor(-8.9553),\n",
       " tensor(-8.9479): tensor(-8.9479),\n",
       " tensor(-8.9406): tensor(-8.9406),\n",
       " tensor(-8.9333): tensor(-8.9333),\n",
       " tensor(-8.9260): tensor(-8.9260),\n",
       " tensor(-8.9186): tensor(-8.9186),\n",
       " tensor(-8.9113): tensor(-8.9113),\n",
       " tensor(-8.9040): tensor(-8.9040),\n",
       " tensor(-8.8966): tensor(-8.8966),\n",
       " tensor(-8.8893): tensor(-8.8893),\n",
       " tensor(-8.8820): tensor(-8.8820),\n",
       " tensor(-8.8746): tensor(-8.8746),\n",
       " tensor(-8.8673): tensor(-8.8673),\n",
       " tensor(-8.8600): tensor(-8.8600),\n",
       " tensor(-8.8526): tensor(-8.8526),\n",
       " tensor(-8.8453): tensor(-8.8453),\n",
       " tensor(-8.8380): tensor(-8.8380),\n",
       " tensor(-8.8306): tensor(-8.8306),\n",
       " tensor(-8.8233): tensor(-8.8233),\n",
       " tensor(-8.8160): tensor(-8.8160),\n",
       " tensor(-8.8087): tensor(-8.8087),\n",
       " tensor(-8.8013): tensor(-8.8013),\n",
       " tensor(-8.7940): tensor(-8.7940),\n",
       " tensor(-8.7867): tensor(-8.7867),\n",
       " tensor(-8.7793): tensor(-8.7793),\n",
       " tensor(-8.7720): tensor(-8.7720),\n",
       " tensor(-8.7647): tensor(-8.7647),\n",
       " tensor(-8.7573): tensor(-8.7573),\n",
       " tensor(-8.7500): tensor(-8.7500),\n",
       " tensor(-8.7427): tensor(-8.7427),\n",
       " tensor(-8.7353): tensor(-8.7353),\n",
       " tensor(-8.7280): tensor(-8.7280),\n",
       " tensor(-8.7207): tensor(-8.7207),\n",
       " tensor(-8.7133): tensor(-8.7133),\n",
       " tensor(-8.7060): tensor(-8.7060),\n",
       " tensor(-8.6987): tensor(-8.6987),\n",
       " tensor(-8.6913): tensor(-8.6913),\n",
       " tensor(-8.6840): tensor(-8.6840),\n",
       " tensor(-8.6767): tensor(-8.6767),\n",
       " tensor(-8.6694): tensor(-8.6694),\n",
       " tensor(-8.6620): tensor(-8.6620),\n",
       " tensor(-8.6547): tensor(-8.6547),\n",
       " tensor(-8.6474): tensor(-8.6474),\n",
       " tensor(-8.6400): tensor(-8.6400),\n",
       " tensor(-8.6327): tensor(-8.6327),\n",
       " tensor(-8.6254): tensor(-8.6254),\n",
       " tensor(-8.6180): tensor(-8.6180),\n",
       " tensor(-8.6107): tensor(-8.6107),\n",
       " tensor(-8.6034): tensor(-8.6034),\n",
       " tensor(-8.5960): tensor(-8.5960),\n",
       " tensor(-8.5887): tensor(-8.5887),\n",
       " tensor(-8.5814): tensor(-8.5814),\n",
       " tensor(-8.5740): tensor(-8.5740),\n",
       " tensor(-8.5667): tensor(-8.5667),\n",
       " tensor(-8.5594): tensor(-8.5594),\n",
       " tensor(-8.5521): tensor(-8.5521),\n",
       " tensor(-8.5447): tensor(-8.5447),\n",
       " tensor(-8.5374): tensor(-8.5374),\n",
       " tensor(-8.5301): tensor(-8.5301),\n",
       " tensor(-8.5227): tensor(-8.5227),\n",
       " tensor(-8.5154): tensor(-8.5154),\n",
       " tensor(-8.5081): tensor(-8.5081),\n",
       " tensor(-8.5007): tensor(-8.5007),\n",
       " tensor(-8.4934): tensor(-8.4934),\n",
       " tensor(-8.4861): tensor(-8.4861),\n",
       " tensor(-8.4787): tensor(-8.4787),\n",
       " tensor(-8.4714): tensor(-8.4714),\n",
       " tensor(-8.4641): tensor(-8.4641),\n",
       " tensor(-8.4567): tensor(-8.4567),\n",
       " tensor(-8.4494): tensor(-8.4494),\n",
       " tensor(-8.4421): tensor(-8.4421),\n",
       " tensor(-8.4348): tensor(-8.4348),\n",
       " tensor(-8.4274): tensor(-8.4274),\n",
       " tensor(-8.4201): tensor(-8.4201),\n",
       " tensor(-8.4128): tensor(-8.4128),\n",
       " tensor(-8.4054): tensor(-8.4054),\n",
       " tensor(-8.3981): tensor(-8.3981),\n",
       " tensor(-8.3908): tensor(-8.3908),\n",
       " tensor(-8.3834): tensor(-8.3834),\n",
       " tensor(-8.3761): tensor(-8.3761),\n",
       " tensor(-8.3688): tensor(-8.3688),\n",
       " tensor(-8.3614): tensor(-8.3614),\n",
       " tensor(-8.3541): tensor(-8.3541),\n",
       " tensor(-8.3468): tensor(-8.3468),\n",
       " tensor(-8.3394): tensor(-8.3394),\n",
       " tensor(-8.3321): tensor(-8.3321),\n",
       " tensor(-8.3248): tensor(-8.3248),\n",
       " tensor(-8.3174): tensor(-8.3174),\n",
       " tensor(-8.3101): tensor(-8.3101),\n",
       " tensor(-8.3028): tensor(-8.3028),\n",
       " tensor(-8.2955): tensor(-8.2955),\n",
       " tensor(-8.2881): tensor(-8.2881),\n",
       " tensor(-8.2808): tensor(-8.2808),\n",
       " tensor(-8.2735): tensor(-8.2735),\n",
       " tensor(-8.2661): tensor(-8.2661),\n",
       " tensor(-8.2588): tensor(-8.2588),\n",
       " tensor(-8.2515): tensor(-8.2515),\n",
       " tensor(-8.2441): tensor(-8.2441),\n",
       " tensor(-8.2368): tensor(-8.2368),\n",
       " tensor(-8.2295): tensor(-8.2295),\n",
       " tensor(-8.2221): tensor(-8.2221),\n",
       " tensor(-8.2148): tensor(-8.2148),\n",
       " tensor(-8.2075): tensor(-8.2075),\n",
       " tensor(-8.2001): tensor(-8.2001),\n",
       " tensor(-8.1928): tensor(-8.1928),\n",
       " tensor(-8.1855): tensor(-8.1855),\n",
       " tensor(-8.1782): tensor(-8.1782),\n",
       " tensor(-8.1708): tensor(-8.1708),\n",
       " tensor(-8.1635): tensor(-8.1635),\n",
       " tensor(-8.1562): tensor(-8.1562),\n",
       " tensor(-8.1488): tensor(-8.1488),\n",
       " tensor(-8.1415): tensor(-8.1415),\n",
       " tensor(-8.1342): tensor(-8.1342),\n",
       " tensor(-8.1268): tensor(-8.1268),\n",
       " tensor(-8.1195): tensor(-8.1195),\n",
       " tensor(-8.1122): tensor(-8.1122),\n",
       " tensor(-8.1048): tensor(-8.1048),\n",
       " tensor(-8.0975): tensor(-8.0975),\n",
       " tensor(-8.0902): tensor(-8.0902),\n",
       " tensor(-8.0828): tensor(-8.0828),\n",
       " tensor(-8.0755): tensor(-8.0755),\n",
       " tensor(-8.0682): tensor(-8.0682),\n",
       " tensor(-8.0609): tensor(-8.0609),\n",
       " tensor(-8.0535): tensor(-8.0535),\n",
       " tensor(-8.0462): tensor(-8.0462),\n",
       " tensor(-8.0389): tensor(-8.0389),\n",
       " tensor(-8.0315): tensor(-8.0315),\n",
       " tensor(-8.0242): tensor(-8.0242),\n",
       " tensor(-8.0169): tensor(-8.0169),\n",
       " tensor(-8.0095): tensor(-8.0095),\n",
       " tensor(-8.0022): tensor(-8.0022),\n",
       " tensor(-7.9949): tensor(-7.9949),\n",
       " tensor(-7.9875): tensor(-7.9875),\n",
       " tensor(-7.9802): tensor(-7.9802),\n",
       " tensor(-7.9729): tensor(-7.9729),\n",
       " tensor(-7.9655): tensor(-7.9655),\n",
       " tensor(-7.9582): tensor(-7.9582),\n",
       " tensor(-7.9509): tensor(-7.9509),\n",
       " tensor(-7.9435): tensor(-7.9435),\n",
       " tensor(-7.9362): tensor(-7.9362),\n",
       " tensor(-7.9289): tensor(-7.9289),\n",
       " tensor(-7.9216): tensor(-7.9216),\n",
       " tensor(-7.9142): tensor(-7.9142),\n",
       " tensor(-7.9069): tensor(-7.9069),\n",
       " tensor(-7.8996): tensor(-7.8996),\n",
       " tensor(-7.8922): tensor(-7.8922),\n",
       " tensor(-7.8849): tensor(-7.8849),\n",
       " tensor(-7.8776): tensor(-7.8776),\n",
       " tensor(-7.8702): tensor(-7.8702),\n",
       " tensor(-7.8629): tensor(-7.8629),\n",
       " tensor(-7.8556): tensor(-7.8556),\n",
       " tensor(-7.8482): tensor(-7.8482),\n",
       " tensor(-7.8409): tensor(-7.8409),\n",
       " tensor(-7.8336): tensor(-7.8336),\n",
       " tensor(-7.8262): tensor(-7.8262),\n",
       " tensor(-7.8189): tensor(-7.8189),\n",
       " tensor(-7.8116): tensor(-7.8116),\n",
       " tensor(-7.8043): tensor(-7.8043),\n",
       " tensor(-7.7969): tensor(-7.7969),\n",
       " tensor(-7.7896): tensor(-7.7896),\n",
       " tensor(-7.7823): tensor(-7.7823),\n",
       " tensor(-7.7749): tensor(-7.7749),\n",
       " tensor(-7.7676): tensor(-7.7676),\n",
       " tensor(-7.7603): tensor(-7.7603),\n",
       " tensor(-7.7529): tensor(-7.7529),\n",
       " tensor(-7.7456): tensor(-7.7456),\n",
       " tensor(-7.7383): tensor(-7.7383),\n",
       " tensor(-7.7309): tensor(-7.7309),\n",
       " tensor(-7.7236): tensor(-7.7236),\n",
       " tensor(-7.7163): tensor(-7.7163),\n",
       " tensor(-7.7089): tensor(-7.7089),\n",
       " tensor(-7.7016): tensor(-7.7016),\n",
       " tensor(-7.6943): tensor(-7.6943),\n",
       " tensor(-7.6869): tensor(-7.6869),\n",
       " tensor(-7.6796): tensor(-7.6796),\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i.item(): i for i in chronos_tokenizer.boundaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-15.0000, -14.9927, -14.9853,  ...,  14.9853,  14.9927,  15.0000])\n",
      "4093\n",
      "tensor([-1.0000e+20, -1.4996e+01, -1.4989e+01,  ...,  1.4989e+01,\n",
      "         1.4996e+01,  1.0000e+20])\n",
      "4094\n"
     ]
    }
   ],
   "source": [
    "pipeline = ChronosPipeline.from_pretrained('amazon/chronos-t5-small')\n",
    "print(pipeline.tokenizer.centers)\n",
    "print(len(pipeline.tokenizer.centers))\n",
    "print(pipeline.tokenizer.boundaries)\n",
    "print(len(pipeline.tokenizer.boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.444445,\n",
       " array([1.5923077 , 0.76153845, 1.1076922 , 1.3846154 , 0.83076924,\n",
       "        0.9692308 , 1.5230769 , 1.3846154 , 1.176923  , 0.9       ,\n",
       "        0.6230769 , 0.83076924, 0.83076924, 0.76153845, 0.76153845,\n",
       "        1.3846154 , 0.41538462, 0.76153845], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry = {\n",
    "    'past_target': np.array([23., 11., 16., 20., 12., 14., 22., 20., 17., 13.,  9., 12., 12., 11., 11., 20.,  6., 11.], dtype=np.float32), \n",
    "    'future_target': np.array([10.,  5., 15., 13., 14., 11., 11., 12., 15., 17., 14., 15.], dtype=np.float32)\n",
    "}\n",
    "past_target = entry['past_target']\n",
    "future_target = entry['future_target']\n",
    "num_special_toks = 2\n",
    "n_tokens = 4096\n",
    "\n",
    "scale = np.mean(past_target)\n",
    "scaled_past_target = past_target / scale\n",
    "scale, scaled_past_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262 tensor(1.5799) 0.012395620346069336\n",
      "2263 tensor(1.5872) 0.005064249038696289\n",
      "2264 tensor(1.5946) -0.002267122268676758\n",
      "2265 tensor(1.6019) -0.009598493576049805\n",
      "2266 tensor(1.6092) -0.01692986488342285\n",
      "2267 tensor(1.6166) -0.0242612361907959\n",
      "2268 tensor(1.6239) -0.031592607498168945\n"
     ]
    }
   ],
   "source": [
    "for i in [2262, 2263, 2264, 2265, 2266, 2267, 2268]:\n",
    "    val = chronos_tokenizer.chronos_tokenizer.boundaries[i].item()\n",
    "    print(i, chronos_tokenizer.chronos_tokenizer.boundaries[i], scaled_past_target[0] - val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-15.0: 2,\n",
       " -14.992668151855469: 3,\n",
       " -14.985337257385254: 4,\n",
       " -14.978005409240723: 5,\n",
       " -14.970674514770508: 6,\n",
       " -14.963342666625977: 7,\n",
       " -14.956011772155762: 8,\n",
       " -14.94867992401123: 9,\n",
       " -14.941349029541016: 10,\n",
       " -14.934017181396484: 11,\n",
       " -14.92668628692627: 12,\n",
       " -14.919354438781738: 13,\n",
       " -14.912023544311523: 14,\n",
       " -14.904691696166992: 15,\n",
       " -14.897360801696777: 16,\n",
       " -14.890028953552246: 17,\n",
       " -14.882698059082031: 18,\n",
       " -14.8753662109375: 19,\n",
       " -14.868035316467285: 20,\n",
       " -14.860703468322754: 21,\n",
       " -14.853372573852539: 22,\n",
       " -14.846040725708008: 23,\n",
       " -14.838709831237793: 24,\n",
       " -14.831377983093262: 25,\n",
       " -14.824047088623047: 26,\n",
       " -14.816715240478516: 27,\n",
       " -14.8093843460083: 28,\n",
       " -14.80205249786377: 29,\n",
       " -14.794721603393555: 30,\n",
       " -14.787389755249023: 31,\n",
       " -14.780058860778809: 32,\n",
       " -14.772727012634277: 33,\n",
       " -14.765396118164062: 34,\n",
       " -14.758064270019531: 35,\n",
       " -14.750733375549316: 36,\n",
       " -14.743401527404785: 37,\n",
       " -14.73607063293457: 38,\n",
       " -14.728738784790039: 39,\n",
       " -14.721407890319824: 40,\n",
       " -14.714076042175293: 41,\n",
       " -14.706745147705078: 42,\n",
       " -14.699413299560547: 43,\n",
       " -14.692082405090332: 44,\n",
       " -14.6847505569458: 45,\n",
       " -14.677419662475586: 46,\n",
       " -14.670087814331055: 47,\n",
       " -14.66275691986084: 48,\n",
       " -14.655425071716309: 49,\n",
       " -14.648094177246094: 50,\n",
       " -14.640762329101562: 51,\n",
       " -14.633431434631348: 52,\n",
       " -14.626099586486816: 53,\n",
       " -14.618768692016602: 54,\n",
       " -14.61143684387207: 55,\n",
       " -14.604105949401855: 56,\n",
       " -14.596774101257324: 57,\n",
       " -14.58944320678711: 58,\n",
       " -14.582111358642578: 59,\n",
       " -14.574780464172363: 60,\n",
       " -14.567448616027832: 61,\n",
       " -14.560117721557617: 62,\n",
       " -14.552785873413086: 63,\n",
       " -14.545454978942871: 64,\n",
       " -14.53812313079834: 65,\n",
       " -14.530792236328125: 66,\n",
       " -14.523460388183594: 67,\n",
       " -14.516129493713379: 68,\n",
       " -14.508797645568848: 69,\n",
       " -14.501466751098633: 70,\n",
       " -14.494134902954102: 71,\n",
       " -14.48680305480957: 72,\n",
       " -14.479472160339355: 73,\n",
       " -14.472140312194824: 74,\n",
       " -14.46480941772461: 75,\n",
       " -14.457477569580078: 76,\n",
       " -14.450146675109863: 77,\n",
       " -14.442814826965332: 78,\n",
       " -14.435483932495117: 79,\n",
       " -14.428152084350586: 80,\n",
       " -14.420821189880371: 81,\n",
       " -14.41348934173584: 82,\n",
       " -14.406158447265625: 83,\n",
       " -14.398826599121094: 84,\n",
       " -14.391495704650879: 85,\n",
       " -14.384163856506348: 86,\n",
       " -14.376832962036133: 87,\n",
       " -14.369501113891602: 88,\n",
       " -14.362170219421387: 89,\n",
       " -14.354838371276855: 90,\n",
       " -14.34750747680664: 91,\n",
       " -14.34017562866211: 92,\n",
       " -14.332844734191895: 93,\n",
       " -14.325512886047363: 94,\n",
       " -14.318181991577148: 95,\n",
       " -14.310850143432617: 96,\n",
       " -14.303519248962402: 97,\n",
       " -14.296187400817871: 98,\n",
       " -14.288856506347656: 99,\n",
       " -14.281524658203125: 100,\n",
       " -14.27419376373291: 101,\n",
       " -14.266861915588379: 102,\n",
       " -14.259531021118164: 103,\n",
       " -14.252199172973633: 104,\n",
       " -14.244868278503418: 105,\n",
       " -14.237536430358887: 106,\n",
       " -14.230205535888672: 107,\n",
       " -14.22287368774414: 108,\n",
       " -14.215542793273926: 109,\n",
       " -14.208210945129395: 110,\n",
       " -14.20088005065918: 111,\n",
       " -14.193548202514648: 112,\n",
       " -14.186217308044434: 113,\n",
       " -14.178885459899902: 114,\n",
       " -14.171554565429688: 115,\n",
       " -14.164222717285156: 116,\n",
       " -14.156891822814941: 117,\n",
       " -14.14955997467041: 118,\n",
       " -14.142229080200195: 119,\n",
       " -14.134897232055664: 120,\n",
       " -14.12756633758545: 121,\n",
       " -14.120234489440918: 122,\n",
       " -14.112903594970703: 123,\n",
       " -14.105571746826172: 124,\n",
       " -14.098240852355957: 125,\n",
       " -14.090909004211426: 126,\n",
       " -14.083578109741211: 127,\n",
       " -14.07624626159668: 128,\n",
       " -14.068915367126465: 129,\n",
       " -14.061583518981934: 130,\n",
       " -14.054252624511719: 131,\n",
       " -14.046920776367188: 132,\n",
       " -14.039589881896973: 133,\n",
       " -14.032258033752441: 134,\n",
       " -14.024927139282227: 135,\n",
       " -14.017595291137695: 136,\n",
       " -14.01026439666748: 137,\n",
       " -14.00293254852295: 138,\n",
       " -13.995600700378418: 139,\n",
       " -13.988269805908203: 140,\n",
       " -13.980937957763672: 141,\n",
       " -13.973607063293457: 142,\n",
       " -13.966275215148926: 143,\n",
       " -13.958944320678711: 144,\n",
       " -13.95161247253418: 145,\n",
       " -13.944281578063965: 146,\n",
       " -13.936949729919434: 147,\n",
       " -13.929618835449219: 148,\n",
       " -13.922286987304688: 149,\n",
       " -13.914956092834473: 150,\n",
       " -13.907624244689941: 151,\n",
       " -13.900293350219727: 152,\n",
       " -13.892961502075195: 153,\n",
       " -13.88563060760498: 154,\n",
       " -13.87829875946045: 155,\n",
       " -13.870967864990234: 156,\n",
       " -13.863636016845703: 157,\n",
       " -13.856305122375488: 158,\n",
       " -13.848973274230957: 159,\n",
       " -13.841642379760742: 160,\n",
       " -13.834310531616211: 161,\n",
       " -13.826979637145996: 162,\n",
       " -13.819647789001465: 163,\n",
       " -13.81231689453125: 164,\n",
       " -13.804985046386719: 165,\n",
       " -13.797654151916504: 166,\n",
       " -13.790322303771973: 167,\n",
       " -13.782991409301758: 168,\n",
       " -13.775659561157227: 169,\n",
       " -13.768328666687012: 170,\n",
       " -13.76099681854248: 171,\n",
       " -13.753665924072266: 172,\n",
       " -13.746334075927734: 173,\n",
       " -13.73900318145752: 174,\n",
       " -13.731671333312988: 175,\n",
       " -13.724340438842773: 176,\n",
       " -13.717008590698242: 177,\n",
       " -13.709677696228027: 178,\n",
       " -13.702345848083496: 179,\n",
       " -13.695014953613281: 180,\n",
       " -13.68768310546875: 181,\n",
       " -13.680352210998535: 182,\n",
       " -13.673020362854004: 183,\n",
       " -13.665689468383789: 184,\n",
       " -13.658357620239258: 185,\n",
       " -13.651026725769043: 186,\n",
       " -13.643694877624512: 187,\n",
       " -13.636363983154297: 188,\n",
       " -13.629032135009766: 189,\n",
       " -13.62170124053955: 190,\n",
       " -13.61436939239502: 191,\n",
       " -13.607038497924805: 192,\n",
       " -13.599706649780273: 193,\n",
       " -13.592375755310059: 194,\n",
       " -13.585043907165527: 195,\n",
       " -13.577713012695312: 196,\n",
       " -13.570381164550781: 197,\n",
       " -13.563050270080566: 198,\n",
       " -13.555718421936035: 199,\n",
       " -13.54838752746582: 200,\n",
       " -13.541055679321289: 201,\n",
       " -13.533724784851074: 202,\n",
       " -13.526392936706543: 203,\n",
       " -13.519062042236328: 204,\n",
       " -13.511730194091797: 205,\n",
       " -13.504399299621582: 206,\n",
       " -13.49706745147705: 207,\n",
       " -13.48973560333252: 208,\n",
       " -13.482404708862305: 209,\n",
       " -13.475072860717773: 210,\n",
       " -13.467741966247559: 211,\n",
       " -13.460410118103027: 212,\n",
       " -13.453079223632812: 213,\n",
       " -13.445747375488281: 214,\n",
       " -13.438416481018066: 215,\n",
       " -13.431084632873535: 216,\n",
       " -13.42375373840332: 217,\n",
       " -13.416421890258789: 218,\n",
       " -13.409090995788574: 219,\n",
       " -13.401759147644043: 220,\n",
       " -13.394428253173828: 221,\n",
       " -13.387096405029297: 222,\n",
       " -13.379765510559082: 223,\n",
       " -13.37243366241455: 224,\n",
       " -13.365102767944336: 225,\n",
       " -13.357770919799805: 226,\n",
       " -13.35044002532959: 227,\n",
       " -13.343108177185059: 228,\n",
       " -13.335777282714844: 229,\n",
       " -13.328445434570312: 230,\n",
       " -13.321114540100098: 231,\n",
       " -13.313782691955566: 232,\n",
       " -13.306451797485352: 233,\n",
       " -13.29911994934082: 234,\n",
       " -13.291789054870605: 235,\n",
       " -13.284457206726074: 236,\n",
       " -13.27712631225586: 237,\n",
       " -13.269794464111328: 238,\n",
       " -13.262463569641113: 239,\n",
       " -13.255131721496582: 240,\n",
       " -13.247800827026367: 241,\n",
       " -13.240468978881836: 242,\n",
       " -13.233138084411621: 243,\n",
       " -13.22580623626709: 244,\n",
       " -13.218475341796875: 245,\n",
       " -13.211143493652344: 246,\n",
       " -13.203812599182129: 247,\n",
       " -13.196480751037598: 248,\n",
       " -13.189149856567383: 249,\n",
       " -13.181818008422852: 250,\n",
       " -13.174487113952637: 251,\n",
       " -13.167155265808105: 252,\n",
       " -13.15982437133789: 253,\n",
       " -13.15249252319336: 254,\n",
       " -13.145161628723145: 255,\n",
       " -13.137829780578613: 256,\n",
       " -13.130498886108398: 257,\n",
       " -13.123167037963867: 258,\n",
       " -13.115836143493652: 259,\n",
       " -13.108504295349121: 260,\n",
       " -13.101173400878906: 261,\n",
       " -13.093841552734375: 262,\n",
       " -13.08651065826416: 263,\n",
       " -13.079178810119629: 264,\n",
       " -13.071847915649414: 265,\n",
       " -13.064516067504883: 266,\n",
       " -13.057185173034668: 267,\n",
       " -13.049853324890137: 268,\n",
       " -13.042522430419922: 269,\n",
       " -13.03519058227539: 270,\n",
       " -13.027859687805176: 271,\n",
       " -13.020527839660645: 272,\n",
       " -13.01319694519043: 273,\n",
       " -13.005865097045898: 274,\n",
       " -12.998534202575684: 275,\n",
       " -12.991202354431152: 276,\n",
       " -12.983870506286621: 277,\n",
       " -12.976539611816406: 278,\n",
       " -12.969207763671875: 279,\n",
       " -12.96187686920166: 280,\n",
       " -12.954545021057129: 281,\n",
       " -12.947214126586914: 282,\n",
       " -12.939882278442383: 283,\n",
       " -12.932551383972168: 284,\n",
       " -12.925219535827637: 285,\n",
       " -12.917888641357422: 286,\n",
       " -12.91055679321289: 287,\n",
       " -12.903225898742676: 288,\n",
       " -12.895894050598145: 289,\n",
       " -12.88856315612793: 290,\n",
       " -12.881231307983398: 291,\n",
       " -12.873900413513184: 292,\n",
       " -12.866568565368652: 293,\n",
       " -12.859237670898438: 294,\n",
       " -12.851905822753906: 295,\n",
       " -12.844574928283691: 296,\n",
       " -12.83724308013916: 297,\n",
       " -12.829912185668945: 298,\n",
       " -12.822580337524414: 299,\n",
       " -12.8152494430542: 300,\n",
       " -12.807917594909668: 301,\n",
       " -12.800586700439453: 302,\n",
       " -12.793254852294922: 303,\n",
       " -12.785923957824707: 304,\n",
       " -12.778592109680176: 305,\n",
       " -12.771261215209961: 306,\n",
       " -12.76392936706543: 307,\n",
       " -12.756598472595215: 308,\n",
       " -12.749266624450684: 309,\n",
       " -12.741935729980469: 310,\n",
       " -12.734603881835938: 311,\n",
       " -12.727272987365723: 312,\n",
       " -12.719941139221191: 313,\n",
       " -12.712610244750977: 314,\n",
       " -12.705278396606445: 315,\n",
       " -12.69794750213623: 316,\n",
       " -12.6906156539917: 317,\n",
       " -12.683284759521484: 318,\n",
       " -12.675952911376953: 319,\n",
       " -12.668622016906738: 320,\n",
       " -12.661290168762207: 321,\n",
       " -12.653959274291992: 322,\n",
       " -12.646627426147461: 323,\n",
       " -12.639296531677246: 324,\n",
       " -12.631964683532715: 325,\n",
       " -12.6246337890625: 326,\n",
       " -12.617301940917969: 327,\n",
       " -12.609971046447754: 328,\n",
       " -12.602639198303223: 329,\n",
       " -12.595308303833008: 330,\n",
       " -12.587976455688477: 331,\n",
       " -12.580645561218262: 332,\n",
       " -12.57331371307373: 333,\n",
       " -12.565982818603516: 334,\n",
       " -12.558650970458984: 335,\n",
       " -12.55132007598877: 336,\n",
       " -12.543988227844238: 337,\n",
       " -12.536657333374023: 338,\n",
       " -12.529325485229492: 339,\n",
       " -12.521994590759277: 340,\n",
       " -12.514662742614746: 341,\n",
       " -12.507331848144531: 342,\n",
       " -12.5: 343,\n",
       " -12.492668151855469: 344,\n",
       " -12.485337257385254: 345,\n",
       " -12.478005409240723: 346,\n",
       " -12.470674514770508: 347,\n",
       " -12.463342666625977: 348,\n",
       " -12.456011772155762: 349,\n",
       " -12.44867992401123: 350,\n",
       " -12.441349029541016: 351,\n",
       " -12.434017181396484: 352,\n",
       " -12.42668628692627: 353,\n",
       " -12.419354438781738: 354,\n",
       " -12.412023544311523: 355,\n",
       " -12.404691696166992: 356,\n",
       " -12.397360801696777: 357,\n",
       " -12.390028953552246: 358,\n",
       " -12.382698059082031: 359,\n",
       " -12.3753662109375: 360,\n",
       " -12.368035316467285: 361,\n",
       " -12.360703468322754: 362,\n",
       " -12.353372573852539: 363,\n",
       " -12.346040725708008: 364,\n",
       " -12.338709831237793: 365,\n",
       " -12.331377983093262: 366,\n",
       " -12.324047088623047: 367,\n",
       " -12.316715240478516: 368,\n",
       " -12.3093843460083: 369,\n",
       " -12.30205249786377: 370,\n",
       " -12.294721603393555: 371,\n",
       " -12.287389755249023: 372,\n",
       " -12.280058860778809: 373,\n",
       " -12.272727012634277: 374,\n",
       " -12.265396118164062: 375,\n",
       " -12.258064270019531: 376,\n",
       " -12.250733375549316: 377,\n",
       " -12.243401527404785: 378,\n",
       " -12.23607063293457: 379,\n",
       " -12.228738784790039: 380,\n",
       " -12.221407890319824: 381,\n",
       " -12.214076042175293: 382,\n",
       " -12.206745147705078: 383,\n",
       " -12.199413299560547: 384,\n",
       " -12.192082405090332: 385,\n",
       " -12.1847505569458: 386,\n",
       " -12.177419662475586: 387,\n",
       " -12.170087814331055: 388,\n",
       " -12.16275691986084: 389,\n",
       " -12.155425071716309: 390,\n",
       " -12.148094177246094: 391,\n",
       " -12.140762329101562: 392,\n",
       " -12.133431434631348: 393,\n",
       " -12.126099586486816: 394,\n",
       " -12.118768692016602: 395,\n",
       " -12.11143684387207: 396,\n",
       " -12.104105949401855: 397,\n",
       " -12.096774101257324: 398,\n",
       " -12.08944320678711: 399,\n",
       " -12.082111358642578: 400,\n",
       " -12.074780464172363: 401,\n",
       " -12.067448616027832: 402,\n",
       " -12.060117721557617: 403,\n",
       " -12.052785873413086: 404,\n",
       " -12.045454978942871: 405,\n",
       " -12.03812313079834: 406,\n",
       " -12.030792236328125: 407,\n",
       " -12.023460388183594: 408,\n",
       " -12.016129493713379: 409,\n",
       " -12.008797645568848: 410,\n",
       " -12.001466751098633: 411,\n",
       " -11.994134902954102: 412,\n",
       " -11.98680305480957: 413,\n",
       " -11.979472160339355: 414,\n",
       " -11.972140312194824: 415,\n",
       " -11.96480941772461: 416,\n",
       " -11.957477569580078: 417,\n",
       " -11.950146675109863: 418,\n",
       " -11.942814826965332: 419,\n",
       " -11.935483932495117: 420,\n",
       " -11.928152084350586: 421,\n",
       " -11.920821189880371: 422,\n",
       " -11.91348934173584: 423,\n",
       " -11.906158447265625: 424,\n",
       " -11.898826599121094: 425,\n",
       " -11.891495704650879: 426,\n",
       " -11.884163856506348: 427,\n",
       " -11.876832962036133: 428,\n",
       " -11.869501113891602: 429,\n",
       " -11.862170219421387: 430,\n",
       " -11.854838371276855: 431,\n",
       " -11.84750747680664: 432,\n",
       " -11.84017562866211: 433,\n",
       " -11.832844734191895: 434,\n",
       " -11.825512886047363: 435,\n",
       " -11.818181991577148: 436,\n",
       " -11.810850143432617: 437,\n",
       " -11.803519248962402: 438,\n",
       " -11.796187400817871: 439,\n",
       " -11.788856506347656: 440,\n",
       " -11.781524658203125: 441,\n",
       " -11.77419376373291: 442,\n",
       " -11.766861915588379: 443,\n",
       " -11.759531021118164: 444,\n",
       " -11.752199172973633: 445,\n",
       " -11.744868278503418: 446,\n",
       " -11.737536430358887: 447,\n",
       " -11.730205535888672: 448,\n",
       " -11.72287368774414: 449,\n",
       " -11.715542793273926: 450,\n",
       " -11.708210945129395: 451,\n",
       " -11.70088005065918: 452,\n",
       " -11.693548202514648: 453,\n",
       " -11.686217308044434: 454,\n",
       " -11.678885459899902: 455,\n",
       " -11.671554565429688: 456,\n",
       " -11.664222717285156: 457,\n",
       " -11.656891822814941: 458,\n",
       " -11.64955997467041: 459,\n",
       " -11.642229080200195: 460,\n",
       " -11.634897232055664: 461,\n",
       " -11.62756633758545: 462,\n",
       " -11.620234489440918: 463,\n",
       " -11.612903594970703: 464,\n",
       " -11.605571746826172: 465,\n",
       " -11.598240852355957: 466,\n",
       " -11.590909004211426: 467,\n",
       " -11.583578109741211: 468,\n",
       " -11.57624626159668: 469,\n",
       " -11.568915367126465: 470,\n",
       " -11.561583518981934: 471,\n",
       " -11.554252624511719: 472,\n",
       " -11.546920776367188: 473,\n",
       " -11.539589881896973: 474,\n",
       " -11.532258033752441: 475,\n",
       " -11.524927139282227: 476,\n",
       " -11.517595291137695: 477,\n",
       " -11.51026439666748: 478,\n",
       " -11.50293254852295: 479,\n",
       " -11.495600700378418: 480,\n",
       " -11.488269805908203: 481,\n",
       " -11.480937957763672: 482,\n",
       " -11.473607063293457: 483,\n",
       " -11.466275215148926: 484,\n",
       " -11.458944320678711: 485,\n",
       " -11.45161247253418: 486,\n",
       " -11.444281578063965: 487,\n",
       " -11.436949729919434: 488,\n",
       " -11.429618835449219: 489,\n",
       " -11.422286987304688: 490,\n",
       " -11.414956092834473: 491,\n",
       " -11.407624244689941: 492,\n",
       " -11.400293350219727: 493,\n",
       " -11.392961502075195: 494,\n",
       " -11.38563060760498: 495,\n",
       " -11.37829875946045: 496,\n",
       " -11.370967864990234: 497,\n",
       " -11.363636016845703: 498,\n",
       " -11.356305122375488: 499,\n",
       " -11.348973274230957: 500,\n",
       " -11.341642379760742: 501,\n",
       " -11.334310531616211: 502,\n",
       " -11.326979637145996: 503,\n",
       " -11.319647789001465: 504,\n",
       " -11.31231689453125: 505,\n",
       " -11.304985046386719: 506,\n",
       " -11.297654151916504: 507,\n",
       " -11.290322303771973: 508,\n",
       " -11.282991409301758: 509,\n",
       " -11.275659561157227: 510,\n",
       " -11.268328666687012: 511,\n",
       " -11.26099681854248: 512,\n",
       " -11.253665924072266: 513,\n",
       " -11.246334075927734: 514,\n",
       " -11.23900318145752: 515,\n",
       " -11.231671333312988: 516,\n",
       " -11.224340438842773: 517,\n",
       " -11.217008590698242: 518,\n",
       " -11.209677696228027: 519,\n",
       " -11.202345848083496: 520,\n",
       " -11.195014953613281: 521,\n",
       " -11.18768310546875: 522,\n",
       " -11.180352210998535: 523,\n",
       " -11.173020362854004: 524,\n",
       " -11.165689468383789: 525,\n",
       " -11.158357620239258: 526,\n",
       " -11.151026725769043: 527,\n",
       " -11.143694877624512: 528,\n",
       " -11.136363983154297: 529,\n",
       " -11.129032135009766: 530,\n",
       " -11.12170124053955: 531,\n",
       " -11.11436939239502: 532,\n",
       " -11.107038497924805: 533,\n",
       " -11.099706649780273: 534,\n",
       " -11.092375755310059: 535,\n",
       " -11.085043907165527: 536,\n",
       " -11.077713012695312: 537,\n",
       " -11.070381164550781: 538,\n",
       " -11.063050270080566: 539,\n",
       " -11.055718421936035: 540,\n",
       " -11.04838752746582: 541,\n",
       " -11.041055679321289: 542,\n",
       " -11.033724784851074: 543,\n",
       " -11.026392936706543: 544,\n",
       " -11.019062042236328: 545,\n",
       " -11.011730194091797: 546,\n",
       " -11.004399299621582: 547,\n",
       " -10.99706745147705: 548,\n",
       " -10.98973560333252: 549,\n",
       " -10.982404708862305: 550,\n",
       " -10.975072860717773: 551,\n",
       " -10.967741966247559: 552,\n",
       " -10.960410118103027: 553,\n",
       " -10.953079223632812: 554,\n",
       " -10.945747375488281: 555,\n",
       " -10.938416481018066: 556,\n",
       " -10.931084632873535: 557,\n",
       " -10.92375373840332: 558,\n",
       " -10.916421890258789: 559,\n",
       " -10.909090995788574: 560,\n",
       " -10.901759147644043: 561,\n",
       " -10.894428253173828: 562,\n",
       " -10.887096405029297: 563,\n",
       " -10.879765510559082: 564,\n",
       " -10.87243366241455: 565,\n",
       " -10.865102767944336: 566,\n",
       " -10.857770919799805: 567,\n",
       " -10.85044002532959: 568,\n",
       " -10.843108177185059: 569,\n",
       " -10.835777282714844: 570,\n",
       " -10.828445434570312: 571,\n",
       " -10.821114540100098: 572,\n",
       " -10.813782691955566: 573,\n",
       " -10.806451797485352: 574,\n",
       " -10.79911994934082: 575,\n",
       " -10.791789054870605: 576,\n",
       " -10.784457206726074: 577,\n",
       " -10.77712631225586: 578,\n",
       " -10.769794464111328: 579,\n",
       " -10.762463569641113: 580,\n",
       " -10.755131721496582: 581,\n",
       " -10.747800827026367: 582,\n",
       " -10.740468978881836: 583,\n",
       " -10.733138084411621: 584,\n",
       " -10.72580623626709: 585,\n",
       " -10.718475341796875: 586,\n",
       " -10.711143493652344: 587,\n",
       " -10.703812599182129: 588,\n",
       " -10.696480751037598: 589,\n",
       " -10.689149856567383: 590,\n",
       " -10.681818008422852: 591,\n",
       " -10.674487113952637: 592,\n",
       " -10.667155265808105: 593,\n",
       " -10.65982437133789: 594,\n",
       " -10.65249252319336: 595,\n",
       " -10.645161628723145: 596,\n",
       " -10.637829780578613: 597,\n",
       " -10.630498886108398: 598,\n",
       " -10.623167037963867: 599,\n",
       " -10.615836143493652: 600,\n",
       " -10.608504295349121: 601,\n",
       " -10.601173400878906: 602,\n",
       " -10.593841552734375: 603,\n",
       " -10.58651065826416: 604,\n",
       " -10.579178810119629: 605,\n",
       " -10.571847915649414: 606,\n",
       " -10.564516067504883: 607,\n",
       " -10.557185173034668: 608,\n",
       " -10.549853324890137: 609,\n",
       " -10.542522430419922: 610,\n",
       " -10.53519058227539: 611,\n",
       " -10.527859687805176: 612,\n",
       " -10.520527839660645: 613,\n",
       " -10.51319694519043: 614,\n",
       " -10.505865097045898: 615,\n",
       " -10.498534202575684: 616,\n",
       " -10.491202354431152: 617,\n",
       " -10.483870506286621: 618,\n",
       " -10.476539611816406: 619,\n",
       " -10.469207763671875: 620,\n",
       " -10.46187686920166: 621,\n",
       " -10.454545021057129: 622,\n",
       " -10.447214126586914: 623,\n",
       " -10.439882278442383: 624,\n",
       " -10.432551383972168: 625,\n",
       " -10.425219535827637: 626,\n",
       " -10.417888641357422: 627,\n",
       " -10.41055679321289: 628,\n",
       " -10.403225898742676: 629,\n",
       " -10.395894050598145: 630,\n",
       " -10.38856315612793: 631,\n",
       " -10.381231307983398: 632,\n",
       " -10.373900413513184: 633,\n",
       " -10.366568565368652: 634,\n",
       " -10.359237670898438: 635,\n",
       " -10.351905822753906: 636,\n",
       " -10.344574928283691: 637,\n",
       " -10.33724308013916: 638,\n",
       " -10.329912185668945: 639,\n",
       " -10.322580337524414: 640,\n",
       " -10.3152494430542: 641,\n",
       " -10.307917594909668: 642,\n",
       " -10.300586700439453: 643,\n",
       " -10.293254852294922: 644,\n",
       " -10.285923957824707: 645,\n",
       " -10.278592109680176: 646,\n",
       " -10.271261215209961: 647,\n",
       " -10.26392936706543: 648,\n",
       " -10.256598472595215: 649,\n",
       " -10.249266624450684: 650,\n",
       " -10.241935729980469: 651,\n",
       " -10.234603881835938: 652,\n",
       " -10.227272987365723: 653,\n",
       " -10.219941139221191: 654,\n",
       " -10.212610244750977: 655,\n",
       " -10.205278396606445: 656,\n",
       " -10.19794750213623: 657,\n",
       " -10.1906156539917: 658,\n",
       " -10.183284759521484: 659,\n",
       " -10.175952911376953: 660,\n",
       " -10.168622016906738: 661,\n",
       " -10.161290168762207: 662,\n",
       " -10.153959274291992: 663,\n",
       " -10.146627426147461: 664,\n",
       " -10.139296531677246: 665,\n",
       " -10.131964683532715: 666,\n",
       " -10.1246337890625: 667,\n",
       " -10.117301940917969: 668,\n",
       " -10.109971046447754: 669,\n",
       " -10.102639198303223: 670,\n",
       " -10.095308303833008: 671,\n",
       " -10.087976455688477: 672,\n",
       " -10.080645561218262: 673,\n",
       " -10.07331371307373: 674,\n",
       " -10.065982818603516: 675,\n",
       " -10.058650970458984: 676,\n",
       " -10.05132007598877: 677,\n",
       " -10.043988227844238: 678,\n",
       " -10.036657333374023: 679,\n",
       " -10.029325485229492: 680,\n",
       " -10.021994590759277: 681,\n",
       " -10.014662742614746: 682,\n",
       " -10.007331848144531: 683,\n",
       " -10.0: 684,\n",
       " -9.992668151855469: 685,\n",
       " -9.985337257385254: 686,\n",
       " -9.978005409240723: 687,\n",
       " -9.970674514770508: 688,\n",
       " -9.963342666625977: 689,\n",
       " -9.956011772155762: 690,\n",
       " -9.94867992401123: 691,\n",
       " -9.941349029541016: 692,\n",
       " -9.934017181396484: 693,\n",
       " -9.92668628692627: 694,\n",
       " -9.919354438781738: 695,\n",
       " -9.912023544311523: 696,\n",
       " -9.904691696166992: 697,\n",
       " -9.897360801696777: 698,\n",
       " -9.890028953552246: 699,\n",
       " -9.882698059082031: 700,\n",
       " -9.8753662109375: 701,\n",
       " -9.868035316467285: 702,\n",
       " -9.860703468322754: 703,\n",
       " -9.853372573852539: 704,\n",
       " -9.846040725708008: 705,\n",
       " -9.838709831237793: 706,\n",
       " -9.831377983093262: 707,\n",
       " -9.824047088623047: 708,\n",
       " -9.816715240478516: 709,\n",
       " -9.8093843460083: 710,\n",
       " -9.80205249786377: 711,\n",
       " -9.794721603393555: 712,\n",
       " -9.787389755249023: 713,\n",
       " -9.780058860778809: 714,\n",
       " -9.772727012634277: 715,\n",
       " -9.765396118164062: 716,\n",
       " -9.758064270019531: 717,\n",
       " -9.750733375549316: 718,\n",
       " -9.743401527404785: 719,\n",
       " -9.73607063293457: 720,\n",
       " -9.728738784790039: 721,\n",
       " -9.721407890319824: 722,\n",
       " -9.714076042175293: 723,\n",
       " -9.706745147705078: 724,\n",
       " -9.699413299560547: 725,\n",
       " -9.692082405090332: 726,\n",
       " -9.6847505569458: 727,\n",
       " -9.677419662475586: 728,\n",
       " -9.670087814331055: 729,\n",
       " -9.66275691986084: 730,\n",
       " -9.655425071716309: 731,\n",
       " -9.648094177246094: 732,\n",
       " -9.640762329101562: 733,\n",
       " -9.633431434631348: 734,\n",
       " -9.626099586486816: 735,\n",
       " -9.618768692016602: 736,\n",
       " -9.61143684387207: 737,\n",
       " -9.604105949401855: 738,\n",
       " -9.596774101257324: 739,\n",
       " -9.58944320678711: 740,\n",
       " -9.582111358642578: 741,\n",
       " -9.574780464172363: 742,\n",
       " -9.567448616027832: 743,\n",
       " -9.560117721557617: 744,\n",
       " -9.552785873413086: 745,\n",
       " -9.545454978942871: 746,\n",
       " -9.53812313079834: 747,\n",
       " -9.530792236328125: 748,\n",
       " -9.523460388183594: 749,\n",
       " -9.516129493713379: 750,\n",
       " -9.508797645568848: 751,\n",
       " -9.501466751098633: 752,\n",
       " -9.494134902954102: 753,\n",
       " -9.48680305480957: 754,\n",
       " -9.479472160339355: 755,\n",
       " -9.472140312194824: 756,\n",
       " -9.46480941772461: 757,\n",
       " -9.457477569580078: 758,\n",
       " -9.450146675109863: 759,\n",
       " -9.442814826965332: 760,\n",
       " -9.435483932495117: 761,\n",
       " -9.428152084350586: 762,\n",
       " -9.420821189880371: 763,\n",
       " -9.41348934173584: 764,\n",
       " -9.406158447265625: 765,\n",
       " -9.398826599121094: 766,\n",
       " -9.391495704650879: 767,\n",
       " -9.384163856506348: 768,\n",
       " -9.376832962036133: 769,\n",
       " -9.369501113891602: 770,\n",
       " -9.362170219421387: 771,\n",
       " -9.354838371276855: 772,\n",
       " -9.34750747680664: 773,\n",
       " -9.34017562866211: 774,\n",
       " -9.332844734191895: 775,\n",
       " -9.325512886047363: 776,\n",
       " -9.318181991577148: 777,\n",
       " -9.310850143432617: 778,\n",
       " -9.303519248962402: 779,\n",
       " -9.296187400817871: 780,\n",
       " -9.288856506347656: 781,\n",
       " -9.281524658203125: 782,\n",
       " -9.27419376373291: 783,\n",
       " -9.266861915588379: 784,\n",
       " -9.259531021118164: 785,\n",
       " -9.252199172973633: 786,\n",
       " -9.244868278503418: 787,\n",
       " -9.237536430358887: 788,\n",
       " -9.230205535888672: 789,\n",
       " -9.22287368774414: 790,\n",
       " -9.215542793273926: 791,\n",
       " -9.208210945129395: 792,\n",
       " -9.20088005065918: 793,\n",
       " -9.193548202514648: 794,\n",
       " -9.186217308044434: 795,\n",
       " -9.178885459899902: 796,\n",
       " -9.171554565429688: 797,\n",
       " -9.164222717285156: 798,\n",
       " -9.156891822814941: 799,\n",
       " -9.14955997467041: 800,\n",
       " -9.142229080200195: 801,\n",
       " -9.134897232055664: 802,\n",
       " -9.12756633758545: 803,\n",
       " -9.120234489440918: 804,\n",
       " -9.112903594970703: 805,\n",
       " -9.105571746826172: 806,\n",
       " -9.098240852355957: 807,\n",
       " -9.090909004211426: 808,\n",
       " -9.083578109741211: 809,\n",
       " -9.07624626159668: 810,\n",
       " -9.068915367126465: 811,\n",
       " -9.061583518981934: 812,\n",
       " -9.054252624511719: 813,\n",
       " -9.046920776367188: 814,\n",
       " -9.039589881896973: 815,\n",
       " -9.032258033752441: 816,\n",
       " -9.024927139282227: 817,\n",
       " -9.017595291137695: 818,\n",
       " -9.01026439666748: 819,\n",
       " -9.00293254852295: 820,\n",
       " -8.995601654052734: 821,\n",
       " -8.988269805908203: 822,\n",
       " -8.980937957763672: 823,\n",
       " -8.973607063293457: 824,\n",
       " -8.966275215148926: 825,\n",
       " -8.958944320678711: 826,\n",
       " -8.95161247253418: 827,\n",
       " -8.944281578063965: 828,\n",
       " -8.936949729919434: 829,\n",
       " -8.929618835449219: 830,\n",
       " -8.922286987304688: 831,\n",
       " -8.914956092834473: 832,\n",
       " -8.907624244689941: 833,\n",
       " -8.900293350219727: 834,\n",
       " -8.892961502075195: 835,\n",
       " -8.88563060760498: 836,\n",
       " -8.87829875946045: 837,\n",
       " -8.870967864990234: 838,\n",
       " -8.863636016845703: 839,\n",
       " -8.856305122375488: 840,\n",
       " -8.848973274230957: 841,\n",
       " -8.841642379760742: 842,\n",
       " -8.834310531616211: 843,\n",
       " -8.826979637145996: 844,\n",
       " -8.819647789001465: 845,\n",
       " -8.81231689453125: 846,\n",
       " -8.804985046386719: 847,\n",
       " -8.797654151916504: 848,\n",
       " -8.790322303771973: 849,\n",
       " -8.782991409301758: 850,\n",
       " -8.775659561157227: 851,\n",
       " -8.768328666687012: 852,\n",
       " -8.76099681854248: 853,\n",
       " -8.753665924072266: 854,\n",
       " -8.746334075927734: 855,\n",
       " -8.73900318145752: 856,\n",
       " -8.731671333312988: 857,\n",
       " -8.724340438842773: 858,\n",
       " -8.717008590698242: 859,\n",
       " -8.709677696228027: 860,\n",
       " -8.702345848083496: 861,\n",
       " -8.695014953613281: 862,\n",
       " -8.68768310546875: 863,\n",
       " -8.680352210998535: 864,\n",
       " -8.673020362854004: 865,\n",
       " -8.665689468383789: 866,\n",
       " -8.658357620239258: 867,\n",
       " -8.651026725769043: 868,\n",
       " -8.643694877624512: 869,\n",
       " -8.636363983154297: 870,\n",
       " -8.629032135009766: 871,\n",
       " -8.62170124053955: 872,\n",
       " -8.61436939239502: 873,\n",
       " -8.607038497924805: 874,\n",
       " -8.599706649780273: 875,\n",
       " -8.592375755310059: 876,\n",
       " -8.585043907165527: 877,\n",
       " -8.577713012695312: 878,\n",
       " -8.570381164550781: 879,\n",
       " -8.563050270080566: 880,\n",
       " -8.555718421936035: 881,\n",
       " -8.54838752746582: 882,\n",
       " -8.541055679321289: 883,\n",
       " -8.533724784851074: 884,\n",
       " -8.526392936706543: 885,\n",
       " -8.519062042236328: 886,\n",
       " -8.511730194091797: 887,\n",
       " -8.504399299621582: 888,\n",
       " -8.49706745147705: 889,\n",
       " -8.48973560333252: 890,\n",
       " -8.482404708862305: 891,\n",
       " -8.475072860717773: 892,\n",
       " -8.467741966247559: 893,\n",
       " -8.460410118103027: 894,\n",
       " -8.453079223632812: 895,\n",
       " -8.445747375488281: 896,\n",
       " -8.438416481018066: 897,\n",
       " -8.431084632873535: 898,\n",
       " -8.42375373840332: 899,\n",
       " -8.416421890258789: 900,\n",
       " -8.409090995788574: 901,\n",
       " -8.401759147644043: 902,\n",
       " -8.394428253173828: 903,\n",
       " -8.387096405029297: 904,\n",
       " -8.379765510559082: 905,\n",
       " -8.37243366241455: 906,\n",
       " -8.365102767944336: 907,\n",
       " -8.357770919799805: 908,\n",
       " -8.35044002532959: 909,\n",
       " -8.343108177185059: 910,\n",
       " -8.335777282714844: 911,\n",
       " -8.328445434570312: 912,\n",
       " -8.321114540100098: 913,\n",
       " -8.313782691955566: 914,\n",
       " -8.306451797485352: 915,\n",
       " -8.29911994934082: 916,\n",
       " -8.291789054870605: 917,\n",
       " -8.284457206726074: 918,\n",
       " -8.27712631225586: 919,\n",
       " -8.269794464111328: 920,\n",
       " -8.262463569641113: 921,\n",
       " -8.255131721496582: 922,\n",
       " -8.247800827026367: 923,\n",
       " -8.240468978881836: 924,\n",
       " -8.233138084411621: 925,\n",
       " -8.22580623626709: 926,\n",
       " -8.218475341796875: 927,\n",
       " -8.211143493652344: 928,\n",
       " -8.203812599182129: 929,\n",
       " -8.196480751037598: 930,\n",
       " -8.189149856567383: 931,\n",
       " -8.181818008422852: 932,\n",
       " -8.174487113952637: 933,\n",
       " -8.167155265808105: 934,\n",
       " -8.15982437133789: 935,\n",
       " -8.15249252319336: 936,\n",
       " -8.145161628723145: 937,\n",
       " -8.137829780578613: 938,\n",
       " -8.130498886108398: 939,\n",
       " -8.123167037963867: 940,\n",
       " -8.115836143493652: 941,\n",
       " -8.108504295349121: 942,\n",
       " -8.101173400878906: 943,\n",
       " -8.093841552734375: 944,\n",
       " -8.08651065826416: 945,\n",
       " -8.079178810119629: 946,\n",
       " -8.071847915649414: 947,\n",
       " -8.064516067504883: 948,\n",
       " -8.057185173034668: 949,\n",
       " -8.049853324890137: 950,\n",
       " -8.042522430419922: 951,\n",
       " -8.03519058227539: 952,\n",
       " -8.027859687805176: 953,\n",
       " -8.020527839660645: 954,\n",
       " -8.01319694519043: 955,\n",
       " -8.005865097045898: 956,\n",
       " -7.998533725738525: 957,\n",
       " -7.991202354431152: 958,\n",
       " -7.983870983123779: 959,\n",
       " -7.976539611816406: 960,\n",
       " -7.969208240509033: 961,\n",
       " -7.96187686920166: 962,\n",
       " -7.954545497894287: 963,\n",
       " -7.947214126586914: 964,\n",
       " -7.939882755279541: 965,\n",
       " -7.932551383972168: 966,\n",
       " -7.925220012664795: 967,\n",
       " -7.917888641357422: 968,\n",
       " -7.910557270050049: 969,\n",
       " -7.903225898742676: 970,\n",
       " -7.895894527435303: 971,\n",
       " -7.88856315612793: 972,\n",
       " -7.881231784820557: 973,\n",
       " -7.873900413513184: 974,\n",
       " -7.8665690422058105: 975,\n",
       " -7.8592376708984375: 976,\n",
       " -7.8519062995910645: 977,\n",
       " -7.844574928283691: 978,\n",
       " -7.837243556976318: 979,\n",
       " -7.829912185668945: 980,\n",
       " -7.822580814361572: 981,\n",
       " -7.815249443054199: 982,\n",
       " -7.807918071746826: 983,\n",
       " -7.800586700439453: 984,\n",
       " -7.79325532913208: 985,\n",
       " -7.785923957824707: 986,\n",
       " -7.778592586517334: 987,\n",
       " -7.771261215209961: 988,\n",
       " -7.763929843902588: 989,\n",
       " -7.756598472595215: 990,\n",
       " -7.749267101287842: 991,\n",
       " -7.7419352531433105: 992,\n",
       " -7.7346038818359375: 993,\n",
       " -7.7272725105285645: 994,\n",
       " -7.719941139221191: 995,\n",
       " -7.712609767913818: 996,\n",
       " -7.705278396606445: 997,\n",
       " -7.697947025299072: 998,\n",
       " -7.690615653991699: 999,\n",
       " -7.683284282684326: 1000,\n",
       " -7.675952911376953: 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i: j for i, j in zip(chronos_tokenizer.chronos_tokenizer.centers.tolist(), list(range(num_special_toks, n_tokens)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262 tensor(1.5836)\n",
      "2263 tensor(1.5909)\n",
      "2264 tensor(1.5982)\n",
      "2265 tensor(1.6056)\n",
      "2266 tensor(1.6129)\n",
      "2267 tensor(1.6202)\n",
      "2268 tensor(1.6276)\n"
     ]
    }
   ],
   "source": [
    "for i in [2262, 2263, 2264, 2265, 2266, 2267, 2268]:\n",
    "    val = chronos_tokenizer.chronos_tokenizer.centers[i].item()\n",
    "    print(i, chronos_tokenizer.chronos_tokenizer.centers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([2266, 2153, 2200, 2238, 2162, 2181, 2257, 2238, 2210, 2172, 2134, 2162,\n",
       "         2162, 2153, 2153, 2238, 2106, 2153,    1]),\n",
       " 'attention_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True]),\n",
       " 'labels': tensor([2143, 2096, 2191, 2172, 2181, 2153, 2153, 2162, 2191, 2210, 2181, 2191,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100,    1])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chronos_tokenizer = ChronosTokenizerWrapper('amazon/chronos-t5-small')\n",
    "type(chronos_tokenizer)\n",
    "chronos_tokenizer._to_hf_format(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should output this:\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([2266, 2153, 2200, 2238, 2162, 2181, 2257, 2238, 2210, 2172, 2134, 2162, 2162, 2153, 2153, 2238, 2106, 2153,    1]),\n",
    " 'attention_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]),\n",
    " 'labels': tensor([2143, 2096, 2191, 2172, 2181, 2153, 2153, 2162, 2191, 2210, 2181, 2191, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    1])}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.Tensor([[10.,  5., 15., 13., 14., 11., 11., 12., 15., 17., 14., 15.]])\n",
    "scale = torch.Tensor([14.4444])\n",
    "\n",
    "centers = torch.linspace(\n",
    "    -15.0,\n",
    "    15.0,\n",
    "    4093,\n",
    ")\n",
    "boundaries = torch.concat(\n",
    "    (\n",
    "        torch.tensor([-1e20], device=centers.device),\n",
    "        (centers[1:] + centers[:-1]) / 2,\n",
    "        torch.tensor([1e20], device=centers.device),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = label\n",
    "attention_mask = ~torch.isnan(context)\n",
    "\n",
    "if scale is None:\n",
    "    scale = torch.nansum(\n",
    "        torch.abs(context) * attention_mask, dim=-1\n",
    "    ) / torch.nansum(attention_mask, dim=-1)\n",
    "    scale[~(scale > 0)] = 1.0\n",
    "\n",
    "scaled_context = context / scale.unsqueeze(dim=-1)\n",
    "token_ids = (\n",
    "    torch.bucketize(\n",
    "        input=scaled_context,\n",
    "        boundaries=boundaries,\n",
    "        right=True,\n",
    "    )\n",
    "    + 2\n",
    ")\n",
    "token_ids[~attention_mask] = self.config.pad_token_id\n",
    "\n",
    "# token_ids, attention_mask, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6923, 0.3462, 1.0385, 0.9000, 0.9692, 0.7615, 0.7615, 0.8308, 1.0385,\n",
       "         1.1769, 0.9692, 1.0385]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chronos.scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchronos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscripts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChronosDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chronos.scripts'"
     ]
    }
   ],
   "source": [
    "from chronos.scripts import ChronosDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>196.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>54.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>230.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>410.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9  \\\n",
       "0     16.0   13.0   15.0   16.0   21.0   12.0   14.0   15.0   21.0   20.0   \n",
       "1     14.0    7.0    8.0   12.0   18.0   19.0   16.0   15.0   16.0   15.0   \n",
       "2    204.0  202.0  206.0  207.0  210.0  228.0  194.0  166.0  198.0  225.0   \n",
       "3    124.0  128.0  131.0  127.0  115.0  131.0  116.0  124.0  116.0   97.0   \n",
       "4     17.0   14.0   10.0   19.0   14.0   10.0   12.0   19.0   11.0   18.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "762   30.0   39.0   33.0   22.0   27.0   33.0   31.0   26.0   47.0   42.0   \n",
       "763   42.0   42.0   51.0   54.0   57.0   62.0   51.0   59.0   61.0   42.0   \n",
       "764   54.0   53.0   60.0   51.0   55.0   54.0   57.0   61.0   60.0   51.0   \n",
       "765  230.0  289.0  231.0  288.0  286.0  291.0  231.0  252.0  286.0  368.0   \n",
       "766   47.0   50.0   44.0   49.0   36.0   43.0   39.0   69.0   45.0   53.0   \n",
       "\n",
       "     ...     20     21     22     23     24     25     26     27     28     29  \n",
       "0    ...   18.0   14.0    6.0   15.0   21.0   17.0   14.0   12.0    8.0   17.0  \n",
       "1    ...   13.0    9.0   17.0   22.0   11.0   19.0   11.0   12.0   20.0   10.0  \n",
       "2    ...  196.0  192.0  210.0  198.0  193.0  190.0  186.0  181.0  198.0  169.0  \n",
       "3    ...  105.0   67.0  102.0   90.0   93.0  103.0  103.0   82.0   76.0   81.0  \n",
       "4    ...   10.0   17.0   18.0   13.0   23.0   14.0   22.0   26.0   24.0   19.0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "762  ...   35.0   31.0   41.0   34.0   34.0   30.0   21.0   33.0   37.0   32.0  \n",
       "763  ...   42.0   54.0   39.0   38.0   45.0   39.0   45.0   55.0   46.0   31.0  \n",
       "764  ...   39.0   42.0   37.0   36.0   40.0   34.0   39.0   42.0   44.0   30.0  \n",
       "765  ...  410.0  361.0  374.0  388.0  401.0  434.0  415.0  546.0  453.0  456.0  \n",
       "766  ...   35.0   46.0   39.0   55.0   52.0   48.0   35.0   50.0   57.0   46.0  \n",
       "\n",
       "[767 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/mnt/workdisk/kushal/llm-foundry/kushal-testing/hospital.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: Union[torch.Tensor, List[torch.Tensor], pd.DataFrame, np.ndarray], \n",
    "                 context_len: int, horizon_len: int, transform: Optional[bool]=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame, np.ndarray, torch.Tensor, or List[torch.Tensor]): The input data containing features.\n",
    "            targets (pd.DataFrame, np.ndarray, torch.Tensor, or List[torch.Tensor]): The target values.\n",
    "            seq_length (int): The length of each sequence.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.context_len = context_len\n",
    "        self.horizon_len = horizon_len\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sequences in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.data.iloc[idx, :self.context_len]\n",
    "        y = self.data.iloc[idx, self.context_len:]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x.to_numpy().astype(np.float16), y.to_numpy().astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in `context` and `horizon` MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>horizon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[16.0, 13.0, 15.0, 16.0, 21.0, 12.0, 14.0, 15....</td>\n",
       "      <td>[13.0, 19.0, 18.0, 14.0, 6.0, 15.0, 21.0, 17.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14.0, 7.0, 8.0, 12.0, 18.0, 19.0, 16.0, 15.0,...</td>\n",
       "      <td>[13.0, 15.0, 13.0, 9.0, 17.0, 22.0, 11.0, 19.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[204.0, 202.0, 206.0, 207.0, 210.0, 228.0, 194...</td>\n",
       "      <td>[205.0, 180.0, 196.0, 192.0, 210.0, 198.0, 193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[124.0, 128.0, 131.0, 127.0, 115.0, 131.0, 116...</td>\n",
       "      <td>[142.0, 115.0, 105.0, 67.0, 102.0, 90.0, 93.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[17.0, 14.0, 10.0, 19.0, 14.0, 10.0, 12.0, 19....</td>\n",
       "      <td>[15.0, 6.0, 10.0, 17.0, 18.0, 13.0, 23.0, 14.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>[30.0, 39.0, 33.0, 22.0, 27.0, 33.0, 31.0, 26....</td>\n",
       "      <td>[34.0, 33.0, 35.0, 31.0, 41.0, 34.0, 34.0, 30....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>[42.0, 42.0, 51.0, 54.0, 57.0, 62.0, 51.0, 59....</td>\n",
       "      <td>[47.0, 40.0, 42.0, 54.0, 39.0, 38.0, 45.0, 39....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>[54.0, 53.0, 60.0, 51.0, 55.0, 54.0, 57.0, 61....</td>\n",
       "      <td>[41.0, 41.0, 39.0, 42.0, 37.0, 36.0, 40.0, 34....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>[230.0, 289.0, 231.0, 288.0, 286.0, 291.0, 231...</td>\n",
       "      <td>[347.0, 373.0, 410.0, 361.0, 374.0, 388.0, 401...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>[47.0, 50.0, 44.0, 49.0, 36.0, 43.0, 39.0, 69....</td>\n",
       "      <td>[35.0, 36.0, 35.0, 46.0, 39.0, 55.0, 52.0, 48....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context  \\\n",
       "0    [16.0, 13.0, 15.0, 16.0, 21.0, 12.0, 14.0, 15....   \n",
       "1    [14.0, 7.0, 8.0, 12.0, 18.0, 19.0, 16.0, 15.0,...   \n",
       "2    [204.0, 202.0, 206.0, 207.0, 210.0, 228.0, 194...   \n",
       "3    [124.0, 128.0, 131.0, 127.0, 115.0, 131.0, 116...   \n",
       "4    [17.0, 14.0, 10.0, 19.0, 14.0, 10.0, 12.0, 19....   \n",
       "..                                                 ...   \n",
       "762  [30.0, 39.0, 33.0, 22.0, 27.0, 33.0, 31.0, 26....   \n",
       "763  [42.0, 42.0, 51.0, 54.0, 57.0, 62.0, 51.0, 59....   \n",
       "764  [54.0, 53.0, 60.0, 51.0, 55.0, 54.0, 57.0, 61....   \n",
       "765  [230.0, 289.0, 231.0, 288.0, 286.0, 291.0, 231...   \n",
       "766  [47.0, 50.0, 44.0, 49.0, 36.0, 43.0, 39.0, 69....   \n",
       "\n",
       "                                               horizon  \n",
       "0    [13.0, 19.0, 18.0, 14.0, 6.0, 15.0, 21.0, 17.0...  \n",
       "1    [13.0, 15.0, 13.0, 9.0, 17.0, 22.0, 11.0, 19.0...  \n",
       "2    [205.0, 180.0, 196.0, 192.0, 210.0, 198.0, 193...  \n",
       "3    [142.0, 115.0, 105.0, 67.0, 102.0, 90.0, 93.0,...  \n",
       "4    [15.0, 6.0, 10.0, 17.0, 18.0, 13.0, 23.0, 14.0...  \n",
       "..                                                 ...  \n",
       "762  [34.0, 33.0, 35.0, 31.0, 41.0, 34.0, 34.0, 30....  \n",
       "763  [47.0, 40.0, 42.0, 54.0, 39.0, 38.0, 45.0, 39....  \n",
       "764  [41.0, 41.0, 39.0, 42.0, 37.0, 36.0, 40.0, 34....  \n",
       "765  [347.0, 373.0, 410.0, 361.0, 374.0, 388.0, 401...  \n",
       "766  [35.0, 36.0, 35.0, 46.0, 39.0, 55.0, 52.0, 48....  \n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_row(row, context_len=18):\n",
    "    c = row[:context_len].to_numpy().astype(np.float64)\n",
    "    h = row[context_len:].to_numpy().astype(np.float64)\n",
    "    return c, h\n",
    "\n",
    "new_data = df.apply(split_row, axis=1)\n",
    "new_df = pd.DataFrame(new_data.tolist(), columns=['context', 'horizon'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to MDS\n",
    "output_dir = '/mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_context_horizon_split/'  # shards written to a local directory\n",
    "columns = {\n",
    "    'context': 'ndarray:float16:18',\n",
    "    'horizon': 'ndarray:float16:12',\n",
    "}\n",
    "\n",
    "%rm -rf /mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_context_horizon_split/\n",
    "from streaming.base import MDSWriter, StreamingDataset\n",
    "dataset = TimeSeriesDataset(data=df, context_len=18, horizon_len=12)\n",
    "with MDSWriter(out=output_dir, columns=columns) as out:\n",
    "    for x, y in dataset:\n",
    "        out.write({'context': x.astype(np.float16), 'horizon': y.astype(np.float16)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n"
     ]
    },
    {
     "ename": "DistStoreError",
     "evalue": "Timed out after 601 seconds waiting for clients. 1/2 clients joined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDistStoreError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Inspect dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mStreamingDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/streaming/base/dataset.py:431\u001b[0m, in \u001b[0;36mStreamingDataset.__init__\u001b[0;34m(self, streams, remote, local, split, download_retry, download_timeout, validate_hash, keep_zip, epoch_size, predownload, cache_limit, sampling_method, sampling_granularity, partition_algo, num_canonical_nodes, batch_size, shuffle, shuffle_algo, shuffle_seed, shuffle_block_size, batching_method, allow_unsafe_types, replication)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch size cannot be negative. Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_size_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Initialize torch dist ourselves, if necessary.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m destroy_dist \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_init_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Initialize the Stream defaults and normalize to a list of Streams.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams:\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/streaming/base/distributed.py:128\u001b[0m, in \u001b[0;36mmaybe_init_dist\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgloo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 128\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:75\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     77\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:89\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     88\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m---> 89\u001b[0m     func_return \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m     92\u001b[0m     msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1305\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m   1303\u001b[0m         not_none(init_method), rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1304\u001b[0m     )\n\u001b[0;32m-> 1305\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/torch/distributed/rendezvous.py:246\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m master_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_get_env_or_raise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    244\u001b[0m use_libuv \u001b[38;5;241m=\u001b[39m query_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_libuv\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE_LIBUV\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 246\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c10d_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_libuv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m (store, rank, world_size)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# If this configuration is invalidated, there is nothing we can do about it\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/workdisk/kushal/llm-foundry/llmfoundry-venv/lib/python3.11/site-packages/torch/distributed/rendezvous.py:174\u001b[0m, in \u001b[0;36m_create_c10d_store\u001b[0;34m(hostname, port, rank, world_size, timeout, use_libuv)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     start_daemon \u001b[38;5;241m=\u001b[39m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTCPStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhostname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_daemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_tenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_libuv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_libuv\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mDistStoreError\u001b[0m: Timed out after 601 seconds waiting for clients. 1/2 clients joined."
     ]
    }
   ],
   "source": [
    "# Inspect dataset\n",
    "dataset = StreamingDataset(local=output_dir, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dataset.num_samples):\n",
    "    print(i)\n",
    "    if i % 50 == 0:\n",
    "        print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep a single column in MDS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestStreamingTimeSeriesDataset(StreamingDataset):\n",
    "    def __init__(\n",
    "        self, tokenizer: PreTrainedTokenizerBase, seq_len: int,\n",
    "        streams: Optional[Sequence[Stream]] = None,remote: Optional[str] = None,local: Optional[str] = None,split: Optional[str] = None,download_retry: int = 2,download_timeout: float = 60,validate_hash: Optional[str] = None,\n",
    "        keep_zip: bool = False,epoch_size: Optional[Union[int, str]] = None,predownload: Optional[int] = None,cache_limit: Optional[Union[int, str]] = None,partition_algo: str = 'relaxed',num_canonical_nodes: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,shuffle: bool = False,shuffle_algo: str = 'py1e',shuffle_seed: int = 9176,shuffle_block_size: Optional[int] = None,sampling_method: str = 'balanced',sampling_granularity: int = 1,\n",
    "        batching_method: str = 'random',allow_unsafe_types: bool = False,replication: Optional[int] = None,transform: Optional[Callable] = None,**kwargs: Any,\n",
    "    ):\n",
    "        \n",
    "        # Build Dataset\n",
    "        super().__init__(\n",
    "            streams=streams,remote=remote,local=local,split=split,download_retry=download_retry,download_timeout=download_timeout,validate_hash=validate_hash,\n",
    "            keep_zip=keep_zip,epoch_size=epoch_size,predownload=predownload,cache_limit=cache_limit,partition_algo=partition_algo,num_canonical_nodes=num_canonical_nodes,\n",
    "            batch_size=batch_size,shuffle=shuffle,shuffle_algo=shuffle_algo,shuffle_seed=shuffle_seed,shuffle_block_size=shuffle_block_size,sampling_method=sampling_method,\n",
    "            sampling_granularity=sampling_granularity,batching_method=batching_method,allow_unsafe_types=allow_unsafe_types,replication=replication,\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.transform = transform        \n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        sample = super().__getitem__(idx)\n",
    "        sample_data = sample['data']\n",
    "        return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[16.0, 13.0, 15.0, 16.0, 21.0, 12.0, 14.0, 15....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14.0, 7.0, 8.0, 12.0, 18.0, 19.0, 16.0, 15.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[204.0, 202.0, 206.0, 207.0, 210.0, 228.0, 194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[124.0, 128.0, 131.0, 127.0, 115.0, 131.0, 116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[17.0, 14.0, 10.0, 19.0, 14.0, 10.0, 12.0, 19....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>[30.0, 39.0, 33.0, 22.0, 27.0, 33.0, 31.0, 26....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>[42.0, 42.0, 51.0, 54.0, 57.0, 62.0, 51.0, 59....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>[54.0, 53.0, 60.0, 51.0, 55.0, 54.0, 57.0, 61....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>[230.0, 289.0, 231.0, 288.0, 286.0, 291.0, 231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>[47.0, 50.0, 44.0, 49.0, 36.0, 43.0, 39.0, 69....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    [16.0, 13.0, 15.0, 16.0, 21.0, 12.0, 14.0, 15....\n",
       "1    [14.0, 7.0, 8.0, 12.0, 18.0, 19.0, 16.0, 15.0,...\n",
       "2    [204.0, 202.0, 206.0, 207.0, 210.0, 228.0, 194...\n",
       "3    [124.0, 128.0, 131.0, 127.0, 115.0, 131.0, 116...\n",
       "4    [17.0, 14.0, 10.0, 19.0, 14.0, 10.0, 12.0, 19....\n",
       "..                                                 ...\n",
       "762  [30.0, 39.0, 33.0, 22.0, 27.0, 33.0, 31.0, 26....\n",
       "763  [42.0, 42.0, 51.0, 54.0, 57.0, 62.0, 51.0, 59....\n",
       "764  [54.0, 53.0, 60.0, 51.0, 55.0, 54.0, 57.0, 61....\n",
       "765  [230.0, 289.0, 231.0, 288.0, 286.0, 291.0, 231...\n",
       "766  [47.0, 50.0, 44.0, 49.0, 36.0, 43.0, 39.0, 69....\n",
       "\n",
       "[767 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_to_numpy(row: pd.Series):\n",
    "    return row.to_numpy().astype(np.float64)\n",
    "\n",
    "one_col_data = df.apply(row_to_numpy, axis=1)\n",
    "one_col_df = pd.DataFrame(one_col_data, columns=['text'])\n",
    "one_col_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[16.0, 13.0, 15.0, 16.0, 21.0, 12.0, 14.0, 15....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14.0, 7.0, 8.0, 12.0, 18.0, 19.0, 16.0, 15.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[204.0, 202.0, 206.0, 207.0, 210.0, 228.0, 194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[124.0, 128.0, 131.0, 127.0, 115.0, 131.0, 116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[17.0, 14.0, 10.0, 19.0, 14.0, 10.0, 12.0, 19....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[12.0, 14.0, 12.0, 12.0, 8.0, 17.0, 10.0, 11.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[6.0, 9.0, 15.0, 15.0, 13.0, 11.0, 9.0, 14.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[26.0, 40.0, 30.0, 25.0, 30.0, 20.0, 22.0, 26....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[23.0, 11.0, 16.0, 20.0, 12.0, 14.0, 22.0, 20....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[31.0, 32.0, 23.0, 35.0, 29.0, 24.0, 21.0, 21....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[15.0, 22.0, 13.0, 11.0, 18.0, 12.0, 23.0, 9.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[17.0, 18.0, 27.0, 21.0, 10.0, 21.0, 32.0, 22....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[14.0, 15.0, 16.0, 16.0, 13.0, 22.0, 16.0, 23....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[18.0, 16.0, 17.0, 21.0, 30.0, 26.0, 13.0, 13....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[20.0, 19.0, 22.0, 17.0, 20.0, 24.0, 19.0, 16....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[8.0, 8.0, 13.0, 22.0, 15.0, 11.0, 14.0, 16.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[55.0, 60.0, 33.0, 40.0, 42.0, 38.0, 42.0, 35....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[29.0, 24.0, 26.0, 31.0, 32.0, 32.0, 62.0, 36....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[41.0, 28.0, 24.0, 33.0, 34.0, 31.0, 25.0, 36....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[257.0, 255.0, 233.0, 246.0, 234.0, 238.0, 228...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   [16.0, 13.0, 15.0, 16.0, 21.0, 12.0, 14.0, 15....\n",
       "1   [14.0, 7.0, 8.0, 12.0, 18.0, 19.0, 16.0, 15.0,...\n",
       "2   [204.0, 202.0, 206.0, 207.0, 210.0, 228.0, 194...\n",
       "3   [124.0, 128.0, 131.0, 127.0, 115.0, 131.0, 116...\n",
       "4   [17.0, 14.0, 10.0, 19.0, 14.0, 10.0, 12.0, 19....\n",
       "5   [12.0, 14.0, 12.0, 12.0, 8.0, 17.0, 10.0, 11.0...\n",
       "6   [6.0, 9.0, 15.0, 15.0, 13.0, 11.0, 9.0, 14.0, ...\n",
       "7   [26.0, 40.0, 30.0, 25.0, 30.0, 20.0, 22.0, 26....\n",
       "8   [23.0, 11.0, 16.0, 20.0, 12.0, 14.0, 22.0, 20....\n",
       "9   [31.0, 32.0, 23.0, 35.0, 29.0, 24.0, 21.0, 21....\n",
       "10  [15.0, 22.0, 13.0, 11.0, 18.0, 12.0, 23.0, 9.0...\n",
       "11  [17.0, 18.0, 27.0, 21.0, 10.0, 21.0, 32.0, 22....\n",
       "12  [14.0, 15.0, 16.0, 16.0, 13.0, 22.0, 16.0, 23....\n",
       "13  [18.0, 16.0, 17.0, 21.0, 30.0, 26.0, 13.0, 13....\n",
       "14  [20.0, 19.0, 22.0, 17.0, 20.0, 24.0, 19.0, 16....\n",
       "15  [8.0, 8.0, 13.0, 22.0, 15.0, 11.0, 14.0, 16.0,...\n",
       "16  [55.0, 60.0, 33.0, 40.0, 42.0, 38.0, 42.0, 35....\n",
       "17  [29.0, 24.0, 26.0, 31.0, 32.0, 32.0, 62.0, 36....\n",
       "18  [41.0, 28.0, 24.0, 33.0, 34.0, 31.0, 25.0, 36....\n",
       "19  [257.0, 255.0, 233.0, 246.0, 234.0, 238.0, 228..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_col_small_df = one_col_df.iloc[:20]\n",
    "one_col_small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761109/1923358326.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out.write({'text': row[0].astype(np.float64)})\n"
     ]
    }
   ],
   "source": [
    "# Write to MDS\n",
    "%rm -rf /mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_single_col/\n",
    "\n",
    "output_dir = '/mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_single_col/'  # shards written to a local directory\n",
    "columns = {'text': 'ndarray:float64:30'}\n",
    "\n",
    "with MDSWriter(out=output_dir, columns=columns) as out:\n",
    "    for index, row in one_col_df.iterrows():\n",
    "        out.write({'text': row[0].astype(np.float64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761109/3823377218.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  out.write({'text': row[0].astype(np.float64)})\n"
     ]
    }
   ],
   "source": [
    "# Write small to MDS\n",
    "%rm -rf /mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_single_col_small/\n",
    "\n",
    "output_dir = '/mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_single_col_small/'  # shards written to a local directory\n",
    "columns = {'text': 'ndarray:float64:30'}\n",
    "\n",
    "with MDSWriter(out=output_dir, columns=columns) as out:\n",
    "    for index, row in one_col_small_df.iterrows():\n",
    "        out.write({'text': row[0].astype(np.float64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n"
     ]
    }
   ],
   "source": [
    "# Read in entire large dataset - CANNOT DO THIS IN JUPYTER NOTEBOOK\n",
    "ds = StreamingTimeSeriesDataset(tokenizer=ChronosTokenizerWrapper('amazon/chronos-t5-small'), seq_len=30, local='/mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_single_col/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n"
     ]
    }
   ],
   "source": [
    "# Read in small dataset - CANNOT DO THIS IN JUPYTER NOTEBOOK\n",
    "ds_small = StreamingTimeSeriesDataset(tokenizer=ChronosTokenizerWrapper('amazon/chronos-t5-small'), seq_len=30, local='/mnt/workdisk/kushal/llm-foundry/kushal-testing/mds_single_col_small/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StreamingTimeSeriesDataset.__init__() missing 1 required positional argument: 'seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m streaming_ts \u001b[38;5;241m=\u001b[39m \u001b[43mStreamingTimeSeriesDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChronosTokenizerWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamazon/chronos-t5-small\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/workdisk/kushal/llm-foundry/kushal-testing/hospital_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: StreamingTimeSeriesDataset.__init__() missing 1 required positional argument: 'seq_len'"
     ]
    }
   ],
   "source": [
    "streaming_ts = StreamingTimeSeriesDataset(tokenizer=ChronosTokenizerWrapper('amazon/chronos-t5-small'), seq_len=30, local='/mnt/workdisk/kushal/llm-foundry/kushal-testing/hospital_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating torch.Dataset or StreamingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for creating your own torch.Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data: Union[torch.Tensor, List[torch.Tensor], pd.DataFrame], labels: Union[torch.Tensor, List[torch.Tensor], pd.DataFrame], transform: Optional[bool]=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: Union[torch.Tensor, List[torch.Tensor], pd.DataFrame, np.ndarray], \n",
    "                 context_len: int, horizon_len: int, transform: Optional[bool]=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (pd.DataFrame, np.ndarray, torch.Tensor, or List[torch.Tensor]): The input data containing features.\n",
    "            targets (pd.DataFrame, np.ndarray, torch.Tensor, or List[torch.Tensor]): The target values.\n",
    "            seq_length (int): The length of each sequence.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.context_len = context_len\n",
    "        self.horizon_len = horizon_len\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sequences in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Get the sequence of data and the corresponding target\n",
    "        # x = self.data.iloc[[idx]][:self.context_len]  # for pd.dataframes\n",
    "        # y = self.data.iloc[[idx]][self.context_len:]  # for pd.dataframes\n",
    "        # x = self.data[idx]  # for torch.tensors\n",
    "        # y = self.targets[idx]  # for torch.tensors\n",
    "        \n",
    "        if idx >= self.__len__():\n",
    "            return\n",
    "        \n",
    "        x = self.data['context'][idx]\n",
    "        y = self.data['horizon'][idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        # return torch.tensor(x.values, dtype=torch.float32), torch.tensor(y.values, dtype=torch.float32)  # for pd.dataframes\n",
    "        # return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)  # for torch.tensors\n",
    "        return x.astype(np.float64), y.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9   ...    20  \\\n",
       "0  16.0  13.0  15.0  16.0  21.0  12.0  14.0  15.0  21.0  20.0  ...  18.0   \n",
       "\n",
       "     21   22    23    24    25    26    27   28    29  \n",
       "0  14.0  6.0  15.0  21.0  17.0  14.0  12.0  8.0  17.0  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TimeSeriesDataset at 0x7f429ad3f550>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in pd.DataFrame\n",
    "dataset = TimeSeriesDataset(data=train, targets=test, seq_length=20, transform=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TimeSeriesDataset at 0x7f429adf51d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in torch.Tensor\n",
    "dataset = TimeSeriesDataset(data=torch.tensor(train.values), targets=torch.tensor(test.values), seq_length=20, transform=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f429d2ee750>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=8, shuffle=True)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Inputs: tensor([[[5.6500e+02, 5.4400e+02, 6.0400e+02, 5.0900e+02, 5.2300e+02,\n",
      "          5.3800e+02, 4.0200e+02, 3.7000e+02, 3.6400e+02, 3.1100e+02,\n",
      "          3.0400e+02, 3.1900e+02, 4.3900e+02, 4.0600e+02, 3.4300e+02,\n",
      "          3.5500e+02, 3.5200e+02, 2.8600e+02]],\n",
      "\n",
      "        [[5.4003e+02, 4.7680e+02, 5.9097e+02, 4.4837e+02, 5.6257e+02,\n",
      "          5.0162e+02, 5.5028e+02, 4.9375e+02, 4.8995e+02, 5.9162e+02,\n",
      "          6.3917e+02, 4.9929e+02, 5.5674e+02, 4.8648e+02, 5.8371e+02,\n",
      "          4.7555e+02, 5.7836e+02, 5.2526e+02]],\n",
      "\n",
      "        [[5.3340e+02, 5.2860e+02, 5.2812e+02, 5.9384e+02, 5.2046e+02,\n",
      "          4.9946e+02, 5.6732e+02, 5.8370e+02, 5.7809e+02, 5.5311e+02,\n",
      "          5.1725e+02, 5.3925e+02, 5.4666e+02, 5.8754e+02, 4.6565e+02,\n",
      "          5.3589e+02, 4.7663e+02, 5.2292e+02]],\n",
      "\n",
      "        [[5.4259e+02, 4.6580e+02, 4.1824e+02, 4.1735e+02, 4.8898e+02,\n",
      "          4.4101e+02, 4.6335e+02, 4.6479e+02, 4.1452e+02, 4.3501e+02,\n",
      "          4.2747e+02, 5.1433e+02, 5.0989e+02, 4.2930e+02, 4.1680e+02,\n",
      "          4.4657e+02, 4.6759e+02, 4.5715e+02]],\n",
      "\n",
      "        [[1.1688e+03, 1.3283e+03, 1.1381e+03, 1.1267e+03, 1.3427e+03,\n",
      "          1.1597e+03, 1.0831e+03, 1.0555e+03, 1.2886e+03, 8.0983e+02,\n",
      "          1.3655e+03, 1.3666e+03, 1.0664e+03, 1.1998e+03, 8.9945e+02,\n",
      "          9.3178e+02, 1.3200e+03, 1.0420e+03]],\n",
      "\n",
      "        [[8.3348e+02, 8.2912e+02, 9.3478e+02, 8.3617e+02, 8.4179e+02,\n",
      "          8.5206e+02, 8.4232e+02, 8.5078e+02, 8.5692e+02, 8.6445e+02,\n",
      "          9.8581e+02, 8.8045e+02, 9.0404e+02, 8.3791e+02, 7.9295e+02,\n",
      "          9.2705e+02, 8.1794e+02, 9.1991e+02]],\n",
      "\n",
      "        [[1.3949e+03, 1.3692e+03, 1.4028e+03, 1.5664e+03, 1.3969e+03,\n",
      "          1.4250e+03, 1.3228e+03, 1.2668e+03, 1.3057e+03, 1.3491e+03,\n",
      "          1.4688e+03, 1.5488e+03, 1.4366e+03, 1.4787e+03, 1.4294e+03,\n",
      "          1.5989e+03, 1.4674e+03, 1.4501e+03]],\n",
      "\n",
      "        [[1.3898e+09, 1.3918e+09, 1.4828e+09, 1.4789e+09, 1.4683e+09,\n",
      "          1.4679e+09, 1.4701e+09, 1.4806e+09, 1.4827e+09, 1.4788e+09,\n",
      "          1.4799e+09, 1.4820e+09, 1.4809e+09, 1.4832e+09, 1.4856e+09,\n",
      "          1.4861e+09, 1.4855e+09, 1.4819e+09]]])\n",
      "Labels: tensor([[[2.3600e+02, 3.1900e+02, 2.7200e+02, 2.3000e+02, 2.8100e+02,\n",
      "          1.5300e+02, 1.7500e+02, 1.7300e+02, 1.7200e+02, 1.2100e+02,\n",
      "          2.1400e+02, 1.7900e+02]],\n",
      "\n",
      "        [[5.8278e+02, 5.2498e+02, 5.7081e+02, 6.1121e+02, 5.8853e+02,\n",
      "          5.1562e+02, 6.1734e+02, 5.7952e+02, 6.1779e+02, 5.7563e+02,\n",
      "          5.7923e+02, 5.3316e+02]],\n",
      "\n",
      "        [[4.6079e+02, 3.4881e+02, 4.9487e+02, 3.7710e+02, 3.8564e+02,\n",
      "          4.9133e+02, 4.4269e+02, 4.8699e+02, 3.9629e+02, 3.3255e+02,\n",
      "          4.0141e+02, 3.4526e+02]],\n",
      "\n",
      "        [[5.2192e+02, 4.5951e+02, 4.2066e+02, 5.0601e+02, 4.0272e+02,\n",
      "          4.6617e+02, 5.5476e+02, 4.9566e+02, 4.3638e+02, 4.2405e+02,\n",
      "          4.7859e+02, 4.1075e+02]],\n",
      "\n",
      "        [[7.6255e+02, 7.1500e+02, 1.1140e+03, 6.2212e+02, 9.8223e+02,\n",
      "          1.0057e+03, 9.7178e+02, 1.0665e+03, 7.8226e+02, 6.6504e+02,\n",
      "          9.8449e+02, 7.1386e+02]],\n",
      "\n",
      "        [[9.1867e+02, 8.8713e+02, 9.2911e+02, 8.3971e+02, 8.6228e+02,\n",
      "          8.3782e+02, 9.4439e+02, 9.6978e+02, 8.6398e+02, 8.6703e+02,\n",
      "          8.7686e+02, 9.8327e+02]],\n",
      "\n",
      "        [[1.3634e+03, 1.3404e+03, 1.3489e+03, 1.4172e+03, 1.5055e+03,\n",
      "          1.6429e+03, 1.4626e+03, 1.5244e+03, 1.4639e+03, 1.6528e+03,\n",
      "          1.5135e+03, 1.4942e+03]],\n",
      "\n",
      "        [[1.4789e+09, 1.4824e+09, 1.4811e+09, 1.4828e+09, 1.4825e+09,\n",
      "          1.4814e+09, 1.4829e+09, 1.4829e+09, 1.4954e+09, 1.4950e+09,\n",
      "          1.4953e+09, 1.4941e+09]]])\n",
      "Batch 2\n",
      "Inputs: tensor([[[7.1428e+04, 7.2064e+04, 6.9571e+04, 6.0041e+04, 5.6135e+04,\n",
      "          5.4774e+04, 5.2599e+04, 5.1700e+04, 4.5175e+04, 4.5710e+04,\n",
      "          4.8625e+04, 3.9804e+04, 4.4244e+04, 3.5629e+04, 3.5745e+04,\n",
      "          3.4445e+04, 3.2574e+04, 3.1169e+04]],\n",
      "\n",
      "        [[7.7906e+02, 7.2393e+02, 7.3343e+02, 7.0778e+02, 7.6992e+02,\n",
      "          7.7539e+02, 7.6223e+02, 7.7136e+02, 7.3563e+02, 7.0206e+02,\n",
      "          6.5219e+02, 7.5474e+02, 6.9940e+02, 7.8054e+02, 7.4822e+02,\n",
      "          6.7720e+02, 7.4672e+02, 7.3616e+02]],\n",
      "\n",
      "        [[4.8029e+02, 4.8486e+02, 4.8398e+02, 4.9598e+02, 4.9427e+02,\n",
      "          4.9357e+02, 4.8296e+02, 4.8845e+02, 4.8529e+02, 4.8692e+02,\n",
      "          4.8660e+02, 4.9182e+02, 4.9862e+02, 4.9876e+02, 4.8842e+02,\n",
      "          4.8513e+02, 4.7859e+02, 4.9424e+02]],\n",
      "\n",
      "        [[3.7900e+02, 3.5700e+02, 4.9900e+02, 5.2100e+02, 4.6600e+02,\n",
      "          4.3300e+02, 4.2200e+02, 3.5600e+02, 3.2100e+02, 4.1200e+02,\n",
      "          2.9200e+02, 3.4700e+02, 3.2700e+02, 3.2200e+02, 3.0500e+02,\n",
      "          4.0000e+02, 4.7200e+02, 4.3000e+02]],\n",
      "\n",
      "        [[3.4499e+07, 3.3362e+07, 3.5358e+07, 3.6208e+07, 3.8413e+07,\n",
      "          3.7982e+07, 3.3982e+07, 3.3162e+07, 3.0312e+07, 2.7398e+07,\n",
      "          3.1508e+07, 3.1367e+07, 2.9129e+07, 2.6399e+07, 2.8571e+07,\n",
      "          2.6733e+07, 2.3733e+07, 2.4689e+07]],\n",
      "\n",
      "        [[1.6668e+03, 1.7193e+03, 1.4958e+03, 1.5922e+03, 1.7028e+03,\n",
      "          1.6186e+03, 1.8686e+03, 1.5933e+03, 1.3969e+03, 1.5032e+03,\n",
      "          1.7124e+03, 1.8363e+03, 1.8571e+03, 1.9468e+03, 1.6140e+03,\n",
      "          1.7784e+03, 1.8540e+03, 1.9772e+03]],\n",
      "\n",
      "        [[7.6598e+02, 9.8392e+02, 8.2713e+02, 1.0490e+03, 1.0148e+03,\n",
      "          9.5772e+02, 9.2058e+02, 8.7806e+02, 9.2109e+02, 9.0545e+02,\n",
      "          9.9143e+02, 1.0899e+03, 9.5200e+02, 9.2383e+02, 9.3259e+02,\n",
      "          9.5565e+02, 1.0918e+03, 8.4877e+02]],\n",
      "\n",
      "        [[6.7049e+02, 8.1241e+02, 7.1942e+02, 6.7572e+02, 7.1718e+02,\n",
      "          8.1515e+02, 7.2403e+02, 7.4857e+02, 7.9772e+02, 6.8773e+02,\n",
      "          7.0788e+02, 7.2912e+02, 7.2799e+02, 7.9394e+02, 8.0019e+02,\n",
      "          7.7724e+02, 7.3819e+02, 8.0794e+02]]])\n",
      "Labels: tensor([[[2.6240e+04, 2.5113e+04, 2.3835e+04, 2.5457e+04, 2.5426e+04,\n",
      "          2.0148e+04, 2.3924e+04, 2.2231e+04, 2.2334e+04, 1.9832e+04,\n",
      "          2.0585e+04, 2.1883e+04]],\n",
      "\n",
      "        [[7.0290e+02, 7.2917e+02, 7.2837e+02, 8.0863e+02, 7.6733e+02,\n",
      "          7.1460e+02, 7.0164e+02, 7.3576e+02, 8.1876e+02, 7.2929e+02,\n",
      "          7.2457e+02, 7.6132e+02]],\n",
      "\n",
      "        [[4.9593e+02, 5.0359e+02, 4.8840e+02, 4.9153e+02, 4.8010e+02,\n",
      "          4.8900e+02, 4.8732e+02, 4.8792e+02, 4.8628e+02, 5.0336e+02,\n",
      "          4.8806e+02, 4.9405e+02]],\n",
      "\n",
      "        [[3.6900e+02, 3.7000e+02, 2.3300e+02, 2.1800e+02, 2.1300e+02,\n",
      "          2.1000e+02, 2.1100e+02, 1.3900e+02, 2.1500e+02, 1.7100e+02,\n",
      "          1.4100e+02, 1.7100e+02]],\n",
      "\n",
      "        [[2.2591e+07, 2.0725e+07, 2.1041e+07, 1.8461e+07, 2.3305e+07,\n",
      "          2.0850e+07, 1.9255e+07, 1.7820e+07, 1.6096e+07, 1.6988e+07,\n",
      "          1.4140e+07, 1.3058e+07]],\n",
      "\n",
      "        [[2.1700e+03, 1.7374e+03, 1.5950e+03, 1.7778e+03, 1.9380e+03,\n",
      "          2.0038e+03, 2.0671e+03, 2.1630e+03, 1.9214e+03, 2.0117e+03,\n",
      "          2.0344e+03, 2.2114e+03]],\n",
      "\n",
      "        [[9.2541e+02, 8.8746e+02, 9.7199e+02, 1.0991e+03, 8.1909e+02,\n",
      "          8.2181e+02, 9.0615e+02, 8.5479e+02, 8.6593e+02, 7.9578e+02,\n",
      "          9.6276e+02, 6.7803e+02]],\n",
      "\n",
      "        [[7.3595e+02, 7.2223e+02, 7.9428e+02, 9.0432e+02, 8.2535e+02,\n",
      "          8.4388e+02, 8.6895e+02, 9.6981e+02, 9.3071e+02, 9.7421e+02,\n",
      "          1.0490e+03, 9.7978e+02]]])\n",
      "Batch 3\n",
      "Inputs: tensor([[[1.1669e+03, 1.4462e+03, 1.7063e+03, 1.5137e+03, 1.4814e+03,\n",
      "          1.5768e+03, 1.2352e+03, 1.4556e+03, 1.5711e+03, 1.2405e+03,\n",
      "          1.4762e+03, 1.4782e+03, 1.2131e+03, 1.4553e+03, 1.7012e+03,\n",
      "          1.5555e+03, 1.5594e+03, 1.5907e+03]],\n",
      "\n",
      "        [[7.9997e+02, 8.0868e+02, 8.8192e+02, 8.5340e+02, 8.4471e+02,\n",
      "          9.3703e+02, 8.2448e+02, 8.0991e+02, 8.2021e+02, 7.8638e+02,\n",
      "          8.3165e+02, 8.2164e+02, 8.0668e+02, 7.7626e+02, 8.0077e+02,\n",
      "          7.9399e+02, 7.3755e+02, 7.5850e+02]],\n",
      "\n",
      "        [[1.4571e+05, 1.3558e+05, 1.3701e+05, 1.3109e+05, 1.4296e+05,\n",
      "          1.2216e+05, 1.0166e+05, 1.0127e+05, 9.1832e+04, 8.5067e+04,\n",
      "          8.7782e+04, 6.5708e+04, 8.1175e+04, 6.9579e+04, 7.6923e+04,\n",
      "          7.3555e+04, 6.8280e+04, 7.1093e+04]],\n",
      "\n",
      "        [[1.6304e+02, 2.0436e+02, 1.6178e+02, 1.3785e+02, 1.4327e+02,\n",
      "          1.4588e+02, 1.7788e+02, 1.7644e+02, 1.6987e+02, 1.7166e+02,\n",
      "          1.9203e+02, 1.9395e+02, 1.7490e+02, 2.1266e+02, 1.7520e+02,\n",
      "          1.4943e+02, 1.5745e+02, 1.5138e+02]],\n",
      "\n",
      "        [[3.9541e+04, 2.2935e+04, 3.5107e+04, 3.1203e+04, 3.8552e+04,\n",
      "          4.2049e+04, 3.8433e+04, 4.9073e+04, 6.7550e+04, 6.6472e+04,\n",
      "          5.9852e+04, 6.8526e+04, 6.8696e+04, 7.3075e+04, 6.5989e+04,\n",
      "          7.1040e+04, 7.8028e+04, 7.3898e+04]],\n",
      "\n",
      "        [[4.5324e+06, 3.9249e+06, 3.4935e+06, 3.3861e+06, 2.7190e+06,\n",
      "          3.6846e+06, 2.0292e+06, 3.2759e+06, 4.3979e+06, 2.5190e+06,\n",
      "          2.1898e+06, 1.8423e+06, 2.1245e+06, 2.0603e+06, 3.2172e+06,\n",
      "          2.2778e+06, 2.5979e+06, 2.7551e+06]],\n",
      "\n",
      "        [[6.4918e+02, 8.5847e+02, 8.4492e+02, 6.7741e+02, 7.0801e+02,\n",
      "          6.9980e+02, 7.3596e+02, 7.4874e+02, 7.6672e+02, 7.5083e+02,\n",
      "          8.4291e+02, 5.2669e+02, 7.8973e+02, 6.9078e+02, 8.7574e+02,\n",
      "          6.9781e+02, 6.3387e+02, 7.0147e+02]],\n",
      "\n",
      "        [[5.3700e+02, 4.6400e+02, 4.3300e+02, 5.7900e+02, 6.5200e+02,\n",
      "          7.2700e+02, 6.6900e+02, 6.7800e+02, 6.8500e+02, 6.7600e+02,\n",
      "          5.6400e+02, 5.5900e+02, 5.5100e+02, 5.4900e+02, 5.8100e+02,\n",
      "          5.3600e+02, 4.7800e+02, 5.0800e+02]]])\n",
      "Labels: tensor([[[1.2012e+03, 1.4502e+03, 1.5625e+03, 1.3321e+03, 1.5433e+03,\n",
      "          1.4827e+03, 1.2666e+03, 1.5040e+03, 1.7437e+03, 1.5669e+03,\n",
      "          1.5645e+03, 1.5980e+03]],\n",
      "\n",
      "        [[7.9713e+02, 7.9058e+02, 7.9290e+02, 7.3269e+02, 7.7335e+02,\n",
      "          7.8295e+02, 7.4966e+02, 7.8458e+02, 7.8665e+02, 6.7232e+02,\n",
      "          6.3725e+02, 7.2585e+02]],\n",
      "\n",
      "        [[5.6814e+04, 4.4752e+04, 4.3407e+04, 4.7858e+04, 4.4714e+04,\n",
      "          3.2323e+04, 3.8684e+04, 3.9832e+04, 4.2784e+04, 4.0035e+04,\n",
      "          4.1685e+04, 4.3279e+04]],\n",
      "\n",
      "        [[1.9174e+02, 1.8598e+02, 1.8775e+02, 1.8988e+02, 2.1046e+02,\n",
      "          2.0052e+02, 1.8921e+02, 2.2430e+02, 1.7981e+02, 1.6569e+02,\n",
      "          1.6876e+02, 1.6707e+02]],\n",
      "\n",
      "        [[8.3296e+04, 8.1395e+04, 7.9799e+04, 9.5174e+04, 7.0811e+04,\n",
      "          6.8351e+04, 7.3658e+04, 6.1547e+04, 5.5399e+04, 7.0902e+04,\n",
      "          5.8200e+04, 5.8203e+04]],\n",
      "\n",
      "        [[1.7361e+06, 3.4650e+06, 3.1928e+06, 2.5128e+06, 2.0779e+06,\n",
      "          2.1468e+06, 2.8880e+06, 1.7391e+06, 2.5500e+06, 1.8245e+06,\n",
      "          2.5216e+06, 1.7694e+06]],\n",
      "\n",
      "        [[9.2742e+02, 6.9906e+02, 7.4733e+02, 6.1821e+02, 7.8517e+02,\n",
      "          6.5945e+02, 6.4854e+02, 6.8487e+02, 6.9656e+02, 6.4700e+02,\n",
      "          6.8686e+02, 7.2344e+02]],\n",
      "\n",
      "        [[4.8300e+02, 5.3900e+02, 4.7100e+02, 3.2800e+02, 4.7700e+02,\n",
      "          2.3700e+02, 2.3500e+02, 2.5600e+02, 1.9700e+02, 1.7000e+02,\n",
      "          1.5700e+02, 1.2400e+02]]])\n",
      "Batch 4\n",
      "Inputs: tensor([[[ 161.2172,  144.2369,  213.9967,  205.4618,  132.9848,  203.2686,\n",
      "           220.6131,  206.5921,  127.3934,  204.8257,  228.2327,  191.1204,\n",
      "           168.0039,  126.8759,  204.1721,  202.2417,  122.8385,  209.6733]],\n",
      "\n",
      "        [[2113.1155, 2061.7371, 2205.6135, 2086.1990, 1663.0920, 2385.9778,\n",
      "          2039.4598, 1646.2819, 2288.2761, 2425.8801, 2160.6709, 2353.9871,\n",
      "          2346.9001, 2185.4294, 2333.8787, 2107.7666, 1917.2341, 2497.3044]],\n",
      "\n",
      "        [[  17.0547,   16.4083,   17.2363,   15.7789,   16.0949,   16.4096,\n",
      "            15.8618,   15.1841,   15.8631,   15.4263,   14.0454,   15.2554,\n",
      "            13.9427,   13.8990,   13.4235,   14.3938,   13.2235,   12.3295]],\n",
      "\n",
      "        [[1724.8795, 1558.7723, 1691.7893, 1568.1213, 1348.5547, 1303.5620,\n",
      "          1671.5732, 1234.0824, 1417.8927, 1535.1693, 1132.9565, 1264.8250,\n",
      "          1446.6017, 1444.2104, 1420.0199, 1513.1810, 1222.8281, 1365.6481]],\n",
      "\n",
      "        [[1952.1500, 1263.6954, 2143.5791, 1528.5325, 1511.9009, 1698.2706,\n",
      "          1301.2245, 1815.8154, 1975.7822, 1597.1914, 1893.2196, 1563.1904,\n",
      "          1806.8414, 1475.1626, 2015.1664, 1296.3312, 1309.0420, 1902.3871]],\n",
      "\n",
      "        [[1590.3875, 1026.7316, 1348.2632, 1241.5630, 1261.0229, 1592.4523,\n",
      "          1041.6240, 1304.2734, 1492.5673, 1504.3811, 1518.0636, 1569.4973,\n",
      "          1486.0558, 1006.1186, 1256.9930, 1226.4236, 1178.1902, 1422.9438]],\n",
      "\n",
      "        [[5249.0000, 5847.0000, 5855.0000, 5423.0000, 5560.0000, 5313.0000,\n",
      "          5160.0000, 5578.0000, 5107.0000, 5229.0000, 5036.0000, 3791.0000,\n",
      "          4941.0000, 4431.0000, 4681.0000, 4559.0000, 4810.0000, 3989.0000]],\n",
      "\n",
      "        [[1556.5320, 2328.7058, 2098.2703, 1910.5533, 2190.0027, 2221.2222,\n",
      "          1849.7771, 1848.8842, 1600.4967, 1725.8334, 1616.4934, 2156.9385,\n",
      "          1611.8490, 2230.2607, 2067.4304, 2259.9639, 2129.8494, 2160.8533]]])\n",
      "Labels: tensor([[[ 212.6209,  196.9433,  132.2496,  202.1054,  210.3835,  197.7813,\n",
      "           160.8750,  119.4531,  197.9672,  202.4235,  119.1576,  199.0261]],\n",
      "\n",
      "        [[1943.1654, 1873.2942, 1962.1810, 2316.8052, 2340.5100, 2745.2490,\n",
      "          2690.1980, 2316.2312, 2442.9377, 2140.7395, 1723.7278, 2579.3855]],\n",
      "\n",
      "        [[  14.0618,   12.4136,   12.8382,   12.2858,   12.8429,   11.3781,\n",
      "            11.9199,   11.9087,   12.2824,   10.3367,   11.3435,   10.5168]],\n",
      "\n",
      "        [[1580.8488,  831.8372, 1111.3600, 1304.6091, 1146.2954,  761.1700,\n",
      "          1176.7754, 1100.5210, 1142.9811, 1284.1323,  941.2864, 1014.4178]],\n",
      "\n",
      "        [[1148.5311, 2009.6661, 1788.1122, 1837.2925, 1692.0657, 1280.2393,\n",
      "          1569.7440, 1169.1677, 1749.4285,  899.2394, 1162.0831, 1986.8884]],\n",
      "\n",
      "        [[1019.2599, 1171.6246, 1362.0165, 1387.5327, 1457.1693, 1548.2910,\n",
      "          1409.4047,  960.2369, 1142.9568, 1046.5319, 1070.6161, 1242.4246]],\n",
      "\n",
      "        [[3509.0000, 3843.0000, 3138.0000, 3015.0000, 3211.0000, 2272.0000,\n",
      "          2983.0000, 2763.0000, 2605.0000, 2304.0000, 2239.0000, 2677.0000]],\n",
      "\n",
      "        [[2111.3516, 2286.3723, 1635.2053, 1899.0532, 1676.0577, 2200.5002,\n",
      "          1660.5155, 2447.9424, 2128.6863, 2489.2280, 2063.4602, 2386.0173]]])\n",
      "Batch 5\n",
      "Inputs: tensor([[[5.5164e+04, 4.9794e+04, 4.6148e+04, 4.4496e+04, 5.4912e+04,\n",
      "          5.4598e+04, 5.0544e+04, 5.2551e+04, 4.7557e+04, 4.8026e+04,\n",
      "          4.9519e+04, 4.2417e+04, 4.9610e+04, 4.4189e+04, 4.7709e+04,\n",
      "          4.4886e+04, 4.5536e+04, 4.5629e+04]],\n",
      "\n",
      "        [[       nan, 7.3321e-01, 7.5068e-01, 7.3996e-01, 6.9810e-01,\n",
      "          7.5631e-01, 7.6748e-01, 7.8910e-01, 8.1962e-01, 7.5086e-01,\n",
      "          7.6387e-01, 8.3776e-01, 7.6034e-01, 8.0336e-01, 8.0238e-01,\n",
      "          8.2448e-01, 8.9286e-01, 8.8356e-01]],\n",
      "\n",
      "        [[1.2966e+04, 1.2311e+04, 1.3965e+04, 1.3196e+04, 1.5596e+04,\n",
      "          1.4894e+04, 1.4204e+04, 1.6470e+04, 1.4820e+04, 1.6952e+04,\n",
      "          1.5995e+04, 1.1287e+04, 1.4931e+04, 1.4161e+04, 1.5106e+04,\n",
      "          1.5165e+04, 1.5632e+04, 1.5145e+04]],\n",
      "\n",
      "        [[1.1700e+02, 1.0533e+02, 1.2484e+02, 1.3991e+02, 1.2616e+02,\n",
      "          1.3634e+02, 1.2932e+02, 1.3381e+02, 1.3300e+02, 1.1526e+02,\n",
      "          1.1594e+02, 1.0208e+02, 1.1155e+02, 1.1138e+02, 1.2976e+02,\n",
      "          1.3366e+02, 1.3108e+02, 1.2878e+02]],\n",
      "\n",
      "        [[2.6048e+02, 2.2399e+02, 2.3667e+02, 2.7674e+02, 2.3785e+02,\n",
      "          2.7139e+02, 2.3572e+02, 3.0503e+02, 2.9282e+02, 3.0132e+02,\n",
      "          2.7045e+02, 3.0294e+02, 2.6771e+02, 2.3773e+02, 2.4224e+02,\n",
      "          2.7432e+02, 2.4022e+02, 2.7254e+02]],\n",
      "\n",
      "        [[3.5040e+02, 3.9933e+02, 4.1454e+02, 4.1327e+02, 3.6286e+02,\n",
      "          4.1351e+02, 4.1717e+02, 4.1410e+02, 4.2496e+02, 4.2762e+02,\n",
      "          4.0315e+02, 3.5622e+02, 4.0460e+02, 4.3151e+02, 4.3114e+02,\n",
      "          4.1630e+02, 4.0266e+02, 3.9979e+02]],\n",
      "\n",
      "        [[5.1017e+02, 5.3316e+02, 5.3171e+02, 4.8889e+02, 5.0870e+02,\n",
      "          5.3349e+02, 5.3707e+02, 5.1645e+02, 5.1059e+02, 5.4120e+02,\n",
      "          5.2289e+02, 5.2233e+02, 5.3119e+02, 5.1792e+02, 5.2221e+02,\n",
      "          5.2692e+02, 5.2860e+02, 5.4094e+02]],\n",
      "\n",
      "        [[       nan,        nan, 1.4828e+09, 1.4789e+09, 1.4683e+09,\n",
      "          1.4679e+09, 1.4701e+09, 1.4806e+09, 1.4827e+09, 1.4788e+09,\n",
      "          1.4799e+09, 1.4820e+09, 1.4809e+09, 1.4832e+09, 1.4856e+09,\n",
      "          1.4861e+09, 1.4855e+09, 1.4819e+09]]])\n",
      "Labels: tensor([[[4.0294e+04, 3.9480e+04, 3.6094e+04, 3.7952e+04, 3.6616e+04,\n",
      "          2.9496e+04, 3.8115e+04, 3.8899e+04, 3.6289e+04, 3.6477e+04,\n",
      "          3.9629e+04, 3.8720e+04]],\n",
      "\n",
      "        [[8.3815e-01, 8.4246e-01, 8.3804e-01, 8.5182e-01, 8.5189e-01,\n",
      "          8.9991e-01, 9.0195e-01, 9.3837e-01, 9.4472e-01, 9.5077e-01,\n",
      "          9.4769e-01, 1.0000e+00]],\n",
      "\n",
      "        [[1.3758e+04, 1.3221e+04, 1.2026e+04, 1.3012e+04, 1.3489e+04,\n",
      "          9.9150e+03, 1.1963e+04, 1.0882e+04, 1.0064e+04, 8.7680e+03,\n",
      "          9.4070e+03, 9.7320e+03]],\n",
      "\n",
      "        [[1.2664e+02, 1.2434e+02, 1.2361e+02, 1.2049e+02, 1.1271e+02,\n",
      "          1.1149e+02, 1.1094e+02, 1.0943e+02, 1.1873e+02, 1.3024e+02,\n",
      "          1.2333e+02, 1.2245e+02]],\n",
      "\n",
      "        [[2.4274e+02, 3.0287e+02, 3.1185e+02, 3.1100e+02, 2.8246e+02,\n",
      "          3.0542e+02, 2.6547e+02, 2.4043e+02, 2.4695e+02, 2.8549e+02,\n",
      "          2.4558e+02, 2.8739e+02]],\n",
      "\n",
      "        [[4.0211e+02, 4.3015e+02, 4.0998e+02, 3.6810e+02, 4.2638e+02,\n",
      "          4.3160e+02, 3.7919e+02, 4.3331e+02, 4.4255e+02, 3.9126e+02,\n",
      "          3.9352e+02, 4.2848e+02]],\n",
      "\n",
      "        [[5.2476e+02, 5.4371e+02, 5.3118e+02, 5.2764e+02, 5.3238e+02,\n",
      "          5.4642e+02, 5.4672e+02, 5.4317e+02, 5.2925e+02, 5.3987e+02,\n",
      "          5.1005e+02, 5.3207e+02]],\n",
      "\n",
      "        [[1.4789e+09, 1.4824e+09, 1.4811e+09, 1.4828e+09, 1.4825e+09,\n",
      "          1.4814e+09, 1.4829e+09, 1.4829e+09, 1.4954e+09, 1.4950e+09,\n",
      "          1.4953e+09, 1.4941e+09]]])\n",
      "Batch 6\n",
      "Inputs: tensor([[[1.3385e+08, 1.3469e+08, 1.3743e+08, 1.2306e+08, 1.0931e+08,\n",
      "          1.1953e+08, 1.2232e+08, 1.2691e+08, 1.2669e+08, 1.0925e+08,\n",
      "          1.2363e+08, 1.2981e+08, 1.3035e+08, 1.2090e+08, 1.1450e+08,\n",
      "          1.1175e+08, 1.1085e+08, 1.0781e+08]],\n",
      "\n",
      "        [[4.8260e+02, 4.5783e+02, 4.6763e+02, 4.4282e+02, 4.8852e+02,\n",
      "          4.4620e+02, 4.7110e+02, 4.6264e+02, 4.9151e+02, 4.6944e+02,\n",
      "          5.2121e+02, 5.3975e+02, 4.5591e+02, 4.4712e+02, 4.9848e+02,\n",
      "          4.9335e+02, 4.7019e+02, 5.2053e+02]],\n",
      "\n",
      "        [[2.8845e+04, 2.9289e+04, 2.5181e+04, 2.5461e+04, 2.5790e+04,\n",
      "          1.7503e+04, 2.0469e+04, 1.7037e+04, 1.6474e+04, 1.5501e+04,\n",
      "          1.6408e+04, 1.3485e+04, 1.2687e+04, 1.3492e+04, 1.4223e+04,\n",
      "          1.3188e+04, 1.0894e+04, 8.9240e+03]],\n",
      "\n",
      "        [[1.4318e+03, 1.2293e+03, 1.3484e+03, 1.4718e+03, 1.4597e+03,\n",
      "          1.5247e+03, 1.5944e+03, 1.3643e+03, 1.3375e+03, 1.2927e+03,\n",
      "          1.5601e+03, 1.5516e+03, 1.3750e+03, 1.2470e+03, 1.3675e+03,\n",
      "          1.4160e+03, 1.5155e+03, 1.5459e+03]],\n",
      "\n",
      "        [[1.9280e+07, 1.6786e+07, 1.5605e+07, 1.6364e+07, 1.7194e+07,\n",
      "          1.3840e+07, 1.3191e+07, 1.7303e+07, 1.6298e+07, 1.3611e+07,\n",
      "          1.2991e+07, 1.5605e+07, 1.6901e+07, 1.2459e+07, 1.0459e+07,\n",
      "          1.1170e+07, 7.9132e+06, 8.6590e+06]],\n",
      "\n",
      "        [[3.6997e+01, 5.3556e+01, 4.9252e+01, 4.4731e+01, 4.0075e+01,\n",
      "          4.6575e+01, 4.8345e+01, 4.0445e+01, 4.3200e+01, 5.0452e+01,\n",
      "          3.9651e+01, 3.8241e+01, 3.7876e+01, 5.4105e+01, 4.8868e+01,\n",
      "          5.2524e+01, 4.0098e+01, 4.9718e+01]],\n",
      "\n",
      "        [[1.6537e+03, 1.4358e+03, 1.3116e+03, 1.6325e+03, 1.3929e+03,\n",
      "          1.4152e+03, 1.4150e+03, 1.5305e+03, 1.4874e+03, 1.2959e+03,\n",
      "          1.2426e+03, 1.2638e+03, 1.3127e+03, 1.2687e+03, 1.1031e+03,\n",
      "          1.0272e+03, 1.1994e+03, 1.1617e+03]],\n",
      "\n",
      "        [[1.1531e+03, 1.1016e+03, 1.1423e+03, 1.1125e+03, 1.0808e+03,\n",
      "          1.0768e+03, 1.0372e+03, 1.0527e+03, 1.1069e+03, 9.7806e+02,\n",
      "          1.0077e+03, 9.8000e+02, 1.0290e+03, 9.3790e+02, 9.5230e+02,\n",
      "          9.5235e+02, 8.7562e+02, 9.0294e+02]]])\n",
      "Labels: tensor([[[1.0912e+08, 1.1532e+08, 1.0831e+08, 9.6940e+07, 1.0341e+08,\n",
      "          1.0820e+08, 1.0207e+08, 9.8338e+07, 9.4140e+07, 9.8507e+07,\n",
      "          8.5204e+07, 8.4661e+07]],\n",
      "\n",
      "        [[5.6203e+02, 4.9773e+02, 4.6615e+02, 4.8563e+02, 4.7838e+02,\n",
      "          4.8975e+02, 4.7112e+02, 5.0811e+02, 5.5015e+02, 4.9335e+02,\n",
      "          5.8851e+02, 5.4492e+02]],\n",
      "\n",
      "        [[1.1635e+04, 1.0405e+04, 1.1066e+04, 9.7720e+03, 1.1019e+04,\n",
      "          9.0170e+03, 7.1410e+03, 5.8470e+03, 4.1380e+03, 4.0450e+03,\n",
      "          4.2280e+03, 3.1740e+03]],\n",
      "\n",
      "        [[1.5649e+03, 1.3093e+03, 1.3712e+03, 1.2929e+03, 1.5495e+03,\n",
      "          1.5191e+03, 1.4105e+03, 1.2669e+03, 1.3424e+03, 1.4150e+03,\n",
      "          1.5220e+03, 1.5717e+03]],\n",
      "\n",
      "        [[1.0446e+07, 7.9846e+06, 8.3623e+06, 8.3565e+06, 8.5937e+06,\n",
      "          8.3625e+06, 6.5979e+06, 6.5416e+06, 6.9554e+06, 5.1786e+06,\n",
      "          5.2076e+06, 3.6302e+06]],\n",
      "\n",
      "        [[5.4524e+01, 4.2167e+01, 4.6868e+01, 5.2595e+01, 4.3187e+01,\n",
      "          4.2541e+01, 4.9788e+01, 6.1578e+01, 5.6217e+01, 5.7270e+01,\n",
      "          5.3752e+01, 6.1923e+01]],\n",
      "\n",
      "        [[9.4971e+02, 1.3100e+03, 1.1325e+03, 1.2152e+03, 8.8570e+02,\n",
      "          1.0441e+03, 9.8269e+02, 9.6236e+02, 1.1182e+03, 1.0042e+03,\n",
      "          1.0139e+03, 8.3631e+02]],\n",
      "\n",
      "        [[8.6433e+02, 8.6438e+02, 8.0971e+02, 8.0451e+02, 7.8964e+02,\n",
      "          8.5732e+02, 7.4022e+02, 7.5141e+02, 6.9763e+02, 6.9880e+02,\n",
      "          6.9928e+02, 6.6844e+02]]])\n",
      "Batch 7\n",
      "Inputs: tensor([[[1.4337e+03, 1.2373e+03, 1.4370e+03, 1.5112e+03, 1.4808e+03,\n",
      "          1.3039e+03, 1.2542e+03, 1.8829e+03, 1.4453e+03, 1.4129e+03,\n",
      "          1.9675e+03, 1.2595e+03, 1.2535e+03, 1.3240e+03, 1.5225e+03,\n",
      "          1.1112e+03, 1.6864e+03, 1.3995e+03]],\n",
      "\n",
      "        [[6.0231e+06, 6.1908e+06, 5.7789e+06, 5.8532e+06, 5.2165e+06,\n",
      "          6.1077e+06, 5.5470e+06, 4.5697e+06, 5.4167e+06, 5.2307e+06,\n",
      "          3.7025e+06, 4.8334e+06, 4.3104e+06, 4.4672e+06, 3.9235e+06,\n",
      "          4.4513e+06, 3.9947e+06, 3.9935e+06]],\n",
      "\n",
      "        [[1.5384e+03, 1.6829e+03, 1.5912e+03, 1.3987e+03, 1.7580e+03,\n",
      "          1.2492e+03, 1.4557e+03, 1.5494e+03, 1.5920e+03, 1.6243e+03,\n",
      "          1.5456e+03, 1.5880e+03, 1.6422e+03, 1.8041e+03, 2.0838e+03,\n",
      "          1.9914e+03, 1.9765e+03, 1.9731e+03]],\n",
      "\n",
      "        [[1.3522e+03, 1.4345e+03, 1.5406e+03, 1.5299e+03, 1.5201e+03,\n",
      "          1.5372e+03, 1.5542e+03, 1.5623e+03, 1.5626e+03, 1.6554e+03,\n",
      "          1.5230e+03, 1.6260e+03, 1.5250e+03, 1.6119e+03, 1.6002e+03,\n",
      "          1.6650e+03, 1.5550e+03, 1.5922e+03]],\n",
      "\n",
      "        [[4.1094e+02, 4.1241e+02, 3.9956e+02, 4.3241e+02, 4.3764e+02,\n",
      "          4.4234e+02, 4.2050e+02, 4.3056e+02, 4.3454e+02, 4.2248e+02,\n",
      "          4.5164e+02, 4.3157e+02, 4.5978e+02, 4.3138e+02, 4.5315e+02,\n",
      "          4.3756e+02, 4.3824e+02, 4.7843e+02]],\n",
      "\n",
      "        [[4.3428e+04, 3.9772e+04, 4.3537e+04, 4.2599e+04, 3.9323e+04,\n",
      "          4.0763e+04, 3.9470e+04, 4.2439e+04, 4.6650e+04, 3.6320e+04,\n",
      "          3.9061e+04, 3.7335e+04, 3.8939e+04, 3.5176e+04, 3.4609e+04,\n",
      "          3.6950e+04, 3.4830e+04, 3.6871e+04]],\n",
      "\n",
      "        [[9.4338e+02, 8.4643e+02, 9.7447e+02, 9.9382e+02, 9.4411e+02,\n",
      "          9.1341e+02, 1.0680e+03, 9.3249e+02, 8.4930e+02, 1.0428e+03,\n",
      "          1.1335e+03, 1.0315e+03, 9.3917e+02, 9.2302e+02, 9.4812e+02,\n",
      "          1.0997e+03, 9.2652e+02, 9.2770e+02]],\n",
      "\n",
      "        [[3.8387e+02, 3.6922e+02, 3.0309e+02, 3.2385e+02, 2.9437e+02,\n",
      "          3.5813e+02, 3.3875e+02, 3.8746e+02, 3.5045e+02, 3.6646e+02,\n",
      "          4.1479e+02, 3.6396e+02, 3.9693e+02, 3.9464e+02, 3.2115e+02,\n",
      "          3.2848e+02, 3.3407e+02, 3.7704e+02]]])\n",
      "Labels: tensor([[[1.2584e+03, 1.9800e+03, 1.3834e+03, 1.1367e+03, 1.8364e+03,\n",
      "          1.2893e+03, 1.5741e+03, 1.4581e+03, 1.8332e+03, 9.7585e+02,\n",
      "          1.5171e+03, 9.3506e+02]],\n",
      "\n",
      "        [[4.4087e+06, 4.4551e+06, 4.2447e+06, 4.0593e+06, 3.6956e+06,\n",
      "          3.7302e+06, 4.1860e+06, 3.7196e+06, 3.9900e+06, 3.9518e+06,\n",
      "          4.2435e+06, 3.6552e+06]],\n",
      "\n",
      "        [[1.5048e+03, 1.9122e+03, 1.8010e+03, 1.9966e+03, 1.8711e+03,\n",
      "          1.9816e+03, 2.0517e+03, 1.7088e+03, 1.8057e+03, 2.0982e+03,\n",
      "          2.1562e+03, 2.0140e+03]],\n",
      "\n",
      "        [[1.6570e+03, 1.6245e+03, 1.5852e+03, 1.6742e+03, 1.6750e+03,\n",
      "          1.6958e+03, 1.6558e+03, 1.6723e+03, 1.6506e+03, 1.7387e+03,\n",
      "          1.7104e+03, 1.6834e+03]],\n",
      "\n",
      "        [[4.6409e+02, 4.5075e+02, 4.5932e+02, 4.7692e+02, 4.8932e+02,\n",
      "          4.7700e+02, 4.9708e+02, 4.8258e+02, 4.8432e+02, 4.8095e+02,\n",
      "          4.8683e+02, 4.7531e+02]],\n",
      "\n",
      "        [[3.3358e+04, 3.3683e+04, 3.8150e+04, 2.8430e+04, 3.3968e+04,\n",
      "          3.1777e+04, 3.3646e+04, 3.1759e+04, 3.3706e+04, 3.2592e+04,\n",
      "          2.8627e+04, 2.6123e+04]],\n",
      "\n",
      "        [[1.1113e+03, 1.0048e+03, 8.9684e+02, 1.1114e+03, 1.1704e+03,\n",
      "          1.1208e+03, 9.6650e+02, 9.4609e+02, 9.9646e+02, 1.1038e+03,\n",
      "          1.0023e+03, 9.9623e+02]],\n",
      "\n",
      "        [[3.2896e+02, 3.9897e+02, 3.6354e+02, 3.5547e+02, 4.4263e+02,\n",
      "          3.7999e+02, 4.2849e+02, 3.9801e+02, 3.5972e+02, 3.3186e+02,\n",
      "          3.9929e+02, 3.9099e+02]]])\n",
      "Batch 8\n",
      "Inputs: tensor([[[3.1421e+02, 3.3847e+02, 2.8233e+02, 3.4450e+02, 3.1794e+02,\n",
      "          3.1335e+02, 3.1272e+02, 2.8797e+02, 3.5924e+02, 3.0697e+02,\n",
      "          3.4724e+02, 3.3675e+02, 3.5838e+02, 3.1083e+02, 3.3106e+02,\n",
      "          3.5350e+02, 3.0258e+02, 3.2345e+02]],\n",
      "\n",
      "        [[1.1156e+03, 1.1813e+03, 1.0741e+03, 1.0752e+03, 9.8680e+02,\n",
      "          1.1097e+03, 1.2056e+03, 1.0635e+03, 1.1657e+03, 1.1321e+03,\n",
      "          1.0904e+03, 1.1954e+03, 1.0924e+03, 1.0777e+03, 1.1992e+03,\n",
      "          1.1691e+03, 1.1946e+03, 1.1359e+03]],\n",
      "\n",
      "        [[6.8329e+02, 7.0327e+02, 6.8540e+02, 7.0055e+02, 7.0309e+02,\n",
      "          7.1382e+02, 6.9467e+02, 7.0060e+02, 7.0892e+02, 7.2136e+02,\n",
      "          6.9186e+02, 7.1657e+02, 7.2596e+02, 7.0049e+02, 7.3351e+02,\n",
      "          7.2466e+02, 7.2967e+02, 7.3864e+02]],\n",
      "\n",
      "        [[4.4294e+02, 4.7019e+02, 4.9799e+02, 5.2496e+02, 4.8588e+02,\n",
      "          4.6691e+02, 4.7527e+02, 5.1510e+02, 4.8294e+02, 4.6752e+02,\n",
      "          4.9866e+02, 5.1861e+02, 4.9466e+02, 4.8871e+02, 4.6766e+02,\n",
      "          4.8157e+02, 4.9150e+02, 4.5443e+02]],\n",
      "\n",
      "        [[       nan, 7.7044e-01, 8.1656e-01, 7.7489e-01, 7.6561e-01,\n",
      "          8.0301e-01, 8.3713e-01, 8.0269e-01, 8.8421e-01, 8.7719e-01,\n",
      "          8.6755e-01, 9.0226e-01, 8.6618e-01, 8.7382e-01, 9.0256e-01,\n",
      "          8.9442e-01, 9.3428e-01, 8.5976e-01]],\n",
      "\n",
      "        [[2.0999e+02, 1.8977e+02, 2.3754e+02, 1.8377e+02, 2.2439e+02,\n",
      "          2.0410e+02, 1.7810e+02, 2.0163e+02, 1.8914e+02, 1.6610e+02,\n",
      "          2.0049e+02, 2.2603e+02, 1.7394e+02, 1.5490e+02, 2.1428e+02,\n",
      "          1.4751e+02, 1.8988e+02, 1.7291e+02]],\n",
      "\n",
      "        [[2.8040e+03, 2.4000e+03, 2.4560e+03, 2.9720e+03, 2.2390e+03,\n",
      "          2.0580e+03, 2.0790e+03, 1.7950e+03, 1.8330e+03, 1.4650e+03,\n",
      "          1.2420e+03, 1.5200e+03, 1.5160e+03, 1.1780e+03, 1.2590e+03,\n",
      "          9.4900e+02, 8.2200e+02, 7.5600e+02]],\n",
      "\n",
      "        [[9.0797e+02, 8.4776e+02, 1.1523e+03, 9.4465e+02, 1.0090e+03,\n",
      "          1.1215e+03, 7.7677e+02, 9.8079e+02, 8.3781e+02, 1.1065e+03,\n",
      "          1.0408e+03, 9.3402e+02, 7.8859e+02, 7.4946e+02, 1.0026e+03,\n",
      "          7.8707e+02, 9.2702e+02, 9.4958e+02]]])\n",
      "Labels: tensor([[[3.2095e+02, 3.2879e+02, 3.3369e+02, 3.4490e+02, 3.4630e+02,\n",
      "          3.3563e+02, 3.1881e+02, 3.7630e+02, 3.2386e+02, 3.3471e+02,\n",
      "          3.6525e+02, 3.3299e+02]],\n",
      "\n",
      "        [[1.1524e+03, 1.2809e+03, 1.1228e+03, 1.2441e+03, 1.0922e+03,\n",
      "          1.1143e+03, 1.0995e+03, 1.2149e+03, 1.0932e+03, 1.1037e+03,\n",
      "          1.3131e+03, 1.3831e+03]],\n",
      "\n",
      "        [[7.5145e+02, 7.1224e+02, 7.2056e+02, 7.4372e+02, 7.2689e+02,\n",
      "          7.3955e+02, 7.4433e+02, 7.4074e+02, 7.5216e+02, 7.5642e+02,\n",
      "          7.5866e+02, 7.6851e+02]],\n",
      "\n",
      "        [[4.5425e+02, 4.7844e+02, 4.8561e+02, 4.7454e+02, 4.4563e+02,\n",
      "          4.3552e+02, 4.2971e+02, 4.9497e+02, 4.5485e+02, 4.6484e+02,\n",
      "          4.4362e+02, 4.4455e+02]],\n",
      "\n",
      "        [[8.7820e-01, 8.8881e-01, 8.9494e-01, 8.8198e-01, 8.7910e-01,\n",
      "          9.2120e-01, 8.9766e-01, 8.7347e-01, 8.9488e-01, 8.9105e-01,\n",
      "          9.0803e-01, 9.2000e-01]],\n",
      "\n",
      "        [[1.5334e+02, 1.6968e+02, 1.6656e+02, 1.3859e+02, 1.6548e+02,\n",
      "          1.9612e+02, 1.5089e+02, 1.2187e+02, 1.8073e+02, 1.3279e+02,\n",
      "          1.5506e+02, 1.4910e+02]],\n",
      "\n",
      "        [[6.5000e+02, 4.5600e+02, 5.6500e+02, 4.7500e+02, 5.5700e+02,\n",
      "          5.4800e+02, 4.6200e+02, 3.0800e+02, 3.6100e+02, 5.4100e+02,\n",
      "          4.7400e+02, 3.2500e+02]],\n",
      "\n",
      "        [[7.2945e+02, 9.0319e+02, 7.5371e+02, 1.0289e+03, 8.6045e+02,\n",
      "          7.2827e+02, 6.4008e+02, 5.3231e+02, 8.3805e+02, 6.3656e+02,\n",
      "          6.9159e+02, 6.8616e+02]]])\n",
      "Batch 9\n",
      "Inputs: tensor([[[10189.0000, 10118.0000, 10331.0000, 10198.0000, 10218.0000,\n",
      "          10193.0000, 10210.0000, 10285.0000, 10282.0000, 10348.0000,\n",
      "          10276.0000, 10271.0000, 10256.0000, 10256.0000, 10277.0000,\n",
      "          10244.0000, 10238.0000, 10212.0000]],\n",
      "\n",
      "        [[  981.8442,   849.6835,   846.7109,   916.0235,   774.8406,\n",
      "            797.6722,   913.7991,   767.5620,   760.7844,   868.3715,\n",
      "            754.6310,   731.1716,   980.5719,   854.5822,   874.5111,\n",
      "            961.2867,   783.8627,   813.1105]],\n",
      "\n",
      "        [[  224.0000,   271.0000,   196.0000,   307.0000,   269.0000,\n",
      "            412.0000,   391.0000,   323.0000,   332.0000,   325.0000,\n",
      "            229.0000,   197.0000,   172.0000,   173.0000,   269.0000,\n",
      "            125.0000,   256.0000,   161.0000]],\n",
      "\n",
      "        [[  512.5825,   521.3327,   509.9202,   510.7278,   510.9718,\n",
      "            513.9943,   524.2466,   525.1160,   531.0635,   537.0496,\n",
      "            518.4779,   533.7045,   505.4162,   529.7808,   529.2861,\n",
      "            532.8159,   527.5356,   522.8134]],\n",
      "\n",
      "        [[  814.2162,   802.2281,   787.5538,   757.1119,   776.9127,\n",
      "            760.1676,   813.9764,   855.7032,   881.6246,   852.1733,\n",
      "            859.7872,   886.9692,   844.3936,   859.7410,   894.1693,\n",
      "            931.1489,   901.8027,   982.9557]],\n",
      "\n",
      "        [[  480.4223,   477.3560,   470.5631,   478.5219,   476.9125,\n",
      "            485.2037,   487.5757,   498.5895,   491.7983,   488.0560,\n",
      "            495.3392,   504.9780,   482.4483,   499.0152,   508.5669,\n",
      "            498.9509,   499.1270,   509.9074]],\n",
      "\n",
      "        [[ 1114.9479,   964.2120,   985.9175,   798.8713,   711.3048,\n",
      "           1015.5964,  1025.7382,   826.1973,  1023.2960,   990.3350,\n",
      "           1044.6356,   833.3911,  1085.1110,   943.4718,  1100.1522,\n",
      "           1093.7023,   975.5409,  1107.4261]],\n",
      "\n",
      "        [[71428.0000, 72064.0000, 69571.0000, 60041.0000, 56135.0000,\n",
      "          54774.0000, 52599.0000, 51700.0000, 45175.0000, 45710.0000,\n",
      "          48625.0000, 39804.0000, 44244.0000, 35629.0000, 35745.0000,\n",
      "          34445.0000, 32574.0000, 31169.0000]]])\n",
      "Labels: tensor([[[10186.0000, 10190.0000, 10156.0000, 10145.0000, 10141.0000,\n",
      "          10165.0000, 10194.0000, 10269.0000, 10288.0000, 10310.0000,\n",
      "          10308.0000, 10302.0000]],\n",
      "\n",
      "        [[  985.7679,   805.8430,   823.1140,   904.9523,   864.6676,\n",
      "            872.9478,  1127.5461,  1008.4918,  1068.4564,  1143.9270,\n",
      "           1049.1090,  1131.6630]],\n",
      "\n",
      "        [[  206.0000,   163.0000,   187.0000,   250.0000,   221.0000,\n",
      "            133.0000,   114.0000,   109.0000,   177.0000,   200.0000,\n",
      "            214.0000,   227.0000]],\n",
      "\n",
      "        [[  560.3329,   564.2494,   577.0764,   566.6412,   604.3291,\n",
      "            580.9587,   637.3886,   635.7139,   634.5837,   691.6423,\n",
      "            710.0450,   739.1020]],\n",
      "\n",
      "        [[  938.0887,   984.0229,  1019.8509,   993.5540,  1000.5445,\n",
      "            981.0147,   989.5646,  1044.5112,  1095.8740,  1117.3225,\n",
      "           1106.1086,  1066.7708]],\n",
      "\n",
      "        [[  498.3400,   506.0905,   506.6271,   512.9168,   521.2244,\n",
      "            515.5690,   524.4525,   526.2334,   521.7393,   518.9246,\n",
      "            527.6877,   525.5370]],\n",
      "\n",
      "        [[  901.4396,   977.7847,  1195.4474,  1097.3693,   975.8320,\n",
      "           1064.7006,  1324.8536,  1076.9136,  1328.4890,  1193.2379,\n",
      "           1090.0315,  1294.2987]],\n",
      "\n",
      "        [[26240.0000, 25113.0000, 23835.0000, 25457.0000, 25426.0000,\n",
      "          20148.0000, 23924.0000, 22231.0000, 22334.0000, 19832.0000,\n",
      "          20585.0000, 21883.0000]]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfoundry-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
