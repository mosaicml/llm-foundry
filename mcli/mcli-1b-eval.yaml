integrations:
- integration_type: git_repo
  git_repo: mosaicml/llm-foundry
  # git_branch: # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[gpu]
  ssh_clone: false # Should be true if using a private repo

command: |
  cd llm-foundry/llmfoundry/icl_eval
  composer eval.py /mnt/config/parameters.yaml

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04

# Mosaic Cloud will use run_name (with a unique suffix) to populate the env var $COMPOSER_RUN_NAME
run_name: mosaic-gpt-1b-eval

gpu_num: 8
gpu_type: a100_80gb
cluster: r0z0 # replace with your cluster here!

# The below is injected as a YAML file: /mnt/config/parameters.yaml
parameters:
  max_seq_len: 2048

  tokenizer:
    name: gpt2
    kwargs:
      model_max_length: ${max_seq_len}

  model:
    name: mpt_causal_lm
    init_device: meta
    d_model: 2048
    n_heads: 16 # Modified 24->16 so that d_head == 128 to statisfy FlashAttention
    n_layers: 24
    expansion_ratio: 4
    max_seq_len: ${max_seq_len}
    vocab_size: 50368
    attn_config:
      attn_impl: triton

  load_path: # Add your (optional) Composer checkpoint path here!

  device_eval_batch_size: 16

  # FSDP config for model sharding
  # fsdp_config:
  #   sharding_strategy: FULL_SHARD
  #   mixed_precision: PURE

  icl_tasks:
  -
    label: piqa
    dataset_uri: # ADD YOUR OWN DATASET URI
    num_fewshot: [5]
    icl_task_type: multiple_choice
    continuation_delimiter: 'Answer: '
  -
    label: lambada
    dataset_uri: # ADD YOUR OWN DATASET URI
    num_fewshot: [0]
    icl_task_type: language_modeling
