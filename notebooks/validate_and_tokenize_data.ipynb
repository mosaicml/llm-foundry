{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f275a21b-47d4-472c-972b-e2a84a597db2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# FM FT API: Data Validation and \\$Token Estimation\n",
    "\n",
    "#### Usage Scenario:\n",
    "This notebook goes hand-in-hand with Databricks-Mosaicml's FT API. Our customers may find it useful in scenarios where there is a risk of data being malformed. It acts as a preventive measure to ensure data integrity and helps in cost assessment for the fine-tuning process.\n",
    "\n",
    "#### Script Purpose:\n",
    "- **Not for Training**: This script is not utilized during the training process.\n",
    "- **Ad-Hoc Validation**: It serves as an ad-hoc utility for users to run independently prior to starting fine-tuning.\n",
    "- **Data Verification**: Its primary function is to validate the user's data before they invoke the Fine-Tuning (FT) API.\n",
    "- **Cost Estimation**: Users can estimate the cost implications with this script.\n",
    "\n",
    "#### Note on Long-Term Solution:\n",
    "- **Temporary Measure**: This script is a stop-gap solution.\n",
    "- **Future Development**: We are in the process of developing a long-term data preparation service, which will eventually replace this script.\n",
    "\n",
    "#### User Defines:\n",
    "- The inputs to this validation script is assumed to be the same or a subset of the FT API arguments, i.e., a configuration like below. Is this a valid assumption?\n",
    "- For the reference, FT API expects following\n",
    "```\n",
    "cfg = {\n",
    "    model: str,\n",
    "    train_data_path: str,\n",
    "    save_folder: str,\n",
    "    *,\n",
    "    task_type: Optional[str] = \"INSTRUCTION_FINETUNE\",\n",
    "    eval_data_path: Optional[str] = None,\n",
    "    eval_prompts: Optional[List[str]] = None,\n",
    "    custom_weights_path: Optional[str] = None,\n",
    "    training_duration: Optional[str] = None,\n",
    "    learning_rate: Optional[float] = None,\n",
    "    context_length: Optional[int] = None,\n",
    "    experiment_trackers: Optional[List[Dict]] = None,\n",
    "    disable_credentials_check: Optional[bool] = None,\n",
    "    timeout: Optional[float] = 10,\n",
    "    future: Literal[False] = False,\n",
    "}\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d08a21c-9f5a-4ad2-af85-e016335cc53d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f330be7-ff76-4fa2-928f-396367b359ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y llm-foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6122e872-44b8-48a3-af61-4b907fc0a71f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e0a248-1d33-4379-841b-6d7d123bbc8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/mosaicml/llm-foundry.git@byod/data_validation\n",
    "%pip install --upgrade git+https://github.com/XiaohanZhangCMU/llm-foundryX.git@validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a3d8a4-c89a-40a6-8093-6c2afc2ae08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dcd849e-a35f-4999-acbe-6370c7a29294",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import datasets \n",
    "\n",
    "from llmfoundry.utils import (create_om_cfg, token_counts_and_validation, token_counts, \n",
    "        check_HF_datasets, is_hf_dataset_path, is_uc_delta_table,\n",
    "        pandas_processing_fn, integrity_check, convert_text_to_mds,\n",
    "        _args_str, plot_hist, dataframe_to_mds)\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a513cdd-967d-4a87-b56f-340053fa79cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instruction Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfebdfdf-b87c-4a77-b97c-4697566a55fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### User Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a30e53a6-d3cb-454b-82c0-2b48ca3dbf55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FT_API_args = Namespace(\n",
    "    model='EleutherAI/gpt-neox-20b',\n",
    "    train_data_path= 'main.streaming.random_large_table', # '/Volumes/main/mosaic_hackathon/managed-volume/IFT/train.jsonl', # 'tatsu-lab/alpaca/train', # , # 'tatsu-lab/alpaca/train',  # 'mosaicml/dolly_hhrlhf/train', # tatsu-lab/alpaca/train',\n",
    "    task_type='INSTRUCTION_FINETUNE',\n",
    "    training_duration=3,\n",
    "    context_length=2048,\n",
    ")\n",
    "\n",
    "temporary_jsonl_data_path = '/Volumes/main/mosaic_hackathon/managed-volume/IFT/ft_data_11Jan24_3/train'\n",
    "# os.environ['HF_ASSETS_CACHE'] = '/tmp/'\n",
    "# os.environ['HF_HOME'] = '/tmp/'\n",
    "# os.environ['HF_HUB_CACHE'] = '/tmp/'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/tmp/'\n",
    "os.makedirs(temporary_jsonl_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c45005-1a77-4162-b9e4-bd8df6f5ec69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data Loading\n",
    "\n",
    "The IFT data needs to stay with a format \n",
    "```\n",
    "prompt: xxx\n",
    "response or completion: yyy\n",
    "```\n",
    "\n",
    "Based on FT_API_args.train_data_path, we will select an ingestion method from three options.\n",
    "\n",
    "- Option-1. Your data is a JSONL file which stores in an object store supported by Composer. [Example file to-be-added](todo - add a link to such a file)\n",
    "- Option-2. You provide a Huggingface dataset ID. Note you need to provide a split as well. [Example dataset link to-be-added](huggingface.co)\n",
    "- Option-3. You have a delta table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751d8e3a-156c-432c-8e6e-a1530a5a9dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_dataset = None\n",
    "\n",
    "if is_hf_dataset_path(FT_API_args.train_data_path):\n",
    "    check_HF_datasets(FT_API_args.train_data_path)\n",
    "    dataset_id, split = '/'.join(FT_API_args.train_data_path.split('/')[:2]), FT_API_args.train_data_path.split('/')[-1]    \n",
    "    raw_dataset = datasets.load_dataset(dataset_id, split=split)       \n",
    "else:\n",
    "    if is_uc_delta_table(FT_API_args.train_data_path):    \n",
    "        df = spark.read.table(FT_API_args.train_data_path).toPandas()\n",
    "        df.to_json(os.path.join(temporary_jsonl_data_path, 'data.jsonl'), orient='records', lines=True)\n",
    "        raw_dataset = datasets.Dataset.from_pandas(df) \n",
    "        FT_API_args.train_data_path = temporary_jsonl_data_path\n",
    "    else: \n",
    "        # train_data_path is a jonsl file (local/remote)\n",
    "        from composer.utils import dist, get_file, parse_uri \n",
    "        data_path = FT_API_args.train_data_path \n",
    "        backend, _, _ = parse_uri(data_path)\n",
    "        if backend not in ['', None]: # It's a remote path, download before loading it\n",
    "            with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "                destination = os.path.join(tmp_dir, 'data.jsonl')\n",
    "                get_file(data_path, destination)\n",
    "                df = pd.read_json(destination, orient='records', lines=True)    \n",
    "        else: \n",
    "            df = pd.read_json(data_path, orient='records', lines=True)    \n",
    "\n",
    "        raw_dataset = datasets.Dataset.from_pandas(df)\n",
    "        FT_API_args.train_data_path = os.path.dirname(data_path)\n",
    "\n",
    "if raw_dataset is None: \n",
    "    raise RuntimeError(\"Can't find a proper ingestion method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d46367-bd32-473a-9f16-1b34a8dd9356",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b89b5c6-bf3a-4425-8645-4840dfeb0848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(raw_dataset))\n",
    "print(\"First example:\")\n",
    "for ex in raw_dataset: \n",
    "    print(ex)\n",
    "    print() \n",
    "    break \n",
    "\n",
    "_ALLOWED_RESPONSE_KEYS = {'response', 'completion'}\n",
    "_ALLOWED_PROMPT_KEYS = {'prompt'}\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in raw_dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1 \n",
    "        continue \n",
    "    \n",
    "    found = False \n",
    "    for key in _ALLOWED_PROMPT_KEYS:\n",
    "        prompts = ex.get(key, None)\n",
    "        if prompts:\n",
    "            found = True \n",
    "    if not found: \n",
    "        format_errors[\"missing_prompt\"] += 1\n",
    "\n",
    "    found = False\n",
    "    for key in _ALLOWED_RESPONSE_KEYS:        \n",
    "        responses = ex.get(\"response\", None)\n",
    "        if responses: \n",
    "            found = True \n",
    "    if not found:\n",
    "        format_errors[\"missing_response\"] += 1\n",
    "        \n",
    "if format_errors:\n",
    "    print(\"Oops! Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"Congratulations! No errors found\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9713a0ce-80f4-4187-b10b-4223b17fe4c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Token Estimation\n",
    "\n",
    "Tokenize the raw dataset and we see some statistics of the tokens and estimate the overall cost based on default trainining duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "439d3bd1-0569-456f-8872-3dbafd50cbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = FT_API_args.training_duration if FT_API_args.training_duration is not None else 1 \n",
    "batch_tokens = token_counts(FT_API_args)\n",
    "n_billing_tokens_in_dataset = sum(batch_tokens['ntokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85eaa02-ee39-4c6b-b14b-8ea1da8bf74d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, ~{n_epochs * n_billing_tokens_in_dataset} tokens will be used in training\")\n",
    "plot_hist(pd.Series(batch_tokens['ntokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e26a8778-d9b9-4028-bda5-1fab58862166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all_tokens = token_counts_and_validation(FT_API_args)\n",
    "# plot_hist(pd.Series(all_tokens))\n",
    "# pd.Series(all_tokens).max(), max(batch_tokens['ntokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6699f47f-9b53-47da-95c0-b862c5826d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Continued Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd37fdce-62d0-493e-bfa9-d823634b2a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### User Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a773173-2a7f-4605-a7ca-0ece52a905f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FT_API_args = Namespace(\n",
    "    model='EleutherAI/gpt-neox-20b',\n",
    "    train_data_path= '/Volumes/main/mosaic_hackathon/managed-volume/ABT',\n",
    "    task_type='CONTINUED_PRETRAIN',\n",
    "    training_duration=3,\n",
    "    context_length=2048,\n",
    ")\n",
    "temporary_mds_output_path = '/Volumes/main/mosaic_hackathon/managed-volume/mds_data_11Jan24_5'\n",
    "# temporary_mds_output_path = '/tmp/CPT/mds_data_11Jan24_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34bcddfb-7d4f-4243-bd02-7ac3e0dce711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf {temporary_mds_output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21e7d1b-db34-4e5d-b6d9-190dc75170d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Ingestion, Tokenization and Materialization\n",
    "\n",
    "CPT takes a folder of txt files as input. It tokenize the text fields and materialize as a streaming dataset of MDS format. \n",
    "\n",
    "FT API uses [llmfoundry/scripts/data_prep/convert_text_to_mds.py](https://github.com/mosaicml/llm-foundry/blob/main/scripts/data_prep/convert_text_to_mds.py) to download all the txt files and convert them to MDS. \n",
    "\n",
    "In this notebook, we provide two additional approaches via Spark and Dask. \n",
    "\n",
    "**Warning** CPT datasets are normally much larger than IFT, so the tokenization and materialization can be very time consuming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29a4a37-c2a0-4a18-8dcb-d9d29d68d683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**1. Delta Ingestion --> Spark Dataframe:** \n",
    "\n",
    "If you don't have a single-user-assigned cluster and DBR < 14.3, move on to option-2. \n",
    "\n",
    "Otherwise, you can leverage Delta Ingestion's tools to ingest the folder of txt files as a Spark dataframe and have the schema automatically inferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40c4d43-8396-4ceb-92ca-1bc037e33ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(FT_API_args.train_data_path)\n",
    "output_location = FT_API_args.train_data_path + '/*.txt'\n",
    "df = spark.sql(\"SELECT * FROM read_files('%s')\" % output_location).withColumnRenamed('value', 'text')\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830ad419-e844-4ae0-8348-167ea4b66f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**2. Dask.bag --> Dask.DataFrame:**  \n",
    "\n",
    "If you are on UC enabled clusters where mapInPandas does not work, you can try Dask. \n",
    "\n",
    "Dask uses the current node as a ```Local Cluster```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89f3a33-5348-4d80-90ce-a6fe84c16306",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "input_folder = FT_API_args.train_data_path\n",
    "pattern = input_folder + '/*.txt'\n",
    "b = db.read_text(pattern, linedelimiter='\\n', blocksize='128MiB')\n",
    "df = b.to_dataframe(columns = ['text'])\n",
    "df = df[df.text != '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbc7944-9b41-49d3-98d6-6eb91425d1ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**3. dataframe_to_mds + tokenization:**  \n",
    "\n",
    "dataframe_to_mds is a utility function. It takes either a dask dataframe or a Spark dataframe, and a tokenization function and convert raw txt to MDS dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c7aaeae-1c1b-498b-b97b-2d36b0e62938",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mds_kwargs = {\n",
    "    'out': temporary_mds_output_path,\n",
    "    'columns': {\n",
    "        'tokens': 'bytes'\n",
    "    },\n",
    "    'keep_local': True, \n",
    "}\n",
    "udf_kwargs = {\n",
    "    'concat_tokens': FT_API_args.context_length,\n",
    "    'tokenizer': FT_API_args.model, \n",
    "    'eos_text': '',\n",
    "    'compression': 'zstd',\n",
    "    'no_wrap': False,\n",
    "    'bos_text': '',\n",
    "}\n",
    "dataframe_to_mds(df,\n",
    "                 merge_index=True,\n",
    "                 mds_kwargs=mds_kwargs,\n",
    "                 udf_iterable=pandas_processing_fn,\n",
    "                 udf_kwargs=udf_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb27026e-5f1e-453f-983d-8909f8999892",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef494943-791e-44c1-87f3-92e022eb480a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We perform integrity checks on MDS dataset\n",
    "- number of shards match with index.json. \n",
    "- Inspect first 5 examples by decode the tokens back to texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d592bc94-c374-493a-9a30-6f2b9203a6d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num examples:\", len(df))\n",
    "print(\"First example:\")\n",
    "for ex in df['text']: \n",
    "    print(ex)\n",
    "    print() \n",
    "    break \n",
    "\n",
    "if not integrity_check(temporary_mds_output_path): \n",
    "    raise ValueError(\"MDS has not been created correctly. There are missing shards!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5f8305-6f00-484c-818c-5dcddcef0aef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "import numpy as np\n",
    "from streaming import StreamingDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(FT_API_args.model)\n",
    "tokenizer.model_max_length = 5000000000  # Hack to prevent warnings from HuggingFace\n",
    "mds_dataset = StreamingDataset(local=temporary_mds_output_path, shuffle=False)\n",
    "for i in range(5):\n",
    "    l = np.frombuffer(mds_dataset[i]['tokens'], dtype=np.int64)\n",
    "    print(''.join(tokenizer.decode(l)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298eb990-9160-4e1b-958f-33dd2c11b54b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Token Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc58cb3-0a19-4512-9584-642f0a2be4df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS_PER_EXAMPLE = FT_API_args.context_length if FT_API_args.context_length is not None else 4096\n",
    "TARGET_EPOCHS = FT_API_args.training_duration if FT_API_args.training_duration is not None else 1 \n",
    "n_epochs = TARGET_EPOCHS\n",
    "\n",
    "n_billing_tokens_in_dataset = len(mds_dataset) * FT_API_args.context_length \n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, ~{n_epochs * n_billing_tokens_in_dataset} tokens will be used in training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8775fed8-6440-4a20-82f3-59b6cff73421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "validate_and_tokenize_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
