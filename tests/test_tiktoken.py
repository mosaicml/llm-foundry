# Copyright 2022 MosaicML LLM Foundry authors
# SPDX-License-Identifier: Apache-2.0

import pathlib

import pytest
import transformers

from llmfoundry import TiktokenTokenizerWrapper

TEST_STRINGS = [
    'Hello world!',
    'def hello_world(input: str):\n    print(input)',
    '0000000000000000000000000000',
    '19234324 asas sf 119aASDFM AW3RAW-AF;;9900',
    '\n\n\n\nhello\n\t,'
]

# taken from https://github.com/explosion/spaCy/blob/8f0d6b0a8c42e4852bf6e24cdf629043f2f39361/spacy/tests/tokenizer/test_naughty_strings.py#L7
NAUGHTY_STRINGS = [
    # ASCII punctuation
    r",./;'[]\-=",
    r'<>?:"{}|_+',
    r'!@#$%^&*()`~"',
    # Unicode additional control characters, byte order marks
    r"Â­Ø€ØØ‚ØƒØ„Ø…ØœÛÜá â€‹â€Œâ€â€â€â€ª",
    r"ï¿¾",
    # Unicode Symbols
    r"Î©â‰ˆÃ§âˆšâˆ«ËœÂµâ‰¤â‰¥Ã·",
    r"Ã¥ÃŸâˆ‚Æ’Â©Ë™âˆ†ËšÂ¬â€¦Ã¦",
    "Å“âˆ‘Â´Â®â€ Â¥Â¨Ë†Ã¸Ï€â€œâ€˜",
    r"Â¡â„¢Â£Â¢âˆÂ§Â¶â€¢ÂªÂºâ€“â‰ ",
    r"Â¸Ë›Ã‡â—ŠÄ±ËœÃ‚Â¯Ë˜Â¿",
    r"Ã…ÃÃÃËÃ“Ã”ï£¿Ã’ÃšÃ†â˜ƒ",
    r"Å’â€Â´â€°Ë‡ÃÂ¨Ë†Ã˜âˆâ€â€™",
    r"`â„â‚¬â€¹â€ºï¬ï¬‚â€¡Â°Â·â€šâ€”Â±",
    r"â…›â…œâ…â…",
    r"ĞĞ‚ĞƒĞ„Ğ…Ğ†Ğ‡ĞˆĞ‰ĞŠĞ‹ĞŒĞĞĞĞĞ‘Ğ’Ğ“Ğ”Ğ•Ğ–Ğ—Ğ˜Ğ™ĞšĞ›ĞœĞĞĞŸĞ Ğ¡Ğ¢Ğ£Ğ¤Ğ¥Ğ¦Ğ§Ğ¨Ğ©ĞªĞ«Ğ¬Ğ­Ğ®Ğ¯Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑ",
    r"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©",
    # Unicode Subscript/Superscript/Accents
    r"â°â´âµ",
    r"â‚€â‚â‚‚",
    r"â°â´âµâ‚€â‚â‚‚",
    r"à¸”à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡ à¸”à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡ à¸”à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡à¹‡à¹‰à¹‰à¹‰à¹‰à¹‰à¹‡à¹‡à¹‡à¹‡",
    r" Ì„  Ì„",
    # Two-Byte Characters
    r"ç”°ä¸­ã•ã‚“ã«ã‚ã’ã¦ä¸‹ã•ã„",
    r"ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã¸è¡Œã‹ãªã„ã‹",
    r"å’Œè£½æ¼¢èª",
    r"éƒ¨è½æ ¼",
    r"ì‚¬íšŒê³¼í•™ì› ì–´í•™ì—°êµ¬ì†Œ",
    r"ì°¦ì°¨ë¥¼ íƒ€ê³  ì˜¨ í²ì‹œë§¨ê³¼ ì‘›ë‹¤ë¦¬ ë˜ ë°©ê°í•˜",
    r"ç¤¾æœƒç§‘å­¸é™¢èªå­¸ç ”ç©¶æ‰€",
    r"ìš¸ë€ë°”í† ë¥´",
    r"ğ œğ œ±ğ ¹ğ ±“ğ ±¸ğ ²–ğ ³",
    # Japanese Emoticons
    r"ãƒ½à¼¼àºˆÙ„Íœàºˆà¼½ï¾‰ ãƒ½à¼¼àºˆÙ„Íœàºˆà¼½ï¾‰",
    r"(ï½¡â—• âˆ€ â—•ï½¡)",
    r"ï½€ï½¨(Â´âˆ€ï½€âˆ©",
    r"__ï¾›(,_,*)",
    r"ãƒ»(ï¿£âˆ€ï¿£)ãƒ»:*:",
    r"ï¾Ÿï½¥âœ¿ãƒ¾â•²(ï½¡â—•â€¿â—•ï½¡)â•±âœ¿ï½¥ï¾Ÿ",
    r",ã€‚ãƒ»:*:ãƒ»ã‚œâ€™( â˜» Ï‰ â˜» )ã€‚ãƒ»:*:ãƒ»ã‚œâ€™",
    r"(â•¯Â°â–¡Â°ï¼‰â•¯ï¸µ â”»â”â”»)" "(ï¾‰à²¥ç›Šà²¥ï¼‰ï¾‰ï»¿ â”»â”â”»", # type: ignore
    r"â”¬â”€â”¬ãƒ( Âº _ Âºãƒ)",
    r"( Í¡Â° ÍœÊ– Í¡Â°)",
    # Emoji
    r"ğŸ˜",
    r"ğŸ‘©ğŸ½",
    r"ğŸ‘¾ ğŸ™‡ ğŸ’ ğŸ™… ğŸ™† ğŸ™‹ ğŸ™ ğŸ™",
    r"ğŸµ ğŸ™ˆ ğŸ™‰ ğŸ™Š",
    r"â¤ï¸ ğŸ’” ğŸ’Œ ğŸ’• ğŸ’ ğŸ’“ ğŸ’— ğŸ’– ğŸ’˜ ğŸ’ ğŸ’Ÿ ğŸ’œ ğŸ’› ğŸ’š ğŸ’™",
    r"âœ‹ğŸ¿ ğŸ’ªğŸ¿ ğŸ‘ğŸ¿ ğŸ™ŒğŸ¿ ğŸ‘ğŸ¿ ğŸ™ğŸ¿",
    r"ğŸš¾ ğŸ†’ ğŸ†“ ğŸ†• ğŸ†– ğŸ†— ğŸ†™ ğŸ§",
    r"0ï¸âƒ£ 1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ 4ï¸âƒ£ 5ï¸âƒ£ 6ï¸âƒ£ 7ï¸âƒ£ 8ï¸âƒ£ 9ï¸âƒ£ ğŸ”Ÿ",
    # Regional Indicator Symbols
    r"ğŸ‡ºğŸ‡¸ğŸ‡·ğŸ‡ºğŸ‡¸ ğŸ‡¦ğŸ‡«ğŸ‡¦ğŸ‡²ğŸ‡¸",
    r"ğŸ‡ºğŸ‡¸ğŸ‡·ğŸ‡ºğŸ‡¸ğŸ‡¦ğŸ‡«ğŸ‡¦ğŸ‡²",
    r"ğŸ‡ºğŸ‡¸ğŸ‡·ğŸ‡ºğŸ‡¸ğŸ‡¦",
    # Unicode Numbers
    r"ï¼‘ï¼’ï¼“",
    r"Ù¡Ù¢Ù£",
    # Right-To-Left Strings
    r"Ø«Ù… Ù†ÙØ³ Ø³Ù‚Ø·Øª ÙˆØ¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ØŒ, Ø¬Ø²ÙŠØ±ØªÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù† Ø¯Ù†Ùˆ. Ø¥Ø° Ù‡Ù†Ø§ØŸ Ø§Ù„Ø³ØªØ§Ø± ÙˆØªÙ†ØµÙŠØ¨ ÙƒØ§Ù†. Ø£Ù‡Ù‘Ù„ Ø§ÙŠØ·Ø§Ù„ÙŠØ§ØŒ Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ§-ÙØ±Ù†Ø³Ø§ Ù‚Ø¯ Ø£Ø®Ø°. Ø³Ù„ÙŠÙ…Ø§Ù†ØŒ Ø¥ØªÙØ§Ù‚ÙŠØ© Ø¨ÙŠÙ† Ù…Ø§, ÙŠØ°ÙƒØ± Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø£ÙŠ Ø¨Ø¹Ø¯, Ù…Ø¹Ø§Ù…Ù„Ø© Ø¨ÙˆÙ„Ù†Ø¯Ø§ØŒ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ø¹Ù„ Ø¥ÙŠÙˆ.",
    r"Ø¥ÙŠÙˆ.",
    r"×‘Ö°Ö¼×¨Öµ××©Ö´××™×ª, ×‘Ö¸Ö¼×¨Ö¸× ×Ö±×œÖ¹×”Ö´×™×, ×Öµ×ª ×”Ö·×©Ö¸Ö¼××Ö·×™Ö´×, ×•Ö°×Öµ×ª ×”Ö¸×Ö¸×¨Ö¶×¥",
    r"×”Ö¸×™Ö°×ªÖ¸×”testØ§Ù„ØµÙØ­Ø§Øª Ø§Ù„ØªÙ‘Ø­ÙˆÙ„",
    r"ï·½",
    r"ï·º",
    r"Ù…ÙÙ†ÙØ§Ù‚ÙØ´ÙØ©Ù Ø³ÙØ¨ÙÙ„Ù Ø§ÙØ³Ù’ØªÙØ®Ù’Ø¯ÙØ§Ù…Ù Ø§Ù„Ù„Ù‘ÙØºÙØ©Ù ÙÙÙŠ Ø§Ù„Ù†Ù‘ÙØ¸ÙÙ…Ù Ø§Ù„Ù’Ù‚ÙØ§Ø¦ÙÙ…ÙØ©Ù ÙˆÙÙÙÙŠÙ… ÙŠÙØ®ÙØµÙ‘Ù Ø§Ù„ØªÙ‘ÙØ·Ù’Ø¨ÙÙŠÙ‚ÙØ§ØªÙ Ø§Ù„Ù’Ø­Ø§Ø³ÙÙˆØ¨ÙÙŠÙ‘ÙØ©ÙØŒ",
    # Trick Unicode
    r"â€ªâ€ªtestâ€ª",
    r"â€«test",
    r"â€©testâ€©",
    r"testâ test",
    r"â¦testâ§",
    # Zalgo Text
    r"á¹°ÌºÌºÌ•oÍ Ì·iÌ²Ì¬Í‡ÌªÍ™nÌÌ—Í•vÌŸÌœÌ˜Ì¦ÍŸoÌ¶Ì™Ì°Ì kÃ¨ÍšÌ®ÌºÌªÌ¹Ì±Ì¤ Ì–tÌÍ•Ì³Ì£Ì»ÌªÍhÌ¼Í“Ì²Ì¦Ì³Ì˜Ì²eÍ‡Ì£Ì°Ì¦Ì¬Í Ì¢Ì¼Ì»Ì±Ì˜hÍšÍÍ™ÌœÌ£Ì²Í…iÌ¦Ì²Ì£Ì°Ì¤vÌ»ÍeÌºÌ­Ì³ÌªÌ°-mÌ¢iÍ…nÌ–ÌºÌÌ²Ì¯Ì°dÌµÌ¼ÌŸÍ™Ì©Ì¼Ì˜Ì³ ÌÌ¥Ì±Ì³Ì­rÌ›Ì—Ì˜eÍ™pÍ rÌ¼ÌÌ»Ì­Ì—eÌºÌ Ì£ÍŸsÌ˜Í‡Ì³ÍÌÍ‰eÍ‰Ì¥Ì¯ÌÌ²ÍšÌ¬ÍœÇ¹Ì¬ÍÍÌŸÌ–Í‡Ì¤tÍÌ¬Ì¤Í“Ì¼Ì­Í˜Í…iÌªÌ±nÍ gÌ´Í‰ ÍÍ‰Í…cÌ¬ÌŸhÍ¡aÌ«Ì»Ì¯Í˜oÌ«ÌŸÌ–ÍÌ™ÌÍ‰sÌ—Ì¦Ì².Ì¨Ì¹ÍˆÌ£",
    r"Ì¡Í“ÌÍ…IÌ—Ì˜Ì¦ÍnÍ‡Í‡Í™vÌ®Ì«okÌ²Ì«Ì™ÍˆiÌ–Í™Ì­Ì¹Ì ÌnÌ¡Ì»Ì®Ì£ÌºgÌ²ÍˆÍ™Ì­Í™Ì¬Í Ì°tÍ”Ì¦hÌÌ²eÌ¢Ì¤ ÍÌ¬Ì²Í–fÌ´Ì˜Í•Ì£Ã¨Í–áº¹Ì¥Ì©lÍ–Í”ÍšiÍ“ÍšÌ¦Í nÍ–ÍÌ—Í“Ì³Ì®gÍ Ì¨oÍšÌªÍ¡fÌ˜Ì£Ì¬ Ì–Ì˜Í–ÌŸÍ™Ì®cÒ‰Í”Ì«Í–Í“Í‡Í–Í…hÌµÌ¤Ì£ÍšÍ”Ã¡Ì—Ì¼Í•Í…oÌ¼Ì£Ì¥sÌ±ÍˆÌºÌ–Ì¦Ì»Í¢.Ì›Ì–ÌÌ Ì«Ì°",
    r"Ì—ÌºÍ–Ì¹Ì¯Í“á¹®Ì¤ÍÌ¥Í‡ÍˆhÌ²ÌeÍÍ“Ì¼Ì—Ì™Ì¼Ì£Í” Í‡ÌœÌ±Ì Í“ÍÍ…NÍ•Í eÌ—Ì±zÌ˜ÌÌœÌºÍ™pÌ¤ÌºÌ¹ÍÌ¯ÍšeÌ Ì»Ì ÍœrÌ¨Ì¤ÍÌºÌ–Í”Ì–Ì–dÌ ÌŸÌ­Ì¬ÌÍŸiÌ¦Í–Ì©Í“Í”Ì¤aÌ Ì—Ì¬Í‰Ì™nÍšÍœ Ì»ÌÌ°ÍšÍ…hÌµÍ‰iÌ³ÌvÌ¢Í‡á¸™ÍÍŸ-Ò‰Ì­Ì©Ì¼Í”mÌ¤Ì­Ì«iÍ•Í‡ÌÌ¦nÌ—Í™á¸ÌŸ Ì¯Ì²Í•ÍÇ«ÌŸÌ¯Ì°Ì²Í™Ì»Ìf ÌªÌ°Ì°Ì—Ì–Ì­Ì˜Í˜cÌ¦ÍÌ²ÌÍÌ©Ì™á¸¥ÍšaÌ®ÍÌŸÌ™ÍœÆ¡Ì©Ì¹ÍsÌ¤.ÌÌ Ò‰ZÌ¡Ì–ÌœÍ–Ì°Ì£Í‰ÌœaÍ–Ì°Í™Ì¬Í¡lÌ²Ì«Ì³ÍÌ©gÌ¡ÌŸÌ¼Ì±ÍšÌÌ¬Í…oÌ—Íœ.ÌŸ",
    r"Ì¦HÌ¬Ì¤Ì—Ì¤ÍeÍœ ÌœÌ¥ÌÌ»ÍÌŸÌwÌ•hÌ–Ì¯Í“oÌÍ™Ì–ÍÌ±Ì® Ò‰ÌºÌ™ÌÌŸÍˆWÌ·Ì¼Ì­aÌºÌªÍÄ¯ÍˆÍ•Ì­Í™Ì¯ÌœtÌ¶Ì¼Ì®sÌ˜Í™Í–Ì• Ì Ì«Ì BÌ»ÍÍ™Í‰Ì³Í…eÌµhÌµÌ¬Í‡Ì«Í™iÌ¹Í“Ì³Ì³Ì®ÍÌ«Ì•nÍŸdÌ´ÌªÌœÌ– Ì°Í‰Ì©Í‡Í™Ì²ÍÍ…TÍ–Ì¼Í“ÌªÍ¢hÍÍ“Ì®Ì»eÌ¬ÌÌŸÍ… Ì¤Ì¹ÌWÍ™ÌÌÍ”Í‡ÍÍ…aÍÍ“Í”Ì¹Ì¼Ì£lÌ´Í”Ì°Ì¤ÌŸÍ”á¸½Ì«.Í•",
    r"ZÌ®ÌÌ Í™Í”Í…á¸€Ì—ÌÍˆÌ»Ì—á¸¶Í™ÍÌ¯Ì¹ÌÍ“GÌ»OÌ­Ì—Ì®",
    # Unicode Upsidedown
    r"Ë™Énbá´‰lÉ ÉuÆƒÉÉ¯ ÇÉ¹olop Ê‡Ç ÇÉ¹oqÉl Ê‡n Ê‡unpá´‰pá´‰É”uá´‰ É¹odÉ¯ÇÊ‡ poÉ¯sná´‰Ç op pÇs 'Ê‡á´‰lÇ Æƒuá´‰É”sá´‰dá´‰pÉ É¹nÊ‡ÇÊ‡É”ÇsuoÉ” 'Ê‡ÇÉ¯É Ê‡á´‰s É¹olop É¯nsdá´‰ É¯ÇÉ¹oË¥",
    r"00Ë™Æ–$-",
    # Unicode font
    r"ï¼´ï½ˆï½… ï½‘ï½•ï½‰ï½ƒï½‹ ï½‚ï½’ï½ï½—ï½ ï½†ï½ï½˜ ï½Šï½•ï½ï½ï½“ ï½ï½–ï½…ï½’ ï½”ï½ˆï½… ï½Œï½ï½šï½™ ï½„ï½ï½‡",
    r"ğ“ğ¡ğ ğªğ®ğ¢ğœğ¤ ğ›ğ«ğ¨ğ°ğ§ ğŸğ¨ğ± ğ£ğ®ğ¦ğ©ğ¬ ğ¨ğ¯ğğ« ğ­ğ¡ğ ğ¥ğšğ³ğ² ğğ¨ğ ",
    r"ğ•¿ğ–ğ–Š ğ––ğ–šğ–ğ–ˆğ– ğ–‡ğ–—ğ–”ğ–œğ–“ ğ–‹ğ–”ğ– ğ–ğ–šğ–’ğ–•ğ–˜ ğ–”ğ–›ğ–Šğ–— ğ–™ğ–ğ–Š ğ–‘ğ–†ğ–Ÿğ– ğ–‰ğ–”ğ–Œ",
    r"ğ‘»ğ’‰ğ’† ğ’’ğ’–ğ’Šğ’„ğ’Œ ğ’ƒğ’“ğ’ğ’˜ğ’ ğ’‡ğ’ğ’™ ğ’‹ğ’–ğ’ğ’‘ğ’” ğ’ğ’—ğ’†ğ’“ ğ’•ğ’‰ğ’† ğ’ğ’‚ğ’›ğ’š ğ’…ğ’ğ’ˆ",
    r"ğ“£ğ“±ğ“® ğ“ºğ“¾ğ“²ğ“¬ğ“´ ğ“«ğ“»ğ“¸ğ”€ğ“· ğ“¯ğ“¸ğ” ğ“³ğ“¾ğ“¶ğ“¹ğ“¼ ğ“¸ğ“¿ğ“®ğ“» ğ“½ğ“±ğ“® ğ“µğ“ªğ”ƒğ”‚ ğ“­ğ“¸ğ“°",
    r"ğ•‹ğ•™ğ•– ğ•¢ğ•¦ğ•šğ•”ğ•œ ğ•“ğ•£ğ• ğ•¨ğ•Ÿ ğ•—ğ• ğ•© ğ•›ğ•¦ğ•ğ•¡ğ•¤ ğ• ğ•§ğ•–ğ•£ ğ•¥ğ•™ğ•– ğ•ğ•’ğ•«ğ•ª ğ••ğ• ğ•˜",
    r"ğšƒğš‘ğš ğššğšğš’ğšŒğš” ğš‹ğš›ğš˜ğš ğš— ğšğš˜ğš¡ ğš“ğšğš–ğš™ğšœ ğš˜ğšŸğšğš› ğšğš‘ğš ğš•ğšŠğš£ğš¢ ğšğš˜ğš",
    r"â’¯â’£â’  â’¬â’°â’¤â’â’¦ â’â’­â’ªâ’²â’© â’¡â’ªâ’³ â’¥â’°â’¨â’«â’® â’ªâ’±â’ â’­ â’¯â’£â’  â’§â’œâ’µâ’´ â’Ÿâ’ªâ’¢",
    # File paths
    r"../../../../../../../../../../../etc/passwd%00",
    r"../../../../../../../../../../../etc/hosts",
    # iOS Vulnerabilities
    r"PowerÙ„ÙÙ„ÙØµÙ‘Ø¨ÙÙ„ÙÙ„ØµÙ‘Ø¨ÙØ±Ø±Ù‹ à¥£ à¥£h à¥£ à¥£å†—",
    r"ğŸ³0ğŸŒˆï¸",
]

TEST_STRINGS += NAUGHTY_STRINGS

@pytest.mark.parametrize('model_name',
                         ['gpt-4', 'gpt-3.5-turbo', 'text-davinci-003'])
def test_tiktoken(model_name: str, tmp_path: pathlib.Path):
    tiktoken = pytest.importorskip('tiktoken')

    # Construction
    wrapped_tokenizer = TiktokenTokenizerWrapper(model_name='gpt-4')
    original_tokenizer = tiktoken.encoding_for_model('gpt-4')

    # Repr works
    _ = wrapped_tokenizer.__repr__()

    # Save and load
    wrapped_tokenizer.save_pretrained(tmp_path)
    reloaded_wrapped_tokenizer = transformers.AutoTokenizer.from_pretrained(
        tmp_path, trust_remote_code=True)

    didnt_match = []
    # Simple tokenization test
    for string in TEST_STRINGS:
        wrapped_output = wrapped_tokenizer(string)
        original_output = original_tokenizer.encode(string)
        reloaded_wrapped_output = reloaded_wrapped_tokenizer(string)
        assert wrapped_output['input_ids'] == original_output
        assert set(wrapped_output.keys()) == {'input_ids', 'attention_mask'}
        assert reloaded_wrapped_output == wrapped_output

    # Round trip
    for string in TEST_STRINGS:
        wrapped_output = wrapped_tokenizer.decode(wrapped_tokenizer(string)['input_ids'])
        original_output = original_tokenizer.decode(original_tokenizer.encode(string))
        reloaded_wrapped_output = reloaded_wrapped_tokenizer.decode(reloaded_wrapped_tokenizer(string)['input_ids'])
        assert wrapped_output == string
        assert original_output == string
        assert reloaded_wrapped_output == string

    # Batched tokenization
    wrapped_output = wrapped_tokenizer(
        ['Hello world!', 'Hello world but longer!'])
    original_output = original_tokenizer.encode_batch(
        ['Hello world!', 'Hello world but longer!'])
    reloaded_wrapped_output = reloaded_wrapped_tokenizer(
        ['Hello world!', 'Hello world but longer!'])
    assert wrapped_output['input_ids'] == original_output
    assert set(wrapped_output.keys()) == {'input_ids', 'attention_mask'}
    assert reloaded_wrapped_output == wrapped_output

    # With padding
    wrapped_tokenizer.pad_token_id = wrapped_tokenizer.eos_token_id
    reloaded_wrapped_tokenizer.pad_token_id = reloaded_wrapped_tokenizer.eos_token_id
    wrapped_output = wrapped_tokenizer(
        ['Hello world!', 'Hello world but longer!'], padding=True)
    original_output = original_tokenizer.encode_batch(
        ['Hello world!', 'Hello world but longer!'])
    reloaded_wrapped_output = reloaded_wrapped_tokenizer(
        ['Hello world!', 'Hello world but longer!'], padding=True)
    for wrapped1, attn_mask, original1 in zip(wrapped_output['input_ids'],
                                              wrapped_output['attention_mask'],
                                              original_output):
        original_length = len(original1)
        assert wrapped1[:original_length] == original1
        assert sum(attn_mask) == original_length

    assert set(wrapped_output.keys()) == {'input_ids', 'attention_mask'}
    assert reloaded_wrapped_output == wrapped_output

    # Get vocab
    wrapped_vocab = wrapped_tokenizer.get_vocab()
    reloaded_wrapped_vocab = reloaded_wrapped_tokenizer.get_vocab()
    assert wrapped_vocab == reloaded_wrapped_vocab

    didnt_match = []
    for key, value in wrapped_vocab.items():
        if original_tokenizer.encode(key, allowed_special='all') == [value]:
            continue
        else:
            didnt_match.append(
                (key, original_tokenizer.encode(key,
                                                allowed_special='all'), value))

    # Decode is lossy because some bytes are not representable in utf-8
    # see https://github.com/openai/tiktoken/blob/39f29cecdb6fc38d9a3434e5dd15e4de58cf3c80/tiktoken/core.py#L245-L247
    assert len(didnt_match) == 77
